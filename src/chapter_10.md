
# Лекция 10: Алгебра и геометрия глубокого обучения

- [Лекция 10: Алгебра и геометрия глубокого обучения](#лекция-10-алгебра-и-геометрия-глубокого-обучения)
  - [1. Алгебраические типы данных (ADT)](#1-алгебраические-типы-данных-adt)
  - [2. Построение архитектуры для различных типов (Vec ↔ Prod, Vec ↔ Sum)](#2-построение-архитектуры-для-различных-типов-vec--prod-vec--sum)
  - [3. Частные случаи архитектур для ADT](#3-частные-случаи-архитектур-для-adt)
  - [4. Графы](#4-графы)
  - [5. Частные случаи архитектур для графов](#5-частные-случаи-архитектур-для-графов)

---

## 1. Алгебраические типы данных (ADT)

**1.1. Введение**
*   **Абстрактное описание данных:** ADT предоставляют способ формального описания структуры данных.
*   **Конструирование типов:** Типы создаются с помощью алгебраических операторов (произведение, сумма).
*   **Описание инвариантов:** Позволяют четко определить свойства и ограничения данных.
*   **Применение:**
    *   **Функциональное программирование:** Основа систем типов (Haskell, Scala, F#).
    *   **Аналитическая комбинаторика:** Используются для подсчета и анализа комбинаторных структур.
    *   **Базы данных:** Для описания схем данных.
    *   **Глубокое обучение:** Могут служить основой для проектирования архитектур нейронных сетей, адаптированных к структуре входных и выходных данных.

**1.2. Тип-произведение (Product Type)**
*   Кодирует **составной тип данных**, где значение состоит из комбинации значений нескольких других типов. Все компоненты должны присутствовать.
*   Обозначается операторами: `*` (звёздочка), `·` (точка), `×` (умножение), `&` (амперсанд).
*   В языках программирования соответствует:
    *   `struct` (C/C++)
    *   `class` (Java, C#, Python - в части полей данных)
    *   `record` (Pascal, SQL)
    *   `tuple` (Python, Haskell)
*   **Пример:** Описание ириса из известного набора данных.
    $$ \text{Iris} = \text{SepalLength} \times \text{SepalWidth} \times \text{PetalLength} \times \text{PetalWidth} $$
    Каждый экземпляр типа `Iris` должен содержать значения для всех четырех признаков.

**1.3. Тип-сумма (Sum Type / Tagged Union / Variant)**
*   Кодирует **объединение типов**, где значение принадлежит одному из нескольких возможных типов.
*   Обозначается операторами: `+` (плюс), `|` (вертикальная черта).
*   В языках программирования соответствует:
    *   `union` (C/C++ - но без тега, что небезопасно)
    *   `enum` (если значения являются типами, а не просто константами)
    *   `interface` (в контексте реализации одного из нескольких вариантов)
    *   Алгебраические типы данных в функциональных языках (например, `data Either a b = Left a | Right b` в Haskell).
*   **Пример:** Виды ирисов.
    $$ \text{IrisSpecies} = \text{Setosa} + \text{Versicolor} + \text{Virginica} $$
    Значение типа `IrisSpecies` может быть либо `Setosa`, либо `Versicolor`, либо `Virginica`.

**1.4. Базовые и коллекционные типы**
*   **Векторные типы ($Vec$ или $\mathbb{R}^n$):**
    *   Используются как **промежуточное состояние** (скрытые представления) в нейронных сетях.
    *   Используются как **вход** для простых архитектур (например, полносвязные сети).
    *   Используются как **выход**, если требуется предсказать непрерывные значения или логиты (значения перед Softmax).
*   **Пустой тип ($Eps$ или $\epsilon$):**
    *   Тип, не имеющий значений (или имеющий одно уникальное значение, например, `()` - unit type).
    *   Можно рассматривать как **0-мерный вектор**.
    *   Используется для обозначения отсутствия информации или как компонент в ADT (например, `Maybe a = Just a | Nothing` где `Nothing` можно представить как `Eps`).
*   **Последовательность ($Seq(X)$):**
    *   Упорядоченное множество элементов типа $X$. Порядок важен.
    *   Пример: $$ \text{Text} = \text{Seq}(\text{Words}) $$
*   **Множество ($Set(X)$):**
    *   Неупорядоченное множество элементов типа $X$. Порядок не важен, дубликаты обычно не учитываются или имеют кратность.
    *   Пример: $$ \text{MovieCast} = \text{Set}(\text{Actor}) $$
*   **Другие коллекционные типы:**
    *   **Цикл ($Cycle(X)$):** Последовательность, где конец соединен с началом.
    *   **Мультимножество ($MSet(X)$):** Множество, где элементы могут повторяться (важна кратность).
    *   **Степенное множество ($PSet(X)$ / Power Set):** Множество всех подмножеств.
    *   В теории эти типы также можно приспособить для построения архитектур.
*   **Двумерные множества:**
    *   **Изображение ($Img(X)$):** Двумерная решетка элементов типа $X$. Порядок строк и столбцов важен.
        *   Пример: $$ \text{CIFAR} = \text{Img}(\text{RGB}) $$ (где RGB - это тип-произведение Red $\times$ Green $\times$ Blue)
        *   Пример: $$ \text{SegmentationTarget} = \text{Img}(\text{Class}) $$ (каждый пиксель имеет метку класса)
    *   **Таблица ($Tab(X)$):** Двумерная структура, где порядок строк и столбцов может быть неважен (в зависимости от интерпретации). Часто рассматривается как множество строк, где каждая строка – это тип-произведение.
        *   Пример: $$ \text{DataSet} = \text{Tab}(\mathbb{R}) $$ (таблица вещественных чисел) или $$ \text{DataSet} = \text{Seq}(\text{RowFeatures}) $$

---

## 2. Построение архитектуры для различных типов (Vec ↔ Prod, Vec ↔ Sum)

Идея состоит в том, чтобы определить, как преобразовывать структурированные данные в векторное представление (кодировщик, $Type \rightarrow Vec$) и обратно (декодировщик, $Vec \rightarrow Type$) для использования в нейронных сетях.

**2.1. Преобразования Vec ↔ Prod (Вектор ↔ Тип-произведение)**

*   **Prod → Vec:** Преобразование типа-произведения $P = T_1 \times T_2 \times \dots \times T_k$ в вектор $\mathbb{R}^n$.
    1.  Заводим вспомогательные функции (кодировщики) $g_i: T_i \rightarrow \mathbb{R}^{m_i}$ для каждого компонентного типа $T_i$.
    2.  **Два основных подхода:**
        *   **Конкатенация:**
            $$ f(x_1, \dots, x_k) = \text{concat}(g_1(x_1), g_2(x_2), \dots, g_k(x_k)) $$
            При этом $\sum_{i=1}^k m_i = n$.
            *Пример:* В RNN вектор памяти (состояния) конкатенируется с вектором очередного входного элемента.
        *   **Суммирование (с одинаковой размерностью $m_i=n$):**
            $$ f(x_1, \dots, x_k) = \sum_{i=1}^k g_i(x_i) $$
            При этом все $m_i = n$.
            *Пример:* В трансформерах эмбеддинг токена складывается с вектором позиционного кодирования.

*   **Vec → Prod:** Преобразование вектора $\mathbb{R}^n$ в тип-произведение $P = T_1 \times \dots \times T_k$.
    1.  Заводим вспомогательные функции (декодировщики) $g_i: \mathbb{R}^{m_i} \rightarrow T_i$.
    2.  **Инвертирование конкатенации:**
        *   Применить общую функцию-декодер ко всему входному вектору $\mathbb{R}^n \rightarrow \mathbb{R}^n$.
        *   "Нарезать" результат на $k$ подвекторов размерностей $m_1, \dots, m_k$.
        *   Применить к каждому подвектору соответствующую функцию $g_i$.
    3.  **Инвертирование суммирования:**
        *   Применить $k$ различных функций-декодеров $g'_i: \mathbb{R}^n \rightarrow \mathbb{R}^{m_i}$ ко всему входному вектору $\mathbb{R}^n$.
        *   Затем применить $g_i: \mathbb{R}^{m_i} \rightarrow T_i$.
        *   Или напрямую применить $g_i: \mathbb{R}^n \rightarrow T_i$ к входному вектору $\mathbb{R}^n$ для получения каждого компонента $x_i$.

**2.2. Преобразования Vec ↔ Sum (Вектор ↔ Тип-сумма)**

*   **Sum → Vec:** Преобразование типа-суммы $S = T_1 + \dots + T_k$ в вектор $\mathbb{R}^n$.
    1.  Заводим вспомогательные функции (кодировщики) $g_i: T_i \rightarrow \mathbb{R}^n$ для каждого варианта $T_i$.
    2.  Если на вход пришло значение $x$ типа $T_j$ (т.е. $x: T_j$), то выходным вектором будет $g_j(x)$.
    3.  Выбор активной функции $g_j$ осуществляется на основе сопоставления с образцом (pattern matching) или тега типа-суммы.

*   **Vec → Sum:** Преобразование вектора $\mathbb{R}^n$ в тип-сумму $S = T_1 + \dots + T_k$.
    1.  Заводим:
        *   Функцию выбора/классификации $c: \mathbb{R}^n \rightarrow \{1, \dots, k\}$ (предсказывает, какой тип $T_j$ является наиболее вероятным).
        *   $k$ вспомогательных функций-декодеров $g_j: \mathbb{R}^n \rightarrow T_j$.
    2.  Предсказанный тип $T_j$ определяется как $j^* = \text{argmax}_j (c(\text{входной вектор}))_j$.
    3.  Выходное значение будет $g_{j^*}(\text{входной вектор})$.
    4.  **Проблема с $\text{argmax}$:** Эта операция "жёсткая" и недифференцируема.
        *   **Решение 1 (Soft Argmax / Взвешенное суммирование):**
            Использовать $w = \text{softargmax}(c(\text{входной вектор}))$ или $w = \text{softmax}(c(\text{входной вектор}))$.
            Затем вычислить несколько ветвей $g_j(\text{входной вектор})$ для всех $j$ и скомбинировать результаты с весами $w_j$. (Например, если $T_j$ - это векторы, то $\sum w_j g_j(\text{входной вектор})$).
        *   **Решение 2 (Обучение с подкреплением):**
            Пусть $c(x)$ предсказывает "качество" или вероятность выбора каждой ветви $T_j$. Выбор $j$ осуществляется стохастически (например, сэмплирование по предсказанным вероятностям или $\epsilon$-greedy). Модель $c(x)$ и декодеры $g_j$ обучаются с помощью методов RL.
        *   **Решение 3 (Классификация + Декодирование):**
            Если на этапе обучения известен истинный тип $T_j$ (верный выбор $j$), то можно напрямую обучать $c(x)$ как классификатор (например, с помощью CrossEntropy), а $g_j(x)$ - как условный декодер для этого класса.

**2.3. Преобразования для базовых и коллекционных типов**
*   **Vec → Vec:** Обычная архитектура нейронной сети (например, MLP, Трансформер).
*   **Vec → Eps:** Бесполезная функция (проекция на пустой тип, т.е. отбрасывание информации).
*   **Eps → Vec:** Обучаемый вектор констант (эмбеддинг для "ничего").
*   **Eps → Sum:** Если варианты в Sum - это действия, то это задача о многоруком бандите. Обучаемый вектор (Eps → Vec) отображается на оценки для каждого действия (варианта Sum).
*   **Для других типов (Seq, Set, Img, Tab):** Обычно сначала генерируется векторное представление (Eps → Vec), а затем из этого вектора получается нужный структурированный тип (Vec → Type).
*   **Для изображений (Eps → Img):** Можно подобрать архитектуру (например, генеративно-состязательные сети - GAN, декодеры вариационных автокодировщиков - VAE) так, чтобы она требовала меньше параметров, чем число пикселей в выходном изображении, используя свойства изображений (локальность, иерархичность).

**2.4. Преобразования для сложных типов (Img, Seq, Set, Tab)**
*   **Img и Seq:** Уже рассмотрены в предыдущих лекциях (CNN для Img, RNN/Transformers для Seq).
*   **Множество (Set):**
    *   **Set(X) → Vec (Агрегация):**
        1.  Перевести каждый элемент $x \in \text{Set}(X)$ в векторное представление с помощью функции $g: X \rightarrow \text{Vec}$.
        2.  Сагрегировать полученный набор векторов в один вектор с помощью инвариантной к порядку операции:
            *   Сумма: $\sum g(x_i)$
            *   Среднее: $\frac{1}{|S|} \sum g(x_i)$
            *   Средневзвешенное (если есть веса элементов)
            *   Максимум (покомпонентный)
            *   Использование механизма self-attention (например, как в Set Transformer).
    *   **Set(X) → Set(Y) (Поэлементное преобразование или более сложное):**
        1.  **Независимо:** Применить функцию $h: X \rightarrow Y$ к каждому элементу множества.
        2.  **С контекстом:** Применить функцию $h: X \rightarrow Z$ для получения эмбеддингов, затем агрегировать их в общий контекстный вектор (как в Set(X) → Vec), а затем использовать этот контекст для преобразования каждого элемента (например, $h': X \times Z \rightarrow Y$).
        3.  **Самовнимание (Self-Attention):** Элементы множества взаимодействуют друг с другом для получения контекстуализированных представлений, которые затем преобразуются в тип Y.
    *   **Vec → Set(X):** Можно использовать архитектуру для Vec → Seq(X) и затем рассматривать результат как множество (если порядок не важен). Или генерировать элементы множества один за другим, возможно, с условием на уже сгенерированные.
*   **Таблица (Tab):**
    Таблицу можно рассматривать как произведение множеств строк и множеств столбцов (или наоборот):
    $$ \text{Tab}(V) \approx \text{Set}_{\text{row}}(\text{Set}_{\text{col}}(V)) \times \text{Set}_{\text{col}}(\text{Set}_{\text{row}}(V)) $$
    Это означает, что можно применять агрегацию по строкам, а затем по столбцам (или наоборот), или использовать более сложные модели, учитывающие взаимодействия.
    Часто таблицы обрабатываются как последовательности строк (если порядок строк важен) или как множества строк. Для каждой строки (которая является типом-произведением признаков) можно применить Prod → Vec.

---

## 3. Частные случаи архитектур для ADT

**3.1. Объект как произведение признаков**
*   Тип объекта: $$ \text{Iris} = \text{SepalLength} \times \text{SepalWidth} \times \text{PetalLength} \times \text{PetalWidth} $$
*   Каждый признак $x_j$ (скаляр) умножается на соответствующий ему вектор весов $w_j$ (обучаемый эмбеддинг признака или столбец в матрице весов).
*   Результаты суммируются: $ \text{representation} = \sum_j x_j w_j $.
*   Если $x$ – это вектор-строка признаков, а $W$ – матрица, где столбцы – это $w_j$, то это эквивалентно умножению $xW$. Если каждый $x_j$ проецируется в вектор, а затем результаты суммируются, это похоже на линейный слой, где каждый входной признак имеет свой набор весов для каждого выходного нейрона.

**3.2. Объект как множество признаков**
*   Тип признака (категориальный):
    $$ \text{IrisFeature} = \text{SepalLengthValue} + \text{SepalWidthValue} + \dots $$
    (где `SepalLengthValue` – это тип, представляющий конкретное значение длины чашелистика, а не сам числовой тип `Real`).
*   Тип объекта: $$ \text{Iris} = \text{Set}(\text{IrisFeature}) $$ (каждый объект – это набор имеющихся у него признаков-значений).
*   Каждому возможному значению признака $f \in \text{IrisFeature}$ сопоставляется обучаемый вектор-эмбеддинг $v_f$.
*   Представление объекта – это сумма эмбеддингов тех признаков, которые у него есть: $ \sum_{f \in \text{object_features}} v_f $.
*   Это эквивалентно умножению матрицы эмбеддингов на one-hot (или multi-hot) вектор признаков объекта. Похоже на подход "мешок слов" (bag-of-words).

**3.3. Пропущенные значения**
*   Признак с возможным пропуском можно кодировать как тип-сумму:
    $$ \text{WeakSepalLength} = \text{SepalWidthValue} + \text{Eps} $$
    (где `SepalWidthValue` – тип для существующего значения, `Eps` – для пропуска).
*   **Подход (Sum → Vec):**
    *   Если значение есть (например, 5.1), то используется кодировщик $g_1: \text{SepalWidthValue} \rightarrow \mathbb{R}^n$. Например, $g_1(v_1) = v_1 \cdot a_1$.
    *   Если значение пропущено (Eps), то используется кодировщик $g_2: \text{Eps} \rightarrow \mathbb{R}^n$. Например, $g_2() = a_2$.
*   Это эквивалентно добавлению **индикатора пропуска**:
    *   Вход: $(v_1, \text{is_missing_indicator})$.
    *   Выход: $v_1 \cdot a_1 \cdot (1-\text{is_missing_indicator}) + a_2 \cdot \text{is_missing_indicator}$.
    *   На слайде: если значение 5.1, то $5.1 \cdot a_1 + 0 \cdot a_2$. Если пропуск (Eps), то $0 \cdot a_1 + 1 \cdot a_2$ (если $a_2$ - вектор для пропуска).

**3.4. Категории**
*   Категориальный признак можно закодировать как тип-сумму его возможных значений:
    $$ \text{IrisSpecies} = \text{Setosa} + \text{Versicolor} + \text{Virginica} $$
*   **Подход (Sum → Vec):** Каждому значению категории (Setosa, Versicolor, Virginica) сопоставляется свой обучаемый вектор-эмбеддинг.
    *   $g(\text{Setosa}) = v_1$
    *   $g(\text{Versicolor}) = v_2$
    *   $g(\text{Virginica}) = v_3$
*   Это эквивалентно стандартному подходу с **обучаемыми эмбеддингами** (embedding lookup): категориальное значение (или его целочисленный код) используется как индекс для выбора соответствующего вектора из матрицы эмбеддингов.

**3.5. Коллаборативная фильтрация**
*   Задача: предсказать взаимодействие (например, рейтинг) между пользователем (User) и элементом (Item).
*   Тип "Пара": $$ \text{Pair} = \text{User} \times \text{Item} $$
*   Тип "Пользователь" (категориальный): $$ \text{User} = \sum_i \text{User}_i $$
*   Тип "Элемент" (категориальный): $$ \text{Item} = \sum_j \text{Item}_j $$
*   Каждому User$_i$ сопоставляется эмбеддинг $u_i$, каждому Item$_j$ – эмбеддинг $v_j$.
*   Предсказание для пары $(User_i, Item_j)$ часто делается как скалярное произведение их эмбеддингов: $\langle u_i, v_j \rangle$.
*   Это соответствует подходу **матричного разложения** в рекомендательных системах.

**3.6. Конфигурация гиперпараметров (слайд 20)**
*   Пространство поиска гиперпараметров для алгоритма машинного обучения можно представить в виде дерева, где узлы соответствуют выбору компонента (например, типа ядра в SVM) или значения параметра.
*   **Вершина типа И (произведение):** Все дочерние узлы (параметры) должны быть выбраны. (Например, для knn нужны dist, window, k, h).
*   **Вершина типа ИЛИ (сумма):** Должен быть выбран один из дочерних узлов. (Например, для dist в knn выбирается manh, eucl или cheb).
*   **Лист:** Конкретное значение гиперпараметра.
*   Числа у узлов могут означать количество возможных конфигураций для данной подветви.
*   Поиск оптимальной конфигурации гиперпараметров (Hyperparameter Optimization, HPO) – сложная задача.

---

## 4. Графы

Графы – это общая структура данных, состоящая из вершин (узлов) и рёбер (связей между узлами). Многие типы данных можно представить в виде графов.

**4.1. Примеры графовых данных**
*   **Суперпиксели (Superpixels):** Группировка смежных пикселей изображения со схожими характеристиками. Суперпиксели можно рассматривать как узлы графа, а их смежность – как рёбра.
*   **Облако точек (Point Cloud):** Набор точек в 3D-пространстве (например, от LiDAR). Можно построить граф на основе близости точек.
*   **Молекулы:** Атомы – узлы, химические связи – рёбра.
*   **Социальные сети:** Люди – узлы, дружба/подписка – рёбра.
*   **Код программы:** Элементы синтаксического дерева или графа потока управления – узлы, зависимости – рёбра.

**4.2. Подходы к обработке графов с помощью нейронных сетей (Graph Neural Networks, GNNs)**

*   **Случайный обход (Random Walks):**
    *   Генерируются случайные пути (последовательности вершин) на графе.
    *   Эти последовательности затем обрабатываются методами для последовательностей (например, RNN, Word2Vec-подобные модели для получения эмбеддингов узлов, как в DeepWalk, node2vec).
    *   Получается множество последовательностей: $\text{Set}(\text{Seq}(X))$, где $X$ – тип узла.

*   **Передача сообщений (Message Passing):**
    *   Основная парадигма для многих GNN.
    *   Каждая вершина $v_i$ имеет векторное представление (эмбеддинг) $\text{vec}_l[v_i]$ на слое $l$.
    *   На каждом слое эмбеддинг вершины обновляется на основе агрегированной информации от ее соседей на предыдущем слое:
        $$ \text{vec}_{l}[v_i] = f_l \left( \sum_{(v_i, v_j) \in E} g_l(\text{vec}_{l-1}[v_j], \text{edge_features}(v_i,v_j)) , \text{vec}_{l-1}[v_i] \right) $$
        Или, как на слайде, более общая форма:
        $$ \text{vec}_l[v_i] = f_l \left( \text{AGGREGATE}_{ (v_j, v_i) \in E \text{ or } v_j \in \text{Neighbors}(v_i) } \{ g_l(\text{vec}_{l-1}[v_j]) \} \right) $$
        где $f_l$ – функция обновления, $g_l$ – функция сообщения (часто просто линейное преобразование или MLP), AGGREGATE – функция агрегации (сумма, среднее, макс).
    *   Предполагается, что всегда существует ребро из вершины в себя (для учета собственной информации).
    *   Рёбра тоже могут иметь метки (вектора признаков), которые учитываются функцией $g_l$.
    *   Для рёбер разного типа можно применять разные функции $g_l$.

*   **Добавление фиктивных рёбер:**
    *   Чтобы информация распространилась между дальними вершинами, число слоев (передач сообщений) должно быть не меньше диаметра графа.
    *   Для сокращения пути информации (уменьшения числа необходимых слоев) можно добавить фиктивные рёбра (например, между вершинами на расстоянии 2, или к "глобальному" узлу).

*   **Матричные методы (Спектральные GNN):**
    *   Используют матрицу смежности графа $A$ или матрицу Лапласа $L = D - A$ (где $D$ – диагональная матрица степеней вершин).
    *   Свёртка на графе может быть определена в спектральной области через собственные векторы Лапласиана (например, ChebNet, GCN).
    *   GCN (Graph Convolutional Network) слой:
        $$ H^{(l+1)} = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)}) $$
        где $\tilde{A} = A + I$ (матрица смежности с добавленными петлями), $\tilde{D}$ – диагональная матрица степеней для $\tilde{A}$, $H^{(l)}$ – матрица активаций узлов на слое $l$, $W^{(l)}$ – матрица весов.

---

## 5. Частные случаи архитектур для графов

Различные типы данных можно представить как частные случаи графов, и GNN-подобные подходы могут быть адаптированы для них.

*   **Множество ($Set(X)$):**
    *   Можно закодировать как **полный граф (all-to-all)**, где каждый элемент связан с каждым другим.
    *   Или как **двудольный граф (all-to-one)**, где все элементы связаны с одним агрегирующим узлом "y".
    *   Или как **разреженный граф (sparse)**, если есть априорная информация о связях.
    *   При агрегации информации для вершины (если это all-to-all) используется агрегация множества (сумма, среднее, self-attention).

*   **Таблица ($Tab(X)$):**
    *   Может быть представлена как **двумерная решетка (grid graph)**, где узлы – ячейки таблицы, а рёбра соединяют соседние ячейки.
    *   Может быть представлена как **двудольный граф** между строками и столбцами.
    *   "all-to-all" на слайде (для таблицы) может означать, что каждая ячейка связана со всеми другими ячейками, или каждая строка/столбец агрегирует информацию со всех других строк/столбцов.
    *   "all-to-one" для таблицы: агрегация всех ячеек в одно представление.

*   **Последовательность ($Seq(X)$):**
    *   Линейный граф, где каждый элемент связан только со следующим (и/или предыдущим для двунаправленных моделей).
    *   "The quick brown fox jumps over the lazy dog" – каждый узел (слово) передает информацию следующему.

*   **Изображение ($Img(X)$):**
    *   Двумерный решетчатый граф.
    *   Если представить, что каждый пиксель связан с собой и 8-ю ближайшими соседями (как в ядре 3x3), то операция передачи сообщений на таком графе эквивалентна **свёртке 3x3**.
    *   Передача сообщений на графе изображения иногда так и называется "свёрткой на графе".
    *   "A pixel depends on 8 neighbors and itself" (для обновления состояния пикселя).
    *   "A pixel depends on 9 pixels from previous layer" (если предыдущий слой тоже решетка, и применяется ядро 3x3).

*   **Код программы:**
    *   Может быть представлен как абстрактное синтаксическое дерево (AST) или граф потока управления (CFG).
    *   Для кода часто недостаточно ациклического дерева, так как переменная определяется ее использованием (и наоборот).
    *   Это может потребовать двунаправленных рёбер или более сложных GNN архитектур (например, Gated Graph Neural Networks, GGNN).

---
