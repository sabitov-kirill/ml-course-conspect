# Лекция 1: Глубокое обучение

**План лекции:**
1.  Введение в машинное и глубокое обучение
2.  Базовое обучаемое преобразование (матричное преобразование)
3.  Функции активации
4.  Остаточные связи (Residual connections)
5.  Декомпозиция моделей (Перенос знаний, дообучение, LoRA, прореживание)
6.  Kolmogorov-Arnold Networks (KAN)

---

## 1. Введение в машинное и глубокое обучение

**1.1. Из чего состоит машинное обучение?**
Задача машинного обучения (МО) состоит из трёх ключевых частей:
*   **Набор данных (Dataset):** Выборка объектов из реального мира.
*   **Модель (Model):** Функция или структура, которая пытается уловить закономерности в данных.
*   **Функция ошибки (Loss Function / Error Function):** Метрика, оценивающая, насколько хорошо модель справляется с задачей. Минимизация этой функции является целью обучения.

| Реальный мир                      | Машинное обучение                         |
| :-------------------------------- | :---------------------------------------- |
| Потенциально бесконечное множество объектов | Набор данных (выборка)                    |
| Закономерность или зависимость   | Модель или функция                        |
| Бизнес-метрика                    | Эмпирический риск или функция ошибки     |

*Эмпирический риск* – это средняя ошибка модели на обучающей выборке, аппроксимация истинного риска (ожидаемой ошибки на всех возможных данных).

**1.2. Глубокое обучение (Deep Learning, DL) vs. "Классическое" Машинное обучение (ML)**

| Признак                 | Машинное обучение (ML)                                      | Глубокое обучение (DL)                                                              |
| :---------------------- | :---------------------------------------------------------- | :---------------------------------------------------------------------------------- |
| **Тип данных**          | Преимущественно табличные данные                             | Сложные сырые данные: изображения, текст, аудио, видео и т.д.                       |
| **Модель (Функция)**    | Часто фиксированная, относительно простая функция (линейные модели, деревья решений) | Почти¹ произвольная, сложная, многослойная функция (нейронные сети)                 |
| **Представление признаков** | Требуется ручное конструирование признаков (feature engineering) | Автоматическое извлечение иерархических признаков из сырых данных                 |
| **Функция ошибки**      | Эмпирический риск                                           | Часто также эмпирический риск, но оптимизация может быть сложнее из-за невыпуклости |

¹ *Примечание:* Функция должна быть дифференцируемой (или почти везде дифференцируемой), чтобы её можно было обучить методами градиентного спуска.

---

## 2. Базовое обучаемое преобразование

**2.1. Линейная классификация и её ограничения**
Линейный классификатор пытается разделить классы с помощью гиперплоскости. Решение принимается на основе знака скалярного произведения:
$$ y = \text{sign}(\langle x, \theta \rangle + \theta_0) $$
где $x$ – вектор признаков объекта, $\theta$ – вектор весов, $\theta_0$ – сдвиг (bias).
*   **Проблема:** Даже простые логические функции, такие как XOR (исключающее ИЛИ), не являются линейно разделимыми.
    *   OR: Линейно разделима.
    *   XOR: Нелинейно разделима.

**2.2. "Линейность" булевых функций**
Не путать линейную разделимость с линейностью в смысле алгебры Жегалкина (полиномы над GF(2)). Из 16 булевых функций от двух переменных, линейно разделимыми являются 14. Неразделимы XOR и XNOR (эквивалентность).

**2.3. Композиция логических функций (Построение XOR)**
Нелинейно разделимые функции можно реализовать комбинацией линейно разделимых элементов (нейронов) в несколько слоёв. Например, XOR можно представить как:
$$ x_1 \oplus x_2 = (x_1 \lor x_2) \land \neg(x_1 \land x_2) $$
Используя пороговую функцию активации $[z > 0]$ (равна 1 если $z>0$, иначе 0):
*   AND: $$x_1 \land x_2 = [x_1 + x_2 - 3/2 > 0]$$
*   OR: $$x_1 \lor x_2 = [x_1 + x_2 - 1/2 > 0]$$
*   NOT: $$\neg x = [-x + 1/2 > 0]$$
*   XOR (один из вариантов): $$x_1 \oplus x_2 = [x_1 + x_2 - 2x_1x_2 - 1/2 > 0]$$ (для $x_i \in \{0,1\}$)
Это основа для многослойных нейронных сетей.

**2.4. Многослойная нейронная сеть (Multilayer Perceptron, MLP)**
Состоит из:
*   **Входного слоя (Input Layer):** Принимает признаки объекта.
*   **Скрытых слоёв (Hidden Layers):** Выполняют промежуточные вычисления. Каждый нейрон скрытого слоя обычно применяет линейное преобразование к выходам предыдущего слоя, добавляет сдвиг и пропускает результат через нелинейную функцию активации.
*   **Выходного слоя (Output Layer):** Формирует предсказание модели.

Интерактивный пример: [playground.tensorflow.org](https://playground.tensorflow.org/)

**2.5. Языковые нюансы (Терминология)**
*   **Синонимы MLP:** Multilayer Neural Network, Fully Connected Neural Network (FCNN), Feedforward Neural Network (FNN), Deep Neural Network (DNN), Artificial Neural Network (ANN).
*   **Матричное преобразование (один слой):**
    $$ X_{in} \xrightarrow{A_i, b_i, \sigma} X_{out} $$
    1.  Умножение на матрицу весов: $$M_i = X_i \cdot A_i$$
    2.  Прибавление вектора сдвига: $$S_i = M_i + b_i$$
    3.  Применение нелинейной функции активации (поэлементно): $$X_{i+1} = \sigma(S_i)$$
    *   $A_i$ – матрица весов (weights).
    *   $b_i$ – вектор сдвигов (biases).
    *   $\sigma$ – нелинейная функция активации.
*   **Термин "Слой":** Исторически архитектуры были строго послойными. Современные архитектуры могут быть произвольными ациклическими графами (DAGs), где "слой" – это скорее блок вычислений.

**2.6. Пакетное вычисление (Batch Computation)**
Для эффективности вычислений (особенно на GPU) и более стабильного обучения, объекты обрабатываются пакетами (батчами).
*   Несколько скалярных произведений $\rightarrow$ умножение вектора на матрицу.
*   Несколько умножений векторов на матрицу $\rightarrow$ умножение матрицы на матрицу.
*   **Тензор:** Многомерный массив (обобщение матриц).

**2.7. Пересчёт производной для одного преобразования (Основа Backpropagation)**
Пусть $i$-е преобразование: $Y_i = f(X_i \cdot A_i + b_i)$.
Обозначим $M_i = X_i \cdot A_i$ и $S_i = M_i + b_i = X_i \cdot A_i + b_i$.
Тогда $Y_i = f(S_i)$.
Производные функции потерь $L$ по параметрам и входам (для одного образца, $\odot$ - поэлементное произведение (Адамара)):
*   $$\frac{\partial L}{\partial S_i} = \frac{\partial L}{\partial Y_i} \odot f'(S_i)$$
*   $$\frac{\partial L}{\partial b_i} = \frac{\partial L}{\partial S_i}$$ (суммируется по батчу, если $b_i$ общий)
*   $$\frac{\partial L}{\partial M_i} = \frac{\partial L}{\partial S_i}$$
*   $$\frac{\partial L}{\partial A_i} = X_i^T \cdot \frac{\partial L}{\partial M_i}$$
*   $$\frac{\partial L}{\partial X_i} = \frac{\partial L}{\partial M_i} \cdot A_i^T$$

**2.8. Анализ матричного преобразования**
*   Позволяет гибко изменять размерность векторов.
*   Сложность модели контролируется числом слоёв и размерами промежуточных векторов.
*   Хорошо параллелизуется.
*   **Ключевое:** Без нелинейных функций активации ($\sigma$) композиция матричных преобразований эквивалентна одному линейному преобразованию. Нелинейность необходима для увеличения выразительной мощности.

**2.9. Низкоранговое произведение матриц**
Вместо одного преобразования $n \rightarrow m$ с матрицей $A \in \mathbb{R}^{n \times m}$, можно использовать два: $n \rightarrow k \rightarrow m$ с матрицами $A_1 \in \mathbb{R}^{n \times k}$ и $A_2 \in \mathbb{R}^{k \times m}$.
Если $k(n+m) < nm$, это уменьшает число параметров. Это "бутылочное горлышко" (bottleneck layer).

**2.10. Преимущества и проблемы нескольких слоёв**
*   **Хорошо:**
    *   **Универсальная теорема аппроксимации (Cybenko, 1989):** Сеть с одним скрытым слоем и достаточным числом нейронов может аппроксимировать любую непрерывную функцию с любой точностью.
    *   Глубокие сети (много слоёв) часто могут быть более эффективны по числу параметров для сложных функций, чем широкие и неглубокие, так как они могут изучать иерархические представления признаков.
*   **Плохо:**
    1.  **Проблема симметрии:** Перестановка нейронов в скрытом слое не меняет функцию, что приводит к множеству эквивалентных минимумов функции ошибки.
        *   *Решение:* Случайная инициализация весов.
    2.  **Проблема затухания градиента (Vanishing Gradient):** В глубоких сетях градиент может экспоненциально уменьшаться при распространении к начальным слоям, замедляя их обучение.
    3.  **Проблема взрыва градиента (Exploding Gradient):** Градиент может экспоненциально расти, приводя к нестабильности обучения.
        *   *Решения для 2 и 3:*
            *   Специальные архитектуры (LSTM, ResNet).
            *   Функции активации, не склонные к насыщению (ReLU).
            *   Нормализация активаций (Batch Normalization).
            *   Тщательная инициализация весов (Xavier, He).
            *   Подрезка градиента (Gradient Clipping):
                *   Глобальная: если $$\|g\| > c \Rightarrow g_{new} = c \cdot \frac{g_{old}}{\|g\|}$$
                *   Локальная: если $$|g_i| > c \Rightarrow g_i = c \cdot \text{sign}(g_i)$$

---

## 3. Функции активации

Функции активации вводят нелинейность в модель, позволяя ей изучать сложные зависимости.

**3.1. Исторический контекст**
Изначально использовалась ступенчатая функция: $$f(x) = \begin{cases} 1 & \text{if } x \ge 0 \\ 0 & \text{if } x < 0 \end{cases}$$. Она не дифференцируема в нуле. Современные функции активации обычно гладкие.

**3.2. Sigmoid (Логистическая функция)**
$$ \sigma(x) = \frac{1}{1+e^{-x}} $$
*   Выход: $(0, 1)$.
*   Производная: $$\sigma'(x) = \sigma(x)(1-\sigma(x))$$. Максимальное значение $0.25$.
*   Проблемы:
    *   Насыщение: при больших $|x|$ производная близка к нулю, что ведёт к затуханию градиента.
    *   Выход не центрирован около нуля.

**3.3. Tanh (Гиперболический тангенс)**
$$ \tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1 $$
*   Выход: $(-1, 1)$.
*   Производная: $$\tanh'(x) = 1 - \tanh^2(x)$$. Максимальное значение $1$.
*   Преимущества над Sigmoid: выход центрирован около нуля, что может ускорять сходимость.
*   Проблемы: также страдает от насыщения.

**3.4. ReLU (Rectified Linear Unit)**
$$ \text{ReLU}(x) = \max(0, x) $$
*   Выход: $[0, \infty)$.
*   Производная: $$\text{ReLU}'(x) = \begin{cases} 1 & \text{if } x > 0 \\ 0 & \text{if } x \le 0 \end{cases}$$ (в $x=0$ обычно берут 0 или 1).
*   Преимущества:
    *   Вычислительно эффективна.
    *   Не насыщается для $x > 0$, помогает бороться с затуханием градиента.
    *   Приводит к разреженности активаций (многие нейроны "молчат").
*   Недостатки:
    *   Выход не центрирован около нуля.
    *   "Мёртвые ReLU": если нейрон всегда получает на вход отрицательные значения, он перестаёт активироваться и обучаться.

**3.5. Модификации ReLU**
*   **Leaky ReLU:** $$ \text{LReLU}(x) = \begin{cases} x & \text{if } x > 0 \\ \alpha x & \text{if } x \le 0 \end{cases} $$ (где $\alpha$ – малая константа, например, 0.01). Позволяет небольшой градиент при $x<0$.
*   **Parametric ReLU (PReLU):** $\alpha$ является обучаемым параметром.
*   **Exponential Linear Unit (ELU):**
    $$ \text{ELU}_\alpha(x) = \begin{cases} x & \text{if } x \ge 0 \\ \alpha(e^x - 1) & \text{if } x < 0 \end{cases} $$
    Стремится к $-\alpha$ при $x \rightarrow -\infty$. Может давать отрицательные выходы, центрируя активации.

**3.6. Softplus**
$$ \text{Softplus}(x) = \ln(1 + e^x) $$
*   Гладкая аппроксимация ReLU. Производная: $$\text{Softplus}'(x) = \sigma(x)$$.
*   Вычислительно дороже ReLU.

**3.7. Swish и Mish**
*   **Swish (или SiLU - Sigmoid Linear Unit):** $$ \text{Swish}_\beta(x) = x \cdot \sigma(\beta x) $$ (часто $\beta=1$)
*   **Mish:** $$ \text{Mish}(x) = x \cdot \tanh(\text{Softplus}(x)) $$
*   Более сложные, но могут давать лучшие результаты. Немонотонны.

**3.8. Сводная таблица и связи**

| Название                | Функция $f(x)$                                 | Производная $f'(x)$                                     |
| :---------------------- | :--------------------------------------------- | :------------------------------------------------------ |
| Sigmoid                 | $$\frac{1}{1+e^{-x}}$$                         | $$f(x)(1-f(x))$$                                        |
| Tanh                    | $$\tanh(x)$$                                   | $$1 - f(x)^2$$                                          |
| ReLU (Leaky, $\alpha$)  | $$\max(\alpha x, x)$$                          | $$\begin{cases} 1 & x>0 \\ \alpha & x \le 0 \end{cases}$$ |
| SoftPlus                | $$\ln(1+e^x)$$                                 | $$\frac{1}{1+e^{-x}} = \sigma(x)$$                      |
| Arctg                   | $$\text{atan}(x)$$                             | $$\frac{1}{1+x^2}$$                                     |
| SoftSign                | $$\frac{x}{1+|x|}$$                            | $$\frac{1}{(1+|x|)^2}$$                                 |

*Связи (концептуально):*
*   `sign(x)` (ступенька в -1,1) $\xrightarrow{\text{сглаживание}}$ `tanh(x)`
*   `I[x>0]` (ступенька в 0,1) $\xrightarrow{\text{сглаживание}}$ `sigmoid(x)`
*   `I[x>0]` (ступенька в 0,1) $\xrightarrow{\text{интегрирование}}$ `ReLU(x)`
*   `ReLU(x)` $\xrightarrow{\text{сглаживание}}$ `Softplus(x)`

---

## 4. Остаточные связи (Residual Connections)

Идея борьбы с затуханием градиента и облегчения обучения очень глубоких сетей.

**4.1. Ранние идеи**
*   **LSTM (1997):** Внутреннее состояние ячейки ($C_t$) обновляется аддитивно ($C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$), что позволяет градиентам проходить без затухания через множество временных шагов.
*   **Активация с линейным компонентом (LeCun, 1998):** $$f(x) = \tanh(x) + ax$$. Добавление линейного "прохода" помогает избежать "плоских участков" (где градиент мал).
*   **GoogLeNet (2015):** Использование вспомогательных классификаторов на промежуточных слоях для "проталкивания" градиента вглубь сети.

**4.2. ResNet (Residual Network, 2016)**
Ключевая идея: обучать **остаточную функцию** $F(x)$ вместо直接 отображения $H(x)$.
Слой (или блок слоёв) вычисляет $F(x)$, а его выход складывается с входом $x$:
$$ H(x) = F(x) + x $$
Если $x$ и $F(x)$ имеют разную размерность, $x$ преобразуется (например, проекцией).
*   **Преимущества:**
    *   Значительно облегчает обучение очень глубоких сетей (сотни, тысячи слоёв).
    *   Если какой-то блок не нужен, сеть может "научиться" делать $F(x) \approx 0$, и блок превращается в тождественное отображение.
    *   Градиенты могут проходить напрямую через тождественные связи.

---

## 5. Декомпозиция моделей и эффективное обучение

**5.1. Перенос знаний (Transfer Learning)**
1.  Обучить модель ($A$) на большом общем наборе данных ($D_A$) для общей задачи ($T_A$).
2.  Взять часть обученной модели ($A'$, например, все слои кроме последнего) в качестве экстрактора признаков.
3.  Добавить к $A'$ новые слои ($B$) и обучить их (или всю конструкцию $A'+B$) на меньшем, специфичном наборе данных ($D_B$) для специфичной задачи ($T_B$).

**5.2. Дообучение (Fine-tuning)**
При переносе знаний:
1.  **Заморозка:** Параметры предобученной части ($A'$) замораживаются (не обновляются). Обучаются только новые слои ($B$). Это предотвращает "разрушение" хороших весов $A'$ необученными слоями $B$.
2.  **Разморозка (опционально):** После обучения $B$, можно разморозить $A'$ и дообучить всю сеть (часто с меньшей скоростью обучения для $A'$), чтобы адаптировать и предобученные признаки.

**5.3. Low-Rank Adaptation (LoRA)**
Метод параметр-эффективного дообучения. Вместо обновления всех весов $W_0$ предобученной модели, $W_0$ замораживается, и обучается низкоранговое "дельт"-изменение $\Delta W = B \cdot A$, где $A$ и $B$ – две маленькие матрицы.
$$ h' = W_0 x + B A x $$
После обучения $W_0$ и $BA$ можно слить: $W_{new} = W_0 + BA$. Значительно уменьшает количество обучаемых параметров.

**5.4. Послойное обучение (Layer-wise Training)**
Метод обучения, при котором слои добавляются и обучаются последовательно:
1.  Обучить $f_1$.
2.  Заморозить $f_1$, добавить $f_2$, обучить $f_2$ (входы от $f_1$).
3.  И так далее.
Менее популярен сейчас, чем end-to-end обучение, но может быть полезен для инициализации или в специфических случаях.

**5.5. Прореживание (Pruning) сетей**
Удаление избыточных весов, нейронов или целых блоков для уменьшения размера модели и ускорения вывода.
*   **Прореживание ResNet:** Благодаря остаточным связям, если блок $F(x)$ становится бесполезным ($F(x) \approx 0$), его можно удалить. Оценка "бесполезности" делается на валидационном наборе. После прореживания модель обычно дообучается.
*   **Оптимальное прореживание (Optimal Brain Damage, LeCun et al., 1989):**
    Удаляются рёбра (веса), которые наименее важны. Важность параметра $a_i$ (вес) оценивается как:
    $$ L_i = \frac{a_i^2}{[H^{-1}]_{i,i}} $$
    где $H$ – матрица Гессе (матрица вторых производных функции потерь). $[H^{-1}]_{i,i}$ – $i$-й диагональный элемент обратной матрицы Гессе.
    Если вычисление Гессиана затруднительно, используется более простой критерий – **прореживание по величине (magnitude pruning):** удаляются веса с наименьшим абсолютным значением $|a_i|$.

---

## 6. Kolmogorov-Arnold Networks (KAN)

**6.1. Теорема Колмогорова – Арнольда**
Любая непрерывная многомерная функция $f(x_1, \dots, x_n)$ на $[0,1]^n$ может быть представлена в виде:
$$ f(x_1, \dots, x_n) = \sum_{q=0}^{2n} \Phi_q \left( \sum_{p=1}^{n} \phi_{p,q}(x_p) \right) $$
где $\Phi_q$ и $\phi_{p,q}$ – это одномерные непрерывные функции.
*Примечание: в оригинальной теореме верхний предел суммы по $q$ равен $2n+1$.*

**6.2. KAN vs MLP**
*   **MLP:** Линейные преобразования (веса) находятся на рёбрах, а фиксированные нелинейные функции активации – на узлах (нейронах).
    $$ \text{MLP}(x) = (W_L \circ \sigma_{L-1} \circ \dots \circ W_2 \circ \sigma_1 \circ W_1)(x) $$
*   **KAN:** Обучаемые одномерные функции активации находятся на рёбрах, а узлы выполняют простое суммирование.
    $$ \text{KAN}(x) = (\Phi_L \circ \Phi_{L-1} \circ \dots \circ \Phi_1)(x) $$
    где каждый $\Phi_k$ представляет собой слой KAN, где каждая связь имеет свою обучаемую 1D функцию.
    Обучаемые одномерные функции часто реализуются с помощью B-сплайнов:
    $$ \phi(x) = \sum_j c_j B_j(x) $$
    где $B_j(x)$ – базисные сплайн-функции, а $c_j$ – обучаемые коэффициенты.
    Может также использоваться комбинация: $$\phi(x) = \alpha \cdot \text{silu}(x) + \beta \cdot \text{spline}(x)$$

**6.3. Свойства KAN (заявленные)**
*   **Точность и параметр-эффективность:** Могут достигать лучшей точности с меньшим числом параметров по сравнению с MLP на некоторых задачах.
*   **Интерпретируемость:** Поскольку обучаемые функции одномерны, их можно легко визуализировать. Иногда они могут соответствовать известным символьным функциям ($x^2$, $\sin(x)$, и т.д.).
*   **Меньшая склонность к катастрофическому забыванию:** Локальность сплайнов может помочь лучше сохранять знания о предыдущих задачах.
*   **Символьная регрессия:** KAN могут использоваться для поиска символьных формул, описывающих данные, путем идентификации простых функций в выученных сплайнах и их последующей комбинации.

**Процесс символьной регрессии с KAN:**
1.  Обучение KAN с регуляризацией для разреживания (удаления неважных связей).
2.  Прореживание сети.
3.  Идентификация простых символьных функций в оставшихся активных сплайнах (например, $x^2$, $e^x$, $\sin(x)$).
4.  Дообучение аффинных параметров (масштаба и сдвига) для этих символьных функций.
5.  Формирование итоговой символьной формулы.
6.  "Number Snap": Приведение численных коэффициентов к простым дробям или известным константам (например, $3.141 \approx \pi$).

---

# Лекция 2: Глубокое обучение

**План лекции:**
1.  Дропаут (Dropout)
2.  Батчевая нормализация (Batch Normalization)
3.  Инициализация параметров
4.  Практика глубокого обучения (GPU, распараллеливание, квантизация, дистилляция)
5.  Нейронные сети (Обзор и история)

---

## 1. Дропаут (Dropout)

**1.1. Основная идея**
Дропаут – это техника регуляризации, используемая для предотвращения переобучения в нейронных сетях.
*   **Механизм:** Во время обучения на каждой итерации (для каждого мини-батча) выходы нейронов (или сами нейроны) в определённом слое "выключаются" (обнуляются) с некоторой вероятностью $p$ (часто $p=0.5$).
*   Обнулённые нейроны не участвуют в прямом и обратном распространении сигнала на данной итерации.
*   Таким образом, на каждой итерации обучения используется "прореженная" версия сети. Все эти подсети разделяют общие параметры (веса).

**1.2. Эффекты и преимущества**
*   **Уменьшение соадаптации нейронов (co-adaptation):** Нейроны становятся менее зависимыми друг от друга, так как они не могут полагаться на наличие конкретных других нейронов. Каждый нейрон вынужден учиться извлекать более полезные и робастные признаки самостоятельно.
*   **Обучение более робастных представлений:** Сеть учится представлениям, которые устойчивы к отсутствию части информации.
*   **Ансамбль моделей:** Дропаут можно рассматривать как обучение большого ансамбля различных "прореженных" сетей с общими весами. На этапе тестирования (inference) обычно используется вся сеть, но выходы нейронов, к которым применялся дропаут, масштабируются (умножаются на $1-p$ или, что эквивалентно и чаще используется на практике, во время обучения активные нейроны масштабируются на $1/(1-p)$ - "inverted dropout"). Это делается для того, чтобы ожидаемая сумма входов в следующий слой была такой же, как и во время обучения.
*   **Снижение переобучения:** Как видно из графиков, сети без дропаута значительно сильнее переобучаются (ошибка на тестовой выборке выше и менее стабильна).
*   **Увеличение времени обучения:** Дропаут может увеличить количество итераций, необходимых для сходимости (примерно вдвое, если $p=0.5$), так как на каждой итерации обновляется только часть параметров.

---

## 2. Батчевая нормализация (Batch Normalization, BN)

**2.1. Проблема внутреннего ковариационного сдвига (Internal Covariate Shift)**
В процессе обучения нейронной сети параметры каждого слоя постоянно меняются. Это приводит к тому, что распределение активаций (выходов) каждого слоя также меняется на протяжении обучения. Изменения в распределении входов последующих слоёв называются внутренним ковариационным сдвигом. Это замедляет обучение, так как слоям приходится постоянно адаптироваться к изменяющимся входным данным.

**2.2. Основная идея BN**
Поддерживать стабильное распределение активаций для входов каждого слоя путем их нормализации. Для каждой активации (каждого нейрона или канала в сверточных слоях) в пределах текущего мини-батча:
1.  Вычисляется среднее значение и дисперсия активаций по мини-батчу.
2.  Активации нормализуются (приводятся к нулевому среднему и единичной дисперсии).
    $$ \hat{x}_d = \frac{x_d - \mathbb{E}[x_d]}{\sqrt{\mathbb{D}[x_d] + \epsilon}} $$
    где $x_d$ – активация $d$-го нейрона, $\mathbb{E}[x_d]$ и $\mathbb{D}[x_d]$ – среднее и дисперсия по мини-батчу, $\epsilon$ – малая константа для численной стабильности.

**2.3. Параметрический слой для масштабирования и сдвига**
Простая нормализация может ограничить выразительную способность слоя (например, если сигмоида получит на вход значения, близкие к нулю, она будет работать в своей линейной области). Чтобы этого избежать, после нормализации вводится параметрическое преобразование:
$$ \hat{y}_d = \gamma_d \hat{x}_d + \beta_d $$
где $\gamma_d$ и $\beta_d$ – обучаемые параметры масштаба и сдвига для $d$-й активации. Если сеть "захочет", она может выучить $\gamma_d = \sqrt{\mathbb{D}[x_d] + \epsilon}$ и $\beta_d = \mathbb{E}[x_d]$, фактически отменив нормализацию.

**2.4. Алгоритм батчевой нормализации (на этапе обучения)**
**Вход:** Значения активаций $x$ по мини-батчу $\mathcal{B} = \{x_1, \dots, x_m\}$; обучаемые параметры $\gamma, \beta$.
**Выход:** $\{y_i = \text{BN}_{\gamma,\beta}(x_i)\}$.

1.  **Среднее по мини-батчу:** $$ \mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^m x_i $$
2.  **Дисперсия по мини-батчу:** $$ \sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2 $$
3.  **Нормализация:** $$ \hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} $$
4.  **Масштабирование и сдвиг:** $$ y_i \leftarrow \gamma \hat{x}_i + \beta \equiv \text{BN}_{\gamma,\beta}(x_i) $$

**На этапе инференса (тестирования):**
Среднее $\mu$ и дисперсия $\sigma^2$ не вычисляются по текущему батчу, а используются накопленные (экспоненциально сглаженные) статистики со всего обучающего набора:
$$ \hat{x} = \frac{x - \mu_{population}}{\sqrt{\sigma^2_{population} + \epsilon}} $$
$$ y = \gamma \hat{x} + \beta $$

**2.5. Градиентный спуск для батчевой нормализации**
Слой BN является дифференцируемым, и градиенты по его входам $x_i$ и параметрам $\gamma, \beta$ могут быть вычислены с помощью обратного распространения ошибки. Формулы для производных (представлены на слайде 10) учитывают зависимость всех $\hat{x}_i$ от $\mu_{\mathcal{B}}$ и $\sigma^2_{\mathcal{B}}$, которые, в свою очередь, зависят от всех $x_i$ в батче.

**2.6. Анализ и эффекты BN**
*   **Ускорение обучения:** Позволяет использовать более высокие скорости обучения и ускоряет сходимость.
*   **Стабилизация обучения:** Уменьшает проблему внутреннего ковариационного сдвига.
*   **Регуляризация:** Оказывает небольшой регуляризующий эффект (из-за шума, вносимого статистиками мини-батча), что может снизить потребность в других методах регуляризации, таких как Dropout.
*   **Меньшая чувствительность к инициализации весов.**
*   **Сравнение (слайд 11):** Графики показывают, что сети с BN (например, BN-x5, BN-x30) сходятся значительно быстрее и достигают лучшей точности, чем базовые модели (Inception без BN, BN-Baseline).

---

## 3. Инициализация параметров

Правильная инициализация весов критически важна для успешного обучения глубоких нейронных сетей.

**3.1. Стандартные подходы**
*   **Векторы сдвига (biases):** Обычно инициализируются нулями.
*   **Матрицы весов (weights):**
    *   **Нельзя инициализировать нулями:** Если все веса нулевые, все нейроны в слое будут вычислять одно и то же, и градиенты будут одинаковыми, что помешает обучению (проблема симметрии).
    *   **Нельзя инициализировать одинаковыми значениями:** По той же причине симметрии.
    *   **Инициализируются случайными значениями:** Обычно из некоторого распределения (например, равномерного или нормального).

**3.2. Проблема затухания/взрыва активаций и градиентов**
Если веса слишком малы, активации и градиенты могут затухать по мере прохождения через слои. Если веса слишком велики, они могут взрываться. Цель хорошей инициализации – поддерживать дисперсию активаций и градиентов примерно одинаковой на всех слоях.

**3.3. Метод Xavier/Glorot (для tanh-подобных активаций)**
*   **Мотивация:** Предполагается, что функция активации $f(x)$ линейна вблизи нуля (например, $f(x) \approx x$ для $\tanh(x)$ при малых $x$). Идея – поддерживать дисперсию активаций и градиентов постоянной.
*   Пусть $u_d$ – активация на слое $d$, $w_d$ – веса слоя $d$, $n_d$ – количество нейронов на входе слоя $d$ (fan-in), $n_{d+1}$ – количество нейронов на выходе слоя $d$ (fan-out).
*   Для сохранения дисперсии активаций при прямом проходе: $$ \text{Var}(u_{d+1}) = n_d \text{Var}(u_d) \text{Var}(w_d) $$
    Чтобы $\text{Var}(u_{d+1}) = \text{Var}(u_d)$, нужно $n_d \text{Var}(w_d) = 1$.
*   Для сохранения дисперсии градиентов при обратном проходе: $$ \text{Var}\left(\frac{\partial L}{\partial u_d}\right) = n_{d+1} \text{Var}\left(\frac{\partial L}{\partial u_{d+1}}\right) \text{Var}(w_d) $$
    Чтобы дисперсии градиентов были равны, нужно $n_{d+1} \text{Var}(w_d) = 1$.
*   **Компромисс Xavier/Glorot:** $$ \text{Var}(w_d) = \frac{2}{n_d + n_{d+1}} $$
    Веса $w_d$ инициализируются из равномерного распределения:
    $$ w_d \sim U\left[-\sqrt{\frac{6}{n_d + n_{d+1}}}, \sqrt{\frac{6}{n_d + n_{d+1}}}\right] $$
    Или из нормального распределения с $\mu=0$ и $\sigma^2 = \frac{2}{n_d + n_{d+1}}$.

**3.4. Метод He (для ReLU-подобных активаций)**
*   ReLU "обнуляет" половину своих входов, что уменьшает дисперсию примерно вдвое.
*   Для сохранения дисперсии активаций при прямом проходе с ReLU:
    $$ \text{Var}(u_{d+1}) = n_d \text{Var}(u_d) \frac{1}{2}\text{Var}(w_d) $$
    Чтобы $\text{Var}(u_{d+1}) = \text{Var}(u_d)$, нужно $\frac{1}{2} n_d \text{Var}(w_d) = 1$, т.е. $n_d \text{Var}(w_d) = 2$.
*   **Инициализация He:** $$ \text{Var}(w_d) = \frac{2}{n_d} $$
    Веса $w_d$ инициализируются из нормального распределения:
    $$ w_d \sim N\left(0, \sqrt{\frac{2}{n_d}}\right) $$
    Или из равномерного распределения $U\left[-\sqrt{\frac{6}{n_d}}, \sqrt{\frac{6}{n_d}}}\right]$.

*   **Сравнение Xavier vs He (слайды 20-21):** Графики показывают, что для сетей с ReLU инициализация He приводит к более быстрой сходимости и лучшей производительности, особенно для глубоких сетей (например, 30 слоёв), где Xavier может привести к затуханию сигнала.

---

## 4. Практика глубокого обучения

**4.1. Вычисления на GPU (GPGPU - General-Purpose computing on Graphics Processing Units)**
*   Видеокарты (GPU) оптимизированы для массовой параллельной обработки однотипных данных, что идеально подходит для матричных операций в нейронных сетях.
*   Используют SIMD (Single Instruction, Multiple Data) параллелизм: одна инструкция выполняется одновременно над многими элементами данных.
*   **CUDA (Compute Unified Device Architecture):** Проприетарная технология NVIDIA, доминирующая в области вычислений на GPU для глубокого обучения. OpenCL является открытой альтернативой, но имеет меньшую поддержку и экосистему.

**4.2. Архитектура AlexNet (2012)**
*   Знаковая архитектура, популяризовавшая глубокое обучение, выиграв соревнование ImageNet.
*   Особенности: использование ReLU, Dropout, обучение на нескольких GPU (архитектура не помещалась в память одной видеокарты того времени).
*   Имела 5 сверточных слоёв и 3 полносвязных.

**4.3. Распараллеливание обучения глубоких моделей**
*   **Data Parallelism (Параллелизм по данным):**
    *   Модель полностью копируется на каждое из $N$ устройств (GPU).
    *   Каждое устройство обрабатывает свою часть (1/N) батча данных.
    *   После вычисления градиентов на каждом устройстве, они агрегируются (например, усредняются) и параметры модели обновляются синхронно на всех устройствах.
    *   Наиболее распространенный тип.
*   **Model Parallelism (Параллелизм по модели) / Tensor Parallelism:**
    *   Части самой модели (слои или даже отдельные тензоры весов) распределяются по разным устройствам.
    *   Данные проходят последовательно через части модели на разных устройствах.
    *   Используется, когда модель слишком велика, чтобы поместиться на одном GPU.
*   **Pipeline Parallelism (Конвейерный параллелизм):**
    *   Модель делится на последовательные стадии (chunks), каждая из которых выполняется на отдельном устройстве.
    *   Мини-батч делится на микро-батчи, которые обрабатываются конвейерно.
    *   Проблема: "пузыри" (bubbles) или простои устройств, когда одна стадия ждет другую. Решается с помощью планирования и перекрытия вычислений (например, GPipe, PipeDream).

**4.4. Квантизация (Quantization)**
Снижение точности представления весов и/или активаций модели (например, с 32-битных чисел с плавающей точкой (FP32) до 16-битных (FP16, BFloat16) или 8-битных целых чисел (INT8)).
*   **Цели:**
    *   Уменьшение размера модели.
    *   Ускорение вычислений (операции с меньшей точностью быстрее и менее энергозатратны).
    *   Снижение энергопотребления.
*   **Типы:**
    *   **Симметричная квантизация:** Диапазон вещественных чисел отображается симметрично относительно нуля в целочисленный диапазон.
        $$ r = S \cdot (Q - Z) $$, где $Z$ обычно 0 для симметричной.
    *   **Несимметричная квантизация:** Отображение может быть несимметричным, используется точка нуля $Z$ (zero-point).
        $$ r \approx S \cdot (Q - Z) $$
        где $r$ - вещественное значение, $Q$ - квантованное целочисленное, $S$ - масштаб (scale), $Z$ - точка нуля.
*   **Производительность и энергопотребление (слайд 29):** Операции с INT8/INT4 значительно быстрее и требуют меньше энергии, чем FP32.
*   **Различные числовые типы (слайд 30):**
    *   Int16: Целочисленный.
    *   Float32: Стандартная одинарная точность.
    *   Float16: Половинная точность (меньший диапазон, но быстрее).
    *   Bfloat16 (Brain Floating Point): Половинная точность, но с диапазоном как у Float32 (за счет меньшей точности мантиссы).
    *   TensorFloat32 (TF32): Используется в GPU NVIDIA Ampere, внутренне для умножения матриц; точность выше FP16, скорость выше FP32.
    *   E4M3/E5M2: Форматы с плавающей точкой с еще меньшим количеством бит (например, 8-битные FP8).
*   **Обучение с учётом квантизации (Quantization-Aware Training, QAT):**
    *   Во время прямого прохода веса и/или активации квантуются.
    *   Во время обратного прохода градиенты вычисляются для полных (неквантованных) весов, но пропускаются через "симулятор" квантизации. Часто используется **Straight-Through Estimator (STE):** градиент функции квантования аппроксимируется градиентом тождественной функции (или функции отсечения по диапазону).
    *   Это позволяет модели адаптироваться к потере точности из-за квантизации во время обучения.

**4.5. Дистилляция знаний (Knowledge Distillation)**
Метод переноса знаний от большой, сложной "учительской" модели (teacher model) к меньшей, более быстрой "студенческой" модели (student model).
1.  Обучается большая учительская модель.
2.  Студенческая модель обучается предсказывать:
    *   **"Мягкие" метки (soft labels):** Выходы учительской модели (логиты или вероятности после softmax с "температурой" $T > 1$). Температура "сглаживает" распределение вероятностей, давая больше информации о том, как учитель "думает".
        $$ q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)} $$
        где $z_i$ - логиты.
    *   **"Твердые" метки (hard labels):** Истинные метки из обучающего набора (как при обычном обучении).
3.  Функция потерь для студента – это комбинация потерь на мягких метках (distillation loss) и потерь на твердых метках (student loss):
    $$ L = \alpha L_{soft} + (1-\alpha) L_{hard} $$
    $L_{soft}$ часто является KL-дивергенцией между распределениями учителя и студента, или кросс-энтропией с мягкими метками учителя.

---

## 5. Нейронные сети (Обзор и история)

**5.1. Мозг vs. Искусственная нейронная сеть (ИНС) vs. Обучаемая функция**
*   **Мозг:** Биологическая система, обладающая интеллектом, сознанием и т.д. Нейроны – его структурные единицы.
    *   *Вопросы:* Почему именно нейроны? Как точно их работа аппроксимируется? Как из их работы возникает интеллект?
*   **ИНС:** Математическая модель, вдохновленная структурой мозга. Состоит из связанных "искусственных нейронов".
    *   *Вопросы:* Как обучать ИНС? Как применять их для задач МО?
*   **Обучаемая функция:** Более общее понятие. Это любая параметрическая функция, параметры которой можно настроить (обучить) для решения задачи, минимизируя функцию потерь.
    *   Не обязательно должна иметь "нейронную" архитектуру.
    *   Может не иметь циклов (для прямого распространения).
    *   Обучается градиентным спуском.
    *   Пример: дерево решений - тоже обучаемая функция, но не ИНС в классическом понимании.

**5.2. Искусственный нейрон (Перцептрон)**
*   **Реальный нейрон:** Сигналы от дендритов накапливаются. Разные дендриты имеют разную важность. Активация происходит после достижения некоторого порога.
*   **Искусственный нейрон (модель Мак-Каллока-Питтса, перцептрон Розенблатта):**
    1.  Входные значения $x_i$ суммируются с весами $w_{ji}$: $$ \text{net}_j = \sum_i w_{ji} x_i $$
    2.  Применяется функция активации $f$: $$ o_j = f(\text{net}_j - \theta_j) $$ или $$ o_j = f(\text{net}_j + b_j) $$
        где $\theta_j$ – порог, $b_j$ – сдвиг.

**5.3. Правила обучения "до изобретения" обратного распространения**
*   **Правило Хебба (1949) (для {-1, 1} классификации):** "Neurons that fire together, wire together". Если предсказание $(\langle w^{[k]}, x_{(k)} \rangle)$ и истинная метка $y_{(k)}$ имеют разные знаки, то вес обновляется в сторону $x_{(k)}y_{(k)}$.
    $$ \text{Если } (\langle w^{[k]}, x_{(k)} \rangle) y_{(k)} < 0, \text{ то } w^{[k+1]} := w^{[k]} + \eta x_{(k)} y_{(k)} $$
*   **Правило Розенблатта (1957) (для {0, 1} классификации):**
    $$ w^{[k+1]} := w^{[k]} - \eta (a_{w}(x_{(k)}) - y_{(k)}) x_{(k)} $$
    где $a_w(x_{(k)})$ – выход перцептрона (0 или 1).
*   **Дельта-правило (Уидроу-Хофф, 1960) (для Adaline):** Минимизирует среднеквадратичную ошибку $L(a_w, x) = (\langle w, x \rangle - y)^2$ (где выход $a_w = \langle w, x \rangle$ еще до применения пороговой функции).
    $$ w^{[k+1]} := w^{[k]} - \eta (\langle w^{[k]}, x_{(k)} \rangle - y_{(k)}) x_{(k)} $$
    Это фактически градиентный спуск для линейного нейрона.

**5.4. Краткая история**
*   **1943:** Искусственный нейрон (Мак-Каллок и Питтс).
*   **1949:** Правило обучения нейронов (Хебб).
*   **1957:** Перцептрон (Розенблатт).
*   **1960:** Правило обучения перцептрона Adaline (Уидроу и Хофф), дельта-правило.
*   **1969:** Книга "Персептроны" (Минский и Паперт) – показала ограничения однослойных перцептронов (не могут решить XOR), что привело к "зиме ИИ".
*   **1974 (и ранее):** Алгоритм обратного распространения ошибки (backpropagation) (предложен независимо несколькими исследователями: Уэрбoc, Линнайнмаа, Галушкин). Популяризован Румельхартом, Хинтоном и Уильямсом в 1986.
*   **1980:** Сверточные сети (Neocognitron, Фукусима).
*   **1982:** Рекуррентные сети Хопфилда.
*   **1991:** Проблема затухания градиента (Хохрайтер).
*   **1997:** LSTM-модуль (Хохрайтер и Шмидхубер).
*   **1998:** Градиентный спуск для сверточных сетей (LeNet, ЛеКун и др.).
*   **1998:** Обобщенная аппроксимационная теорема (Универсальная теорема аппроксимации для сетей с различными активациями, не только сигмоидами).
*   **2006:** Глубокие сети доверия (Deep Belief Networks, Хинтон и др.) – начало возрождения глубокого обучения (послойная предобучение).

**5.5. Начало современности**
*   **2012:** Хинтон, Крижевский и Суцкевер предложили Dropout и создали AlexNet, которая с большим отрывом победила на соревновании ImageNet. Это событие считается началом современной эры глубокого обучения.
*   График ILSVRC top-5 error on ImageNet показывает резкое снижение ошибки после 2012 года благодаря DL.

**5.6. Почему "сейчас"? (Факторы успеха DL)**
1.  **Огромные наборы данных (Huge Datasets):** Наличие больших аннотированных датасетов (ImageNet, Wikipedia, Common Crawl и т.д.).
2.  **Мощное оборудование (Powerful Hardware):** Развитие GPU, обеспечивающих необходимой вычислительной мощностью.
3.  **Новые алгоритмы и архитектуры (New Algorithms):** Разработка эффективных архитектур (ResNet, Transformers), техник регуляризации (Dropout, BN), оптимизаторов (Adam) и методов инициализации.

---

# Лекция 3: Автоматическое дифференцирование

**План лекции:**
1.  Сведение задачи обучения к задаче дифференцирования
2.  Дифференцирование составных функций по графу вычислений
3.  Дифференцирование сложных функций (векторных, матричных)
4.  Улучшения градиентного спуска
5.  Методы второго порядка

---

## 1. Сведение задачи обучения к задаче дифференцирования

**1.1. Выбор модели для данных**
Примеры моделей с обучаемыми параметрами $a_0, a_1, \dots$:
*   **Полином:**
    Модель: $$y(x) = a_0 + a_1 \cdot x + a_2 \cdot x^2 + \dots$$
    Обучаемые параметры: $a_0, a_1, a_2, \dots$
*   **Синусоида:**
    Модель: $$y(x) = \sin(a_0 \cdot x + a_1) \cdot a_2 + a_3$$
    Обучаемые параметры: $a_0, a_1, a_2, a_3$
*   **Кривая второго порядка (классификатор):**
    Модель: $$a_0 y^2 + a_1 x^2 + a_2 xy + a_3 y + a_4 x + a_5 > 0$$
    Обучаемые параметры: $a_0, a_1, a_2, a_3, a_4, a_5$
*   **"Мощная" модель (например, нейронная сеть):**
    Если структура зависимости неизвестна, используется гибкая модель, способная аппроксимировать широкий класс функций.
    Модель: $$f(x, y, a) = (p_1, p_2, p_3)$$ (вектор-функция)
    Обучаемые параметры: $a_0, a_1, a_2, \dots$

**1.2. Обучение функции**
*   **Обучаемая функция:** $f(x_1, \dots, x_n, a_1, \dots, a_m) : \mathbb{R}^{n+m} \rightarrow \mathbb{R}^k$
    *   $x_i$ – входные признаки (их $n$ штук).
    *   $a_j$ – обучаемые параметры (их $m$ штук).
    *   $k$ – число предсказываемых (целевых) признаков.
*   **Набор данных:** $D = \{(\vec{x}_i, \vec{y}_i)\}$, где $\vec{x}_i$ – вектор входных признаков $i$-го объекта, $\vec{y}_i$ – вектор истинных целевых значений.
*   **Функция ошибки (потерь, Loss Function):** $E(\hat{y}_1, \dots, \hat{y}_{|D|}, y_1, \dots, y_{|D|})$
    *   $\hat{y}_i$ – предсказанный вектор для $i$-го объекта.
    *   $y_i$ – реальный (истинный) вектор для $i$-го объекта.
    *   Пример: **MSE (Mean Squared Error)**
        $$ \text{MSE}(\hat{Y}, Y) = \frac{1}{|D| \cdot k} \sum_{i=1}^{|D|} \sum_{j=1}^{k} (y_{i,j} - \hat{y}_{i,j})^2 $$
*   **Режим обучения:**
    Цель – найти параметры $\vec{a} = (a_1, \dots, a_m)$, минимизирующие функцию ошибки на обучающем наборе:
    $$ E_{train}(a_1, \dots, a_m) = E(f(\vec{x}_1, \vec{a}), \dots, f(\vec{x}_{|D|}, \vec{a}), y_1, \dots, y_{|D|}) \rightarrow \min_{\vec{a}} $$
    *   Входные данные $\vec{x}_i$ и целевые значения $y_i$ зафиксированы.
    *   Параметры $\vec{a}$ изменяются в процессе минимизации.
*   **Режим предсказания (inference):**
    Используется обученная функция $f_{predict}(x_1, \dots, x_n) = f(x_1, \dots, x_n, a_1^*, \dots, a_m^*)$ с найденными оптимальными параметрами $a^*$.
    *   Входные данные $x$ изменяются от запроса к запросу.
    *   Параметры $\vec{a}^*$ зафиксированы.

**1.3. Мягкая классификация**
Если обучаемая функция $f(x_1, \dots, x_n, a_1, \dots, a_m) = \vec{p} = (p_1, \dots, p_k)$ выдает распределение вероятностей принадлежности к классам.
*   **Наивный подход:** Использовать любую функцию ошибки для регрессии (например, MSE на вероятностях). Не всегда оптимально.
*   **Специализированные функции ошибки для классификации:**
    *   **F1-мера:** Если $CM$ – матрица неточностей (confusion matrix), то $F_1: CM \rightarrow \mathbb{R}$.
        *   $F_1$-мера дифференцируема.
        *   Для вычисления $F_1$-меры не обязательно, чтобы $CM$ содержала только целые числа. Можно использовать "мягкие" предсказания $\vec{p}$ для заполнения $CM$ без округления.
    *   **Перекрёстная энтропия (Cross-Entropy):**
        $$ H(q, p) = - \sum_{i=1}^k q_i \cdot \log p_i $$
        где $q$ – истинное распределение вероятностей (обычно one-hot вектор, например, $q=(0, \dots, 1, \dots, 0)$ для $c$-го класса), $p$ – предсказанное распределение.
        Для one-hot $q$ (где $q_c=1$ и $q_i=0$ для $i \ne c$):
        $$ H(q, p) = - \log p_c $$
        Минимизация перекрестной энтропии эквивалентна максимизации логарифма правдоподобия (Method of Maximum Likelihood).

**1.4. Машинное обучение как задача оптимизации**
1.  Взяли набор данных и модель.
2.  Подставили предсказания модели в функцию ошибки.
3.  **Свели задачу обучения к задаче оптимизации:** найти параметры модели, минимизирующие функцию ошибки.
4.  **Как решать задачу оптимизации?** Чаще всего – **градиентным спуском**.
5.  **Как вычислять градиент?**
    *   Руками (аналитически) – трудоёмко и чревато ошибками для сложных моделей.
    *   **Автоматически** – с помощью техник автоматического дифференцирования.
6.  **Что делать, если функция ошибки невыпуклая?** (Типично для глубокого обучения).
    1.  Паниковать. Смириться. Ничего не делать. (Неконструктивно)
    2.  "Если в теории не работает, то и на практике не должно." (Пессимистично и часто неверно для DL)
    3.  Предположить, что функция ошибки "почти выпуклая" или имеет "хорошие" локальные минимумы.
    4.  Обновлять параметры в градиентном спуске более "хитрыми" методами (адаптивные методы, momentum).
    5.  Попробовать на практике! (Прагматичный подход в DL)

---

## 2. Дифференцирование составных функций по графу вычислений

**2.1. Языковой нюанс (1)**
*   **"Сложная функция" (complex function):** Неточный перевод с английского "composite function".
*   **Составная функция (composite function):** Функция, которая состоит из композиции элементарных функций, для которых мы умеем вычислять производные. Описывается графом вычислений.
*   **Сложная функция (в контексте лекции):** Функция, тип значения которой отличен от скаляра (например, вектор, матрица).

**2.2. Наивное вычисление функции**
Если собрать составную функцию в одно аналитическое выражение, подставляя одну функцию в другую, размер этого выражения может расти экспоненциально. Это неудобно для анализа и дифференцирования.

**2.3. Представление функции сетью (граф вычислений)**
История вычисления функции представляется в виде направленного ациклического графа (DAG):
*   Узлы (вершины) графа – элементарные операции или переменные.
*   Рёбра – зависимости между операциями/переменными.
*   **Статический граф вычислений:** Строится до фактического вычисления функции (например, TensorFlow 1.x, Theano). Позволяет предварительные оптимизации графа.
*   **Динамический граф вычислений:** Строится "на лету" во время вычисления функции (например, PyTorch, TensorFlow 2.x). Более гибкий, удобнее для моделей с переменной структурой (например, RNN).

**2.4. Виды автоматического дифференцирования (AD)**
*   **Прямое (Forward Mode AD):**
    *   Вместе с вычислением значения функции $L$ для каждой вершины $u$ графа вычисляется её производная $\frac{\partial u}{\partial w_i}$ по одной из входных переменных $w_i$.
    *   Для вычисления полного градиента $\nabla_w L$ (по всем $N$ входам) требуется $N$ проходов по графу.
    *   Вычислительная сложность $\mathcal{O}(N \cdot (\text{стоимость вычисления } L))$.
    *   Эффективно, когда число входов мало, а выходов много.
    *   *Языковой нюанс:* На практике термин "forward pass" часто означает просто вычисление значения функции (без производных по входам).
*   **Обратное (Backward Mode AD / Reverse Mode AD) - основа Backpropagation:**
    *   Сначала вычисляется значение функции $L$ (forward pass), запоминая промежуточные значения.
    *   Затем производная $\frac{\partial L}{\partial u}$ вычисляется для всех вершин $u$ в порядке, обратном топологической сортировке графа (от выхода к входам), используя цепное правило.
    *   Требуется $\mathcal{O}((\text{стоимость вычисления } L))$ времени (несколько раз больше, чем просто вычисление $L$, но не зависит от числа входов $N$).
    *   Требуется $\mathcal{O}(M)$ памяти для запоминания $M$ промежуточных значений (активаций), где $M$ - число узлов в графе. Можно сократить потребление памяти, если сохранять не все промежуточные операции, а пересчитывать их ("рематериализация" или "gradient checkpointing").
    *   Эффективно, когда число выходов мало (например, скалярная функция потерь), а входов много (параметры модели). Это типичный случай для обучения нейронных сетей.

**2.5. Элементарные дифференцируемые блоки**
Каждый узел в графе вычислений представляет собой элементарную операцию (блок), для которой известна локальная производная.
*   Пусть блок имеет вход $X$ и выход $Y$.
*   Во время прямого прохода вычисляется $Y = \text{block}(X)$.
*   Во время обратного прохода, получив $\frac{\partial E}{\partial Y}$ (градиент итоговой функции $E$ по выходу блока), блок должен вычислить $\frac{\partial E}{\partial X}$ (градиент по входу блока) и $\frac{\partial E}{\partial W}$ (градиент по параметрам блока $W$, если они есть).
*   Пример для умножения $y = a \cdot b$:
    *   `def mul(a,b): return a*b`
    *   `def d_mul(da, db, a, b, y, dc): return (db + dc*a, da + dc*b)` (упрощенно, если $y$ выход и по нему пришел $dc$)
    *   Если $E = f(y)$, то $\frac{\partial E}{\partial a} = \frac{\partial E}{\partial y} \frac{\partial y}{\partial a} = \frac{\partial E}{\partial y} \cdot b$. И $\frac{\partial E}{\partial b} = \frac{\partial E}{\partial y} \cdot a$.
*   Блок может запоминать $X, Y$ и другие значения для пересчёта производной.
*   Блоку не нужно знать всю функцию $E$, только свой локальный вклад.

**2.6. Композиция блоков**
Блоки могут состоять из других блоков.
Обозначение: $dX$ - это сокращение для $\frac{\partial E}{\partial X}$.
$dX = df_{X \rightarrow Y}(dY)$ – пересчёт производной ("pullback") из $dY$ в $dX$ через блок $f: X \rightarrow Y$.

**2.7. Цепное правило и обобщение**
*   Для скалярной функции одной переменной: $(f(g(x)))' = f'(g(x)) \cdot g'(x)$. В "операторной" форме: $\frac{\partial f}{\partial x} = \frac{\partial f}{\partial g} \frac{\partial g}{\partial x}$.
*   Для функции многих переменных $f(g_1(x), \dots, g_n(x))$ (где $x$ может быть вектором):
    $$ \frac{\partial f(g_1(x), \dots, g_n(x))}{\partial x} = \sum_i \frac{\partial f}{\partial g_i} \frac{\partial g_i}{\partial x} $$
*   Если вершина (промежуточная переменная) используется несколько раз, то градиент, приходящий к ней с разных путей, суммируется. Это следует из обобщенного цепного правила.
*   Для "сложных" функций (векторных, матричных) суммироваться будут не скаляры, а векторы или матрицы (с учетом правил умножения матриц и тензоров).

**2.8. Алгоритм дифференцирования по графу вычислений (Обратный режим)**
1.  **Прямой проход:** Вычисляем составную функцию $Q(\text{входы}, \text{параметры})$, сохраняя граф вычислений $G=(V,E)$. Вершина $v$ напрямую зависит от $u$, если $(u,v) \in E$. Сохраняем значения всех промежуточных узлов.
2.  **Инициализация градиентов:** Для всех вершин $v \in V$ устанавливаем $\frac{\partial Q}{\partial v} = 0$. Для выходной вершины $Q$ (конечный результат, например, функция потерь) устанавливаем $\frac{\partial Q}{\partial Q} = 1$.
3.  **Обратный проход:** Для каждого ребра $(u,v) \in E$ в порядке, обратном топологической сортировке графа (от выхода к входам):
    Обновляем градиент для $u$:
    $$ \frac{\partial Q}{\partial u} \mathrel{+}= \frac{\partial Q}{\partial v} \cdot \frac{\partial v}{\partial u} $$
    (если $v$ – "простая" функция от $u$, т.е. $\frac{\partial v}{\partial u}$ – скаляр или якобиан)
    Или, более общо, если $v = \text{block}(u, \dots)$:
    $$ \frac{\partial Q}{\partial u} \mathrel{+}= \text{pullback}_{\text{block}, u \leftarrow v} \left( \frac{\partial Q}{\partial v} \right) $$
    (где $\text{pullback}$ вычисляет вклад $\frac{\partial Q}{\partial v}$ в $\frac{\partial Q}{\partial u}$)

**2.9. Пример дифференцирования для простых функций (скалярных)**
Пусть $dy$ – это $\frac{\partial Q}{\partial y}$.
*   **Сумма:** $y = \sum_i x_i$. Локальные производные $\frac{\partial y}{\partial x_i} = 1$.
    Обратный проход: $dx_i = \frac{\partial Q}{\partial x_i} = \frac{\partial Q}{\partial y} \cdot \frac{\partial y}{\partial x_i} = dy \cdot 1 = dy$.
*   **Произведение:** $y = \prod_i x_i$. Локальные производные $\frac{\partial y}{\partial x_i} = \prod_{j \ne i} x_j$.
    Обратный проход: $dx_i = dy \cdot \prod_{j \ne i} x_j$.
*   **Применение функции:** $y = f(x)$. Локальная производная $\frac{\partial y}{\partial x} = f'(x)$.
    Обратный проход: $dx = dy \cdot f'(x)$.

**2.10. Пример дифференцирования функции $F = \frac{\sin(x+y)}{x \cdot y}$ (слайд 17)**
Представляется графом:
1.  $S = x+y$
2.  $M = x \cdot y$
3.  $G = \sin(S)$
4.  $H = \text{inv}(M) = 1/M$
5.  $F = G \cdot H$

Обратный проход (начиная с $dF=1$):
*   $dG = dF \cdot H = H = 1/M$
*   $dH = dF \cdot G = G = \sin(S)$
*   $dS = dG \cdot \cos(S) = (1/M) \cos(S)$ (т.к. $G=\sin(S) \Rightarrow \frac{\partial G}{\partial S} = \cos(S)$)
*   $dM = dH \cdot (-1/M^2) + dS \cdot 0$ (т.к. $H=1/M \Rightarrow \frac{\partial H}{\partial M} = -1/M^2$)
    $dM = \sin(S) \cdot (-1/M^2) = -\sin(S)/M^2$
*   $dx = dS \cdot 1 + dM \cdot y = (1/M)\cos(S) - y \sin(S)/M^2$
*   $dy = dS \cdot 1 + dM \cdot x = (1/M)\cos(S) - x \sin(S)/M^2$
Подставляя $S=x+y$ и $M=xy$, получаем аналитические производные.
Визуализация: [alexandrsinitsyn.github.io/ad-visualization/](https://alexandrsinitsyn.github.io/ad-visualization/)

**2.11. Языковой нюанс (2): Обратное распространение ошибки vs. Вычисление производной**
*   **Неправильно (исторически):** Метод обратного распространения *ошибки*.
*   **Правильно:** Вычисление (пересчёт) *производной* (градиента).
*   Формально "ошибка" (разница $y_{true} - y_{pred}$) отвечает на вопрос: "Что нужно прибавить к полученному ответу, чтобы получить верный ответ?"
*   Если функция потерь, например, MSE $L = (y_{true} - y_{pred})^2 / 2$, то её производная по $y_{pred}$ равна $-(y_{true} - y_{pred})$, что совпадает с ошибкой (с точностью до знака). Это верно только для последнего слоя. Для внутренних слоев распространяются производные, а не "ошибки" в этом смысле.
*   На самом деле, используется производная. Теоретически можно вывести метод на основе "распределения разностей", но это сложнее.

---

## 3. Дифференцирование сложных функций (векторных, матричных)

**3.1. Сложное скалярное произведение (внутри циклов)**
Пример: `c[z] += a[x] * b[y]` внутри циклов.
При автоматическом дифференцировании этот код для прямого прохода копируется, и скалярное произведение заменяется на два для обратного прохода:
`da[x] += dc[z] * b[y]`
`db[y] += dc[z] * a[x]`
Важно, чтобы индексы `x, y, z` и условия циклов не зависели от значений `a[x], b[y], c[z]` (т.е. поток выполнения не должен меняться из-за изменения этих значений).
Пример такой операции – свёртка в свёрточных сетях.

**3.2. Произведение матриц**
$C_{[n,m]} = A_{[n,k]} \cdot B_{[k,m]}$ (в индексах $C_{ij} = \sum_p A_{ip} B_{pj}$)
Производные (если $dC$ – это $\frac{\partial L}{\partial C}$):
$$ dA = dC \cdot B^T $$
$$ dB = A^T \cdot dC $$
*   Произведение матриц можно представить как множество скалярных произведений.
*   Пересчёт производной для произведения матриц также является произведением матриц.

**3.3. Другие матричные функции**
Обозначим $dX$ как $\frac{\partial L}{\partial X}$.
*   **Сумма:** $Y = \sum_i X_i \implies dX_i = dY$
*   **Произведение Адамара (покомпонентное):** $Y = X_1 \circ X_2 \circ \dots \circ X_n$
    $$ dX_i = X_1 \circ \dots \circ X_{i-1} \circ dY \circ X_{i+1} \circ \dots \circ X_n $$
    (на самом деле, $dX_i = dY \circ (\prod_{j \ne i} X_j)$ если понимать произведение как Адамара)
    Точнее, если $Y = A \circ B$, то $dA = dY \circ B$ и $dB = dY \circ A$.
*   **Функция (покомпонентное применение):** $Y = f(X)$ (т.е. $Y_{i,j} = f(X_{i,j})$)
    $$ dX = f'(X) \circ dY $$
    где $f'(X)$ также применяется покомпонентно.
*   **Обратная матрица:** $Y = X^{-1}$
    $$ dX = -X^{-T} \cdot dY \cdot X^{-T} = -Y^T \cdot dY \cdot Y^T $$
    (Здесь $\cdot$ - матричное умножение)

**3.4. Пример: линейная регрессия с MSE в матричном виде**
Функция потерь: $E = (X \cdot A - Y)^T \cdot (X \cdot A - Y)$ (сумма квадратов остатков).
Где $X$ – матрица объектов-признаков, $A$ – вектор весов, $Y$ – вектор целевых значений.
Промежуточные переменные:
1.  $P = X \cdot A$
2.  $D = P - Y$
3.  $R = D^T$
4.  $E = R \cdot D$ (скаляр)

Обратный проход (начиная с $dE = [1]$):
*   $dR = dE \cdot D^T = D^T$ (если $dE=1$)
*   $dD = R^T \cdot dE + \text{ (от } dR^T \text{)} = D \cdot dE + dR^T = D + D = 2D$ (учитывая, что $d(D^T)$ приходит в $D$)
    Точнее, $dE = \frac{\partial E}{\partial R} dR + \frac{\partial E}{\partial D} dD$.
    $\frac{\partial E}{\partial R} = D^T$, $\frac{\partial E}{\partial D} = R^T = D$.
    $dD_{fromR} = dR^T = (D^T)^T = D$. $dD_{fromD} = R^T dE = D \cdot 1 = D$.
    Итого $dD = dD_{fromR} + dD_{fromD} = 2D$.
*   $dP = dD \cdot 1 = 2D$
*   $dY = dD \cdot (-1) = -2D$
*   $dX = dP \cdot A^T = 2D \cdot A^T$
*   $dA = X^T \cdot dP = X^T \cdot (2D)$
Итого:
$$ \frac{\partial E}{\partial A} = X^T \cdot (2D) = X^T \cdot 2(X A - Y) $$
Это совпадает с аналитически выведенной производной.

**3.5. Перестановки**
*   **Транспонирование матрицы:** $Y = X^T \implies dX = (dY)^T$.
*   **Перестановка $\pi$ элементов массива $X$:** $Y = \pi X \implies dX = \pi^{-1} dY$, где $\pi^{-1}$ – обратная перестановка.
*   **Сортировка массива:** `arg-sort` (возвращает индексы отсортированного массива) зависит от значений в $X$. Однако, при автоматическом дифференцировании через `arg-sort` градиент обычно не пересчитывается (рассматривается как константа), или используется "нечестная" производная (например, STE), так как операция сортировки не дифференцируема в обычном смысле. Производная для `sort` (которая возвращает отсортированные значения) передается через перестановку, которую определил `arg-sort`.

---

## 4. Улучшения градиентного спуска

Обычный градиентный спуск: $w^{(k+1)} = w^{(k)} - \mu \frac{\partial \mathcal{L}(w^{(k)})}{\partial w}$.

**4.1. Импульсный градиентный спуск (Momentum)**
*   Идея: учитывать "инерцию" движения из предыдущих шагов.
    $$ v^{(k+1)} = \gamma v^{(k)} + \mu \frac{\partial \mathcal{L}(w^{(k)})}{\partial w} $$
    $$ w^{(k+1)} = w^{(k)} - v^{(k+1)} $$
    *   $v$ – вектор "скорости" или "момента".
    *   $\gamma$ – коэффициент момента (обычно ~0.9).
*   **Преимущества:** Ускоряет движение по "оврагам" (длинным узким долинам) и сглаживает осцилляции.
*   **Недостатки:** Может "проскакивать" минимумы из-за инерции.

**4.2. Метод Нестерова (Nesterov Accelerated Gradient, NAG)**
*   Модификация Momentum: градиент вычисляется не в текущей точке $w^{(k)}$, а в точке, куда "предположительно" сместит момент $w^{(k)} - \gamma v^{(k)}$.
    $$ v^{(k+1)} = \gamma v^{(k)} + \mu \frac{\partial \mathcal{L}(w^{(k)} - \gamma v^{(k)})}{\partial w} $$
    $$ w^{(k+1)} = w^{(k)} - v^{(k+1)} $$
*   **Преимущества:** "Заглядывает вперед", что позволяет раньше скорректировать движение. Часто работает лучше Momentum. Сходимость доказана при определенных условиях.

**4.3. Adagrad (Adaptive Gradient Algorithm)**
*   Адаптирует скорость обучения для каждого параметра индивидуально.
*   Для каждого параметра $w_i$:
    $$ w_i^{(k+1)} = w_i^{(k)} - \frac{\mu}{\sqrt{G_{i,i}^{(k)} + \epsilon}} g_{i,(k)} $$
    *   $g_{i,(k)} = \frac{\partial \mathcal{L}(w^{(k)})}{\partial w_i}$.
    *   $G^{(k)}$ – диагональная матрица, где $G_{i,i}^{(k)} = \sum_{j=0}^k (g_{i,(j)})^2$ (сумма квадратов градиентов для $i$-го параметра до шага $k$).
    *   $\epsilon$ – малая сглаживающая константа.
*   **Преимущества:** Устраняет необходимость ручной настройки скорости обучения $\mu$ (часто используется $\mu=0.01$). Хорошо работает с разреженными данными.
*   **Недостатки:** Сумма квадратов градиентов $G_{i,i}^{(k)}$ монотонно растет, что приводит к очень быстрому уменьшению эффективной скорости обучения. Алгоритм может "застрять" и перестать учиться.

**4.4. RMSProp (Root Mean Square Propagation)**
*   Модификация Adagrad для борьбы с проблемой затухания скорости обучения.
*   Вместо полной суммы квадратов градиентов используется экспоненциально затухающее среднее:
    $$ E[g_i^2]^{(k)} = \gamma E[g_i^2]^{(k-1)} + (1-\gamma) (g_{i,(k)})^2 $$
    $$ w_i^{(k+1)} = w_i^{(k)} - \frac{\mu}{\sqrt{E[g_i^2]^{(k)} + \epsilon}} g_{i,(k)} $$
    *   $\gamma$ – коэффициент затухания (обычно 0.9).
*   Знаменатель не растет бесконечно.

**4.5. Анализ размерностей (для адаптивных методов)**
*   Пусть параметры $w$ измеряются в $[c]$, а функция потерь $\mathcal{L}$ – в $[m]$.
*   Тогда градиент $g_i = \frac{\partial \mathcal{L}}{\partial w_i}$ измеряется в $[m]/[c]$.
*   **Обычный ГС:** $w_{new} = w_i - \mu g_i$. Из $[c]$ вычитаем $\mu \cdot [m]/[c]$. Чтобы размерности совпадали, $\mu$ должна иметь размерность $[c]^2/[m]$.
*   **Адаптивный метод (Adagrad, RMSProp):** $w_{new} = w_i - \frac{\mu}{\sqrt{E[g^2]}} g_i$.
    Знаменатель $\sqrt{E[g^2]}$ имеет размерность $[m]/[c]$.
    Дробь $\frac{g_i}{\sqrt{E[g^2]}}$ безразмерна (1).
    Тогда из $[c]$ вычитаем $\mu \cdot 1$. Чтобы размерности совпадали, $\mu$ должна иметь размерность $[c]$.
*   **Метод Ньютона:** (см. ниже) $w_{new} = w - H^{-1} g$. $H$ имеет размерность $[m]/[c]^2$. $H^{-1}$ имеет $[c]^2/[m]$. $H^{-1}g$ имеет $([c]^2/[m]) \cdot ([m]/[c]) = [c]$. Размерности совпадают.

**4.6. Adadelta**
*   Идея: аппроксимировать шаг Ньютона $w^{(k+1)} = w^{(k)} - (\mathcal{L}''(w^{(k)}))^{-1} \mathcal{L}'(w^{(k)})$.
*   $(\mathcal{L}'')^{-1}$ (обратный Гессиан) сложно оценить. Предполагается диагональная структура.
*   Аппроксимация диагонали Гессиана: $\frac{\partial^2 \mathcal{L}}{\partial w_i^2} \approx \frac{|\partial \mathcal{L} / \partial w_i|}{|\Delta w_i^{(k)}|}$ (отношение изменения градиента к изменению параметра).
*   Использует экспоненциально затухающее среднее для квадратов градиентов $E[g^2]$ и квадратов изменений параметров $E[\Delta w^2]$.
    $$ \Delta w_i^{(k)} = - \frac{\text{RMS}[\Delta w]^{(k-1)}}{\text{RMS}[g]^{(k)}} g_{i,(k)} $$
    $$ w_i^{(k+1)} = w_i^{(k)} + \Delta w_i^{(k)} $$
    Где $\text{RMS}[x] = \sqrt{E[x^2] + \epsilon}$.
*   **Преимущества:** Теоретически не требует установки скорости обучения $\mu$. Размерности шага совпадают с размерностями параметров.
*   **Практика:** Скорость обучения $\mu$ все еще может добавляться для улучшения производительности.

**4.7. Adam (Adaptive Moment Estimation)**
*   Сочетает идеи Momentum и RMSProp.
*   Хранит экспоненциально затухающие средние первого момента (среднее градиентов, $m^{(k)}$) и второго момента (среднее квадратов градиентов, $b^{(k)}$).
    $$ m^{(k)} = \beta_1 m^{(k-1)} + (1-\beta_1) g_{(k)} $$
    $$ b^{(k)} = \beta_2 b^{(k-1)} + (1-\beta_2) g_{(k)}^2 $$
*   Коррекция смещения для начальных шагов (когда $m^{(k)}$ и $b^{(k)}$ смещены к нулю):
    $$ \hat{m}^{(k)} = \frac{m^{(k)}}{1-\beta_1^k} $$
    $$ \hat{b}^{(k)} = \frac{b^{(k)}}{1-\beta_2^k} $$
*   Обновление параметров:
    $$ w^{(k+1)} = w^{(k)} - \frac{\mu}{\sqrt{\hat{b}^{(k)}} + \epsilon} \hat{m}^{(k)} $$
*   Популярный и часто используемый по умолчанию оптимизатор. $\beta_1 \approx 0.9, \beta_2 \approx 0.999, \mu \approx 0.001$.

---

## 5. Методы второго порядка

Используют информацию о вторых производных (Гессиан).

**5.1. Метод Ньютона-Рафсона**
Минимизация $\mathcal{L}(w)$ путем итеративного решения $\mathcal{L}'(w)=0$.
Шаг итерации:
$$ w^{(t+1)} = w^{(t)} - \eta_t (\mathcal{L}''(w^{(t)}))^{-1} \mathcal{L}'(w^{(t)}) $$
*   $\mathcal{L}'(w^{(t)})$ – градиент $\mathcal{L}$ в $w^{(t)}$.
*   $\mathcal{L}''(w^{(t)})$ – Гессиан $\mathcal{L}$ в $w^{(t)}$ (матрица вторых производных).
*   $\eta_t$ – шаг (обычно $\eta_t=1$).
*   **Проблема:** Вычисление и обращение Гессиана очень затратно для больших моделей (кубическая сложность по числу параметров $p$: $O(p^3)$ для обращения, $O(p^2)$ для хранения).

**5.2. Квазиньютоновские методы**
Аппроксимируют Гессиан или его обратную матрицу, чтобы избежать прямого вычисления.
*   **Алгоритм BFGS (Broyden — Fletcher — Goldfarb — Shanno):**
    Один из самых популярных квазиньютоновских методов. Итеративно строит аппроксимацию обратного Гессиана $C_{(k+1)} \approx (\mathcal{L}''(w^{(t)}))^{-1}$:
    $$ C_{(k+1)} = (I - \rho_k s_k y_k^T) C_{(k)} (I - \rho_k y_k s_k^T) + \rho_k s_k s_k^T $$
    где $s_k = w^{(k+1)} - w^{(k)}$, $y_k = \nabla \mathcal{L}(w^{(k+1)}) - \nabla \mathcal{L}(w^{(k)})$, $\rho_k = \frac{1}{y_k^T s_k}$.
*   **L-BFGS (Limited-memory BFGS):** Модификация BFGS, требующая меньше памяти. Хранит не всю матрицу $C_{(k)}$, а только несколько последних векторов $s_k$ и $y_k$, из которых $C_{(k)}$ может быть восстановлена.
*   **L-BFGS-B:** Модификация L-BFGS с поддержкой простых ограничений на параметры (box constraints).

**5.3. Natural Gradient Descent (Естественный градиентный спуск)**
*   Идея: пространство параметров не является евклидовым, а имеет риманову метрику, определяемую информацией Фишера. Шаг градиентного спуска делается в этом "естественном" пространстве.
*   Гессиан аппроксимируется **матрицей информации Фишера** $F$.
    $$ w^{(t+1)} = w^{(t)} - \eta_t F^{-1} \nabla \mathcal{L}(w^{(t)}) $$
*   Матрица информации Фишера $F = \mathbb{E}_{x \sim p_{data}} \mathbb{E}_{y \sim p_{model}(y|x;\theta)} [(\nabla_\theta \log p_{model}(y|x;\theta)) (\nabla_\theta \log p_{model}(y|x;\theta))^T ]$.
    Для MSE потерь это эквивалентно $\mathbb{E}_{x \sim p_{data}} [ J^T J ]$, где $J$ - якобиан выходов модели по параметрам.
*   Вычисляется на тренировочном множестве данных.
*   Вместо полной матрицы $F$ можно использовать блочно-диагональную аппроксимацию (например, K-FAC) для снижения вычислительной сложности.

---

# Лекция 5: Свёрточные сети и работа с изображениями

## 1. Особенности изображений

**1.1. Кодирование изображения**
*   Цветное изображение обычно представляется как трёхмерный тензор (матрица) с измерениями: **ширина (width) × высота (height) × глубина (depth/channels)**.
    *   Глубина обычно соответствует цветовым каналам (например, 3 для RGB: Red, Green, Blue).
    *   Для черно-белых изображений глубина равна 1.
*   Простое "вытягивание" матрицы пикселей в один длинный вектор признаков (как для полносвязных сетей) приводит к потере важной пространственной информации и инвариантов (например, инвариантность к сдвигу, локальность связей).

**1.2. Аугментация данных (Data Augmentation)**
Искусственное увеличение размера обучающего набора данных путем применения различных преобразований к существующим изображениям. Это помогает модели стать более робастной и уменьшает переобучение.
*   **Примеры аугментаций:**
    *   **Горизонтальное отражение (Horizontal Flip):** Зеркальное отражение изображения.
    *   **Кадрирование (Crop):** Вырезание случайной или центральной части изображения.
    *   **Изменение контраста (Contrast).**
    *   **Изменение цветового пространства (Hue / Saturation / Value).**
    *   **Размытие (Median Blur, Gaussian Blur).**
    *   **Гамма-коррекция (Gamma).**
    *   Повороты, масштабирование, сдвиги, добавление шума и т.д.

**1.3. Краткая история компьютерного зрения (подходы к извлечению признаков)**
1.  **Handcrafted Features + Predictor:** Ручное создание признаков (SIFT, SURF, HOG) и последующее использование классического классификатора (SVM, Logistic Regression).
2.  **Handcrafted Features + Kernel Embedding + Predictor:** Использование ядерных методов для улучшения представления признаков.
3.  **Handcrafted Features + Kernel Embedding + Metric Learning + Predictor:** Обучение метрики для улучшения разделения классов в пространстве признаков.
4.  **Deep Learning (End-to-End):** Нейронная сеть сама обучается извлекать иерархические признаки из сырых пикселей и выполнять предсказание.

**1.4. Основные концепции свёрточных сетей**
*   **Локальное восприятие (Local Receptive Fields):** Каждый нейрон (или фильтр) обрабатывает только небольшую локальную область входного тензора. Это отражает тот факт, что пиксели, близкие друг к другу, обычно сильно коррелируют и формируют локальные структуры (грани, углы, текстуры).
*   **Общие параметры (Shared Weights / Parameter Sharing):** Один и тот же набор весов (ядро/фильтр) применяется ко всем локальным областям входного изображения. Это позволяет:
    *   Значительно уменьшить количество настраиваемых параметров по сравнению с полносвязными сетями.
    *   Обнаруживать один и тот же признак (например, вертикальную линию) в разных частях изображения. Это обеспечивает **инвариантность к сдвигу**.
*   **Субдискретизация / Пулинг (Subsampling / Pooling):** Уменьшение пространственной размерности карт признаков. Это помогает:
    *   Сделать представление более компактным и робастным к небольшим сдвигам и искажениям (инвариантность к масштабированию и локальным деформациям).
    *   Увеличить "поле зрения" последующих слоев.

---

## 2. Свёрточные сети (Convolutional Neural Networks, CNNs)

**2.1. Обработка изображений свёртками (классические фильтры)**
Свёртка – это математическая операция, широко используемая в обработке сигналов и изображений. Применение различных ядер (фильтров) к изображению позволяет выделять различные характеристики:
*   **Размытие (Blur):** Усредняющее ядро.
*   **Повышение резкости (Sharpen):** Ядро, подчеркивающее различия между соседними пикселями.
*   **Выделение границ (Edge Detection):** Ядра, реагирующие на перепады яркости (например, фильтр Собеля, Лапласиан).

**2.2. Дискретная свёртка**
Для двумерного изображения $x$ и ядра $k$, операция свёртки $(x * k)$ в точке $(i,j)$ вычисляется как:
$$ (x * k)_{ij} = \sum_{p,q} x_{i+p, j+q} \cdot k_{r-p, r-q} $$
Чаще на практике используется **кросс-корреляция**, где ядро не переворачивается:
$$ (x \star k)_{ij} = \sum_{p,q} x_{i+p, j+q} \cdot k_{p,q} $$
В контексте нейронных сетей, поскольку ядра обучаемы, разница между свёрткой и кросс-корреляцией нивелируется (сеть может выучить "перевернутое" ядро).

**2.3. Свёртка 1D массива**
Свёртка массива $m$ (сигнал) с помощью ядра $a$ (фильтр) для получения массива $n$:
$$ n[k] = (m * a)[k] = \sum_{i=-w}^{w} m[k+i] \cdot a[i+w] $$
(Формула на слайде $n[k] = \sum_{i=-w}^{w} m[k+i] \cdot a[i+w]$ предполагает, что центр ядра $a$ находится в $a[w]$ и его полуширина $w$).
Более общая форма для дискретной свёртки:
$$ (m * a)[k] = \sum_{i=-\infty}^{\infty} m[i] \cdot a[k-i] $$

**2.4. Свойства свёрток**
*   **Ассоциативность:** $(f * g) * h = f * (g * h)$
*   **Коммутативность:** $f * g = g * f$
*   **Линейность:** $f * (\alpha g + \beta h) = \alpha (f * g) + \beta (f * h)$

**2.5. Дополнения (Padding)**
Чтобы ядро свёртки могло обрабатывать пиксели на границах изображения и чтобы выходная карта признаков имела желаемый размер (например, тот же, что и входная), входное изображение часто дополняется по краям.
*   **Нулевой паддинг (Zero Padding):** Добавление нулей.
*   **Расширение границ (Border Extension / Replication Padding):** Копирование крайних пикселей.
*   **Зеркальный паддинг (Reflection Padding):** Зеркальное отражение пикселей относительно границы.
*   **Циклический паддинг (Circular Padding / Wrap Around):** "Заворачивание" изображения.

**2.6. 2D свёртка в CNN**
Центральный элемент ядра накладывается на текущий пиксель входной карты признаков. Значение соответствующего пикселя выходной карты признаков вычисляется как взвешенная сумма пикселей входной карты, попадающих под ядро.

**2.7. Свёрточные тензоры (работа с многоканальными изображениями)**
*   Входное изображение (например, RGB) имеет несколько каналов (глубину).
*   Свёрточное ядро также имеет глубину, соответствующую глубине входного тензора (например, для RGB входа ядро будет иметь размер $K_w \times K_h \times 3$).
*   Каждый фильтр (набор таких ядер) производит одну 2D карту признаков на выходе. Если используется $F$ фильтров, выходной тензор будет иметь $F$ каналов.
*   **Input Layer:** $[H \times W \times C_{in}]$ (например, $H \times W \times 3$ для RGB)
*   **Convolutional Layer Output:** $[H_{out} \times W_{out} \times F]$ (где $F$ - число фильтров)

**2.8. Языковой нюанс (терминология)**
*   **Фильтры:** Параметры свёрточного преобразования (ядра) иногда называют фильтрами, хотя они не "фильтруют" в классическом смысле, а обучаются выделять признаки.
*   **Карты признаков (Feature Maps):** Выходные тензоры свёрточных слоев (результат применения фильтров).

**2.9. Размер карты признаков после свёртки**
Для входного изображения шириной $I$, ядра шириной $K$, паддинга $P$ (с каждой стороны) и шага (stride) $S$:
$$ O = \frac{I - K + 2P}{S} + 1 $$
Эта формула применяется независимо к ширине и высоте.

**2.10. Пулинг (Pooling / Subsampling)**
Операция уменьшения пространственной размерности карт признаков.
*   **Цели:**
    *   Снижение вычислительной сложности.
    *   Обеспечение инвариантности к небольшим сдвигам и деформациям.
    *   Увеличение рецептивного поля последующих слоев.
*   **Типы пулинга:**
    *   **Max Pooling:** В каждой локальной области выбирается максимальное значение.
    *   **Average Pooling:** Вычисляется среднее значение в локальной области.
*   Пулинг также применяется с некоторым размером окна и шагом.
*   Пулинг может "декоррелировать" нейроны, заставляя их реагировать на более общие признаки.

**2.11. Размер карты признаков после пулинга**
Для входной карты шириной $I$, окна пулинга шириной $P_s$ и шага $S$:
$$ O = \frac{I - P_s}{S} + 1 $$
(Здесь $P_s$ - это размер окна пулинга, не паддинг).

**2.12. Визуализация активаций слоёв**
*   **Ранние слои:** Фильтры обучаются распознавать простые низкоуровневые признаки: грани, углы, цветовые пятна, простые текстуры.
*   **Средние слои:** Комбинируют признаки из предыдущих слоев для распознавания более сложных паттернов: части объектов (глаза, колеса), более сложные текстуры.
*   **Глубокие слои:** Распознают целые объекты или их крупные фрагменты. Признаки становятся более абстрактными и семантически значимыми.

---

## 3. Обзор архитектур CNN

**3.1. Неокогнитрон (Neocognitron, Фукусима, 1980)**
*   Одна из самых ранних архитектур, вдохновленных зрительной корой.
*   Состояла из чередующихся S-слоев (simple cells, аналог свёрточных) и C-слоев (complex cells, аналог пулинга).
*   Демонстрировала иерархическое извлечение признаков.

**3.2. LeNet (LeCun et al., 1998)**
*   Одна из первых успешных CNN, применявшихся для распознавания рукописных цифр (набор MNIST).
*   Архитектура: Вход $\rightarrow$ Conv1 $\rightarrow$ Pool1 $\rightarrow$ Conv2 $\rightarrow$ Pool2 $\rightarrow$ FC1 $\rightarrow$ FC2 $\rightarrow$ Выход (Softmax).
    *   Conv слои использовали сигмоидную или гиперболическую тангенс функцию активации.
    *   Pool слои – average pooling.
*   Ключевые идеи: иерархическая структура, общие веса, обратное распространение ошибки для обучения.
*   Пример последовательности тензоров в LeNet-5 (слайд 16):
    *   Вход: $32 \times 32 \times 1$
    *   C1 (Conv, 6 фильтров 5x5, stride 1): $28 \times 28 \times 6$
    *   S2 (Pool, 2x2, stride 2): $14 \times 14 \times 6$
    *   C3 (Conv, 16 фильтров 5x5, stride 1): $10 \times 10 \times 16$
    *   S4 (Pool, 2x2, stride 2): $5 \times 5 \times 16$ (это 400 нейронов)
    *   C5 (FC, 120 нейронов)
    *   F6 (FC, 84 нейрона)
    *   Выход (10 нейронов, для 10 цифр)

**3.3. AlexNet (Krizhevsky et al., 2012)**
*   Значительно более глубокая и широкая версия LeNet.
*   Победитель соревнования ImageNet 2012, что положило начало буму глубокого обучения.
*   **Особенности:**
    *   Использование ReLU в качестве функции активации (вместо сигмоиды/tanh), что ускорило обучение.
    *   Использование Dropout для регуляризации.
    *   Обучение на нескольких GPU (модель была разделена на две части из-за ограничений памяти GPU того времени).
    *   Применение Local Response Normalization (LRN), хотя позже было показано, что Batch Normalization эффективнее.
    *   Использование перекрывающегося пулинга (overlapping pooling).
*   Размер свёрточных ядер уменьшался от входа к выходу (например, с 11x11 до 3x3).

**3.4. VGG-16 и VGG-19 (Simonyan & Zisserman, 2014)**
*   Очень глубокие сети (16 и 19 слоёв соответственно).
*   **Ключевая идея:** Использование очень маленьких свёрточных фильтров (3x3) последовательно. Два свёрточных слоя 3x3 имеют такое же эффективное рецептивное поле, как один слой 5x5, но с меньшим количеством параметров и большей нелинейностью. Три слоя 3x3 эквивалентны одному 7x7.
*   Простая и однородная архитектура: последовательности блоков Conv(3x3)-ReLU, за которыми следует MaxPool(2x2).
*   Большое количество параметров (138 млн для VGG-16, 144 млн для VGG-19).

**3.5. Network in Network (NiN, Lin et al., 2014)**
*   Идея: заменить простые линейные свёрточные фильтры более сложными микро-сетями (например, небольшими многослойными перцептронами, MLPConv) для вычисления карт признаков.
*   Использование Global Average Pooling перед последним полносвязным слоем, что уменьшает количество параметров и переобучение.
*   **1x1 свёртки (Pointwise Convolutions):**
    *   Используются для изменения количества каналов (глубины) без изменения пространственных размеров.
    *   Могут рассматриваться как полносвязные слои, применяемые к каждому пикселю (по глубине).
    *   Используются в NiN и последующих архитектурах (GoogLeNet, ResNet) для уменьшения размерности ("бутылочное горлышко") перед более дорогими свёртками 3x3 или 5x5.
    *   CCCP (Cascaded Cross-Channel Pooling) – одна из идей, связанных с 1x1 свёртками.

**3.6. GoogLeNet / Inception (Szegedy et al., 2014)**
*   Победитель ImageNet 2014.
*   **Inception модуль:** Основной строительный блок. Параллельно применяет свёртки разных размеров (1x1, 3x3, 5x5) и Max Pooling к входной карте признаков. Результаты конкатенируются по глубине.
    *   **Идея:** Объекты на изображении могут иметь разный масштаб. Разные размеры фильтров позволяют захватывать информацию на разных уровнях детализации.
    *   **1x1 свёртки для уменьшения размерности:** Перед свёртками 3x3 и 5x5 (и после Max Pooling) используются 1x1 свёртки для уменьшения числа каналов, что делает Inception модуль более вычислительно эффективным.
*   Использование вспомогательных классификаторов на промежуточных слоях для борьбы с затуханием градиента.
*   Глубокая, но относительно эффективная по числу параметров сеть.

**3.7. ResNet (Residual Network, He et al., 2015)**
*   Победитель ImageNet 2015. Позволила успешно обучать сети глубиной в сотни и даже тысячи слоёв.
*   **Ключевая идея: Остаточные блоки (Residual Blocks).**
    *   Выход блока $H(x)$ вычисляется как $F(x) + x$, где $F(x)$ – это функция, которую обучают несколько слоёв блока, а $x$ – это вход блока (проброшенная "короткая" связь, skip connection).
    *   Это позволяет градиентам легче проходить через глубокие сети и облегчает обучение тождественного отображения (если $F(x)=0$, то $H(x)=x$).
*   Значительно улучшила результаты на многих задачах компьютерного зрения.

**3.8. Сравнение архитектур (слайд 40)**
*   Графики показывают рост точности (Top-1 accuracy на ImageNet) и числа операций/параметров для различных архитектур.
*   Наблюдается тенденция к увеличению глубины и сложности моделей, что приводит к улучшению качества.

---

## 4. Задачи компьютерного зрения

**4.1. Наборы данных (Datasets)**
*   **MNIST:** Рукописные цифры (0-9). 28x28 пикселей, черно-белые.
*   **Fashion-MNIST:** Изображения 10 категорий одежды. 28x28 пикселей, черно-белые. Альтернатива MNIST для тестирования моделей.
*   **CIFAR-10 / CIFAR-100:** Маленькие цветные изображения (32x32 пикселя) 10 или 100 классов соответственно.
*   **ImageNet (ILSVRC - ImageNet Large Scale Visual Recognition Challenge):**
    *   ~1.2 миллиона обучающих изображений, 1000 классов.
    *   Сыграл ключевую роль в развитии глубокого обучения для компьютерного зрения.

**4.2. Основные задачи**
*   **Классификация изображений (Image Classification):** Присвоить изображению одну метку класса (например, "кошка", "собака").
*   **Классификация + Локализация (Classification + Localization):** Классифицировать основной объект на изображении и определить его местоположение с помощью ограничивающей рамки (bounding box).
*   **Детектирование объектов (Object Detection):** Найти и классифицировать все объекты на изображении, указав их ограничивающие рамки.
*   **Сегментация изображений (Image Segmentation):**
    *   **Семантическая сегментация (Semantic Segmentation):** Присвоить каждому пикселю изображения метку класса (например, "дорога", "небо", "автомобиль", "пешеход"). Не различает отдельные экземпляры одного класса.
    *   **Сегментация экземпляров (Instance Segmentation):** Присвоить каждому пикселю метку класса и различить отдельные экземпляры объектов одного класса (например, "пешеход 1", "пешеход 2").

**4.3. Методы для задач сегментации и детектирования**

**Для семантической сегментации:**
*   **Полностью свёрточные сети (Fully Convolutional Networks, FCN):** Заменяют полносвязные слои в конце классификационных CNN на свёрточные, чтобы получить карту предсказаний той же пространственной размерности, что и вход.
*   **Деконволюция / Транспонированная свёртка (Deconvolution / Transposed Convolution):** Операции, обратные свёртке и пулингу, используемые для увеличения пространственного разрешения карт признаков (upsampling) в декодирующей части сетей для сегментации.
    *   **Max Unpooling:** Использует сохраненные индексы максимальных элементов из соответствующего слоя Max Pooling для восстановления карты признаков.
    *   **Transposed Convolution (иногда неточно называют deconvolution):** Обучаемый слой для апсемплинга. Можно представить как свёртку, где ядро "разбрасывает" значения входной карты на большую выходную карту.
*   **U-Net:** Архитектура с симметричным кодировщиком (encoder) и декодировщиком (decoder) и "skip connections" между ними. Кодировщик извлекает признаки, уменьшая разрешение, а декодировщик восстанавливает разрешение, используя признаки из кодировщика для уточнения локализации. Широко используется в медицинской сегментации.

**Для локализации и детектирования объектов:**
*   **Подход с "головами" (Heads):** После общей свёрточной "основы" (backbone), которая извлекает карты признаков, добавляются отдельные "головы" (небольшие полносвязные или свёрточные сети) для разных задач:
    *   **Classification head:** Предсказывает классы объектов.
    *   **Regression head:** Предсказывает координаты ограничивающих рамок.
*   **R-CNN (Regions with CNN features):**
    1.  **Генерация предложений регионов (Region Proposals):** Используется алгоритм (например, Selective Search) для генерации ~2000 потенциальных регионов, где могут находиться объекты.
    2.  **Извлечение признаков CNN:** Каждый регион "выпрямляется" (warped) до фиксированного размера и пропускается через предобученную CNN для извлечения признаков.
    3.  **Классификация регионов:** Признаки подаются на SVM для классификации объектов.
    4.  **Уточнение рамок (Bounding Box Regression):** Обучается регрессор для уточнения координат рамок.
    *   Недостатки: медленный, так как каждый регион обрабатывается отдельно.
*   **Быстрые модификации R-CNN:** Fast R-CNN, Faster R-CNN (с Region Proposal Network).
*   **YOLO (You Only Look Once):**
    *   Обрабатывает изображение за один проход.
    *   Делит изображение на сетку ячеек. Каждая ячейка предсказывает несколько ограничивающих рамок и вероятности классов для этих рамок.
    *   Значительно быстрее R-CNN, позволяет работать в реальном времени.
*   **SSD (Single Shot MultiBox Detector):** Другой однопроходный детектор, использующий признаки из нескольких слоев для детектирования объектов разного масштаба.

---

# Лекция 7: Рекуррентные сети и работа с последовательностями

**План лекции:**
1.  Рекуррентные нейронные сети (RNN)
2.  Обработка последовательностей
3.  Продвинутые архитектуры (LSTM, GRU)
4.  Больше связей (двунаправленные, многослойные RNN)
5.  Механизм внимания (Attention Mechanism)
6.  Авторегрессионная генерация

---

## 1. Рекуррентные нейронные сети (RNN)

**1.1. Рекурсия и биологическая аналогия**
*   Биологические нейронные сети обладают рекуррентными связями (нейроны могут влиять сами на себя или на предыдущие по ходу обработки нейроны через обратные связи).
*   Для моделирования процессов, аналогичных работе мозга (например, обработка временной информации, память), требуются рекурсивные функции.

**1.2. Проблема дифференцирования рекурсивных функций**
*   Стандартное автоматическое дифференцирование (AD) требует ациклического графа вычислений. Рекуррентные связи создают циклы.
*   **Решение:** "Разворот во времени" (Unrolling in Time) – рекурсивная функция вычисляется итеративно на протяжении конечного числа шагов, превращая циклический граф в глубокий ациклический граф. Каждый временной шаг становится слоем в этом развернутом графе.

**1.3. Временной ряд и последовательность**
*   Задачу обработки рекурсивной функции можно свести к обработке временнóго ряда.
*   Временнóй ряд (последовательность наблюдений, упорядоченных во времени) можно рассматривать как частный случай общей последовательности.

**1.4. Сеть Хопфилда (Hopfield Network)**
*   Один из ранних примеров рекуррентной нейронной сети (1982).
*   Представляет собой **ассоциативную память:** способна запоминать паттерны и восстанавливать их из зашумленных или неполных входных данных.
*   Нейроны бинарные (или биполярные). Связи симметричные ($w_{ij} = w_{ji}$), нет связей нейрона с самим собой ($w_{ii}=0$).
*   Динамика сети: нейроны асинхронно обновляют свои состояния до тех пор, пока сеть не сойдется к одному из запомненных паттернов (аттрактору).
*   Могут демонстрировать стабильное поведение (схождение к аттрактору), осцилляции или хаотическое поведение в зависимости от параметров и структуры.

**1.5. Разворот во времени (Unfolding/Unrolling in Time)**
*   Схема (слайд 5):
    *   Скрытое состояние $h(t)$ на шаге $t$ зависит от входа $x(t)$ и предыдущего скрытого состояния $h(t-1)$.
    *   $h(t+1)$ вычисляется блоком "Hidden Units" и передается на следующий временной шаг через блок "Delay".
    *   Выход "Outputs" генерируется на основе $h(t+1)$ (или $h(t)$).
*   При развороте (слайд 6) рекуррентный блок A копируется для каждого временного шага $t=0, 1, \dots, T$. Скрытое состояние $h_t$ передается от копии A на шаге $t$ к копии A на шаге $t+1$.
*   Это позволяет применять алгоритм обратного распространения ошибки (Backpropagation Through Time, BPTT).

*Неправильный разворот во времени (терминологический нюанс):*
Иногда говорят, что последовательности сводят к рекурсии, а затем обратно к последовательностям. Более точно, рекуррентное определение используется для обработки последовательностей.
Тип "последовательность от X", $Seq(X)$, можно определить рекурсивно:
$Seq(X) = \epsilon \lor (X \times Seq(X))$, где $\epsilon$ – пустая последовательность, $\lor$ – или, $\times$ – конкатенация/добавление элемента.

---

## 2. Обработка последовательностей

**2.1. Области применения RNN**
*   **Временные ряды:** Прогнозирование погоды, цен на акции, анализ медицинских сигналов (ЭКГ).
*   **Естественные языки (NLP):** Машинный перевод, генерация текста, анализ тональности, распознавание именованных сущностей.
*   **Речь:** Распознавание речи, синтез речи.
*   **Динамические системы:** Моделирование физических или биологических систем.
*   **Изображения и видео:** Описание изображений (image captioning), анализ видео (action recognition).
*   В целом, любые данные, имеющие последовательную структуру.

**2.2. Методы обработки последовательностей (кроме RNN)**
*   **Числовые последовательности (классические методы):**
    *   **Спектральные методы:** Анализ частотных характеристик (преобразование Фурье).
    *   **Временные методы:** Авторегрессионные модели (AR), скользящего среднего (MA), ARMA, ARIMA.
    *   **Частотно-временные методы:** Вейвлет-преобразование, кратковременное преобразование Фурье (STFT).
*   **Вероятностные графические модели:**
    *   Рассматривают временной ряд как результат стохастического процесса с условными независимостями.
    *   **Скрытые Марковские Модели (HMM):** Моделируют последовательности наблюдений, зависящие от скрытых состояний, которые образуют Марковскую цепь.
    *   **Динамические Байесовские Сети (DBN):** Обобщение HMM, позволяющее моделировать более сложные зависимости между переменными во времени.

**2.3. Обработка последовательности с помощью "простой" RNN**
На каждом временном шаге $i$:
$$ (h_i, y_i) = f_a(h_{i-1}, x_i) $$
*   $x_i$: элемент входной последовательности на шаге $i$.
*   $h_{i-1}$: скрытое состояние (память) с предыдущего шага.
*   $h_i$: новое скрытое состояние.
*   $y_i$: выход на шаге $i$.
*   $a$: обучаемые параметры функции $f_a$ (веса), общие для всех временных шагов.
Типичная реализация "простой" RNN ячейки:
$$ h_i = \tanh(W_{hh}h_{i-1} + W_{xh}x_i + b_h) $$
$$ y_i = W_{hy}h_i + b_y $$
(Функция активации для выхода может быть другой, например, softmax для классификации).

**2.4. Анализ "простой" RNN**
*   **Преимущества:**
    *   Способны аппроксимировать не просто статические функции, а динамические системы (функции с памятью).
    *   Являются частью "экосистемы" нейронных сетей, интегрируются с другими типами слоев.
*   **Недостатки:**
    *   **Проблема исчезающих/взрывающихся градиентов (Vanishing/Exploding Gradients):** При распространении градиента через много временных шагов (в BPTT) он может экспоненциально уменьшаться (исчезать) или увеличиваться (взрываться) из-за многократных умножений на матрицу весов $W_{hh}$. Это затрудняет обучение долговременных зависимостей.
    *   **Трудности с долговременными зависимостями:** Из-за проблемы градиентов простые RNN плохо запоминают информацию на длительных интервалах.
    *   Всегда смешивают предыдущий сигнал (память) с текущим входом, что может приводить к потере информации.
    *   Обучение может быть медленным.

---

## 3. Продвинутые архитектуры RNN

Разработаны для решения проблемы исчезающих/взрывающихся градиентов и улучшения способности запоминать долговременные зависимости.

**3.1. Долгая краткосрочная память (Long Short-Term Memory, LSTM)**
*   **Идея:** Ввести отдельный **блок памяти (cell state, $C_t$)**, который может хранить информацию на длительных интервалах. Информация в этот блок добавляется или удаляется с помощью специальных **вентилей (gates)**.
*   LSTM ячейка параметрическая и используется для хранения "глобального" состояния $C_t$ и генерации "локального" скрытого состояния $h_t$.

**Компоненты LSTM ячейки:**
*   **Лента (Conveyor Belt) / Состояние ячейки ($C_t$):** Основной поток информации. Операции с $C_t$ в основном аддитивные или поэлементное умножение на значения вентилей, что позволяет градиентам легко проходить.
    $$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$
*   **Фильтр забывания (Forget Gate, $f_t$):** Определяет, какую информацию из предыдущего состояния ячейки $C_{t-1}$ нужно "забыть".
    $$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$
    $\sigma$ – сигмоидная функция (выход в $[0,1]$). $[h_{t-1}, x_t]$ – конкатенация предыдущего скрытого состояния и текущего входа.
*   **Фильтр входа (Input Gate, $i_t$):** Определяет, какую новую информацию из кандидата на обновление $\tilde{C}_t$ нужно записать в состояние ячейки.
    $$ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) $$
*   **Кандидат на обновление состояния ($\tilde{C}_t$):** Новая информация, которая может быть добавлена к состоянию ячейки.
    $$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) $$
    $\tanh$ – гиперболический тангенс (выход в $[-1,1]$).
*   **Обновление памяти (Состояния ячейки):** Комбинация старой информации (пропущенной через фильтр забывания) и новой информации (пропущенной через фильтр входа).
    $$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$
    $\odot$ – поэлементное умножение.
*   **Фильтр выхода (Output Gate, $o_t$):** Определяет, какая часть состояния ячейки $C_t$ будет использована для формирования скрытого состояния $h_t$.
    $$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$
*   **Обновление скрытого состояния ($h_t$):**
    $$ h_t = o_t \odot \tanh(C_t) $$

**LSTM со смотровыми глазками (Peephole Connections):**
Модификация, где вентили ($f_t, i_t, o_t$) также зависят от состояния ячейки $C_{t-1}$ (для $f_t, i_t$) или $C_t$ (для $o_t$).
$$ f_t = \sigma(W_f \cdot [C_{t-1}, h_{t-1}, x_t] + b_f) $$
$$ i_t = \sigma(W_i \cdot [C_{t-1}, h_{t-1}, x_t] + b_i) $$
$$ o_t = \sigma(W_o \cdot [C_t, h_{t-1}, x_t] + b_o) $$

**3.2. Gated Recurrent Unit (GRU)**
*   Упрощенная версия LSTM с меньшим количеством параметров и вентилей. Часто показывает сравнимую производительность.
*   Объединяет фильтры забывания и входа в один **фильтр обновления (update gate, $z_t$)**.
*   Использует **фильтр сброса (reset gate, $r_t$)**.
*   Не имеет отдельного состояния ячейки $C_t$, вся информация хранится в $h_t$.
*   Уравнения:
    $$ z_t = \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(Update gate)} $$
    $$ r_t = \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(Reset gate)} $$
    $$ \tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) \quad \text{(Candidate hidden state)} $$
    $$ h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t $$

**3.3. Анализ LSTM/GRU**
*   **Достоинства:**
    *   Значительно лучше справляются с долговременными зависимостями по сравнению с простыми RNN.
    *   Менее подвержены проблеме исчезающих/взрывающихся градиентов благодаря вентильной структуре и аддитивным обновлениям состояния ячейки.
*   **Недостатки:**
    *   Более вычислительно сложные и требуют больше времени для обучения, чем простые RNN.
    *   Все еще могут забывать очень длинные зависимости, хотя и реже.

---

## 4. Больше связей в RNN

**4.1. Добавление обратного направления (Bidirectional RNN, BiRNN)**
*   Для многих задач (например, машинный перевод, анализ тональности) информация не только из прошлого, но и из будущего контекста может быть полезна для обработки текущего элемента последовательности.
    *   Пример: "He said 'Teddy bears are on **sail**!'" vs "He said 'Teddy Roosevelt was a great **President**!'" – значение "Teddy" зависит от последующих слов.
*   **BiRNN:** Состоит из двух независимых RNN, обрабатывающих последовательность в двух направлениях:
    *   **Прямая RNN (Forward RNN):** Обрабатывает последовательность от начала к концу ($x_0, x_1, \dots, x_T$), генерируя скрытые состояния $\overrightarrow{h_t}$.
    *   **Обратная RNN (Backward RNN):** Обрабатывает последовательность от конца к началу ($x_T, x_{T-1}, \dots, x_0$), генерируя скрытые состояния $\overleftarrow{h_t}$.
*   Выход BiRNN на шаге $t$ обычно является конкатенацией или другой комбинацией $\overrightarrow{h_t}$ и $\overleftarrow{h_t}$.
    $$ y_t = g([\overrightarrow{h_t}, \overleftarrow{h_t}]) $$
*   Подходит для задач, где доступна вся входная последовательность (не для онлайн-прогнозирования).

**4.2. Многослойная рекуррентная нейронная сеть (Stacked/Deep RNN)**
*   Несколько слоев RNN укладываются друг на друга.
*   Выходная последовательность скрытых состояний одного слоя RNN ($h^{(l)}_t$) становится входной последовательностью для следующего слоя RNN ($h^{(l+1)}_t$).
*   Позволяет сети изучать более сложные иерархические представления последовательностей.
*   На схеме (слайд 26):
    *   $h_t$ – скрытые состояния первого слоя.
    *   $z_t$ – скрытые состояния второго слоя.
    *   $x_t$ – вход, $O_t$ – выход.
    *   Матрицы $S, W1, G, W2, V$ – обучаемые веса.

**4.3. Классификация архитектур RNN по типу задачи (many-to-many, etc.)**
В зависимости от того, как входы и выходы соотносятся по длине последовательности:
*   **One-to-one:** Стандартная нейронная сеть (не рекуррентная), один вход – один выход. (Пример: классификация изображений).
*   **One-to-many:** Один вход (например, изображение или вектор) – последовательность на выходе. (Пример: генерация описания изображения, image captioning).
*   **Many-to-one:** Последовательность на входе – один выход. (Пример: анализ тональности текста, классификация последовательности).
*   **Many-to-many (синхронный):** Входная и выходная последовательности имеют одинаковую длину, выход на шаге $t$ зависит от входа на шаге $t$ (и предыдущих). (Пример: разметка частей речи для каждого слова, видеоклассификация по кадрам).
*   **Many-to-many (асинхронный / Seq2Seq):** Входная и выходная последовательности могут иметь разную длину. (Пример: машинный перевод, распознавание речи).

**4.4. Модель Seq2Seq (Sequence-to-Sequence)**
*   Архитектура для задач many-to-many, где длины входной и выходной последовательностей могут различаться.
*   Состоит из двух основных частей:
    1.  **Кодировщик (Encoder):** RNN (например, LSTM, GRU), которая обрабатывает входную последовательность ($A, B, C$) и сжимает ее в векторное представление фиксированной длины – **вектор контекста (context vector, $c$)**. Обычно это последнее скрытое состояние кодировщика.
    2.  **Декодировщик (Decoder):** Другая RNN, которая инициализируется вектором контекста $c$ и генерирует выходную последовательность ($W, X, Y, Z, \text{<eos>}$) шаг за шагом. На каждом шаге декодер получает предыдущий сгенерированный элемент и свое предыдущее скрытое состояние для генерации следующего элемента.
*   Между кодировщиком и декодировщиком передается вектор контекста, кодирующий всю входную последовательность.
*   Ячейки RNN (в кодировщике и декодировщике) могут быть любой сложности (простые RNN, LSTM, GRU).
*   **Проблема:** Вектор контекста фиксированной длины становится "бутылочным горлышком" для длинных последовательностей, так как ему сложно удержать всю информацию. Это решается механизмом внимания.

---

## 5. Механизм внимания (Attention Mechanism)

**5.1. Предпосылка**
*   Предложение (особенно длинное) очень сложно или невозможно адекватно закодировать одним вектором контекста фиксированной длины.
*   **Идея:** Вместо того, чтобы полагаться на один вектор контекста, декодер на каждом шаге генерации может "обращать внимание" на различные части входной последовательности, используя все скрытые состояния кодировщика.

**5.2. Использование векторов скрытых состояний кодировщика**
*   Декодеру на каждом шаге $t$ предоставляется доступ ко всем скрытым состояниям кодировщика $\{ \bar{h}_1, \bar{h}_2, \dots, \bar{h}_S \}$, где $S$ – длина входной последовательности.
*   Декодер вычисляет "веса внимания" $\alpha_{ts}$ для каждого скрытого состояния кодировщика $\bar{h}_s$. Эти веса показывают, насколько релевантно каждое $\bar{h}_s$ для генерации выхода на текущем шаге $t$ декодера.
*   На основе этих весов вычисляется **контекстный вектор $c_t$** как взвешенная сумма скрытых состояний кодировщика. Этот $c_t$ специфичен для каждого шага декодера $t$.

**5.3. Вычисление механизма внимания**
1.  **Оценка релевантности (Alignment Score Function, $score(h_t, \bar{h}_s)$):**
    Вычисляется "оценка" того, насколько скрытое состояние декодера на предыдущем шаге $h_{t-1}$ (или текущее $h_t$ перед генерацией выхода) "соответствует" каждому скрытому состоянию кодировщика $\bar{h}_s$.
    *   **Типы функций score:**
        *   **Content-based (Dot-Product):** $$ score(h_t, \bar{h}_s) = h_t^T \bar{h}_s $$
        *   **Additive (Bahdanau Attention):** $$ score(h_t, \bar{h}_s) = v_a^T \tanh(W_a[h_t; \bar{h}_s]) $$ (где $W_a, v_a$ – обучаемые параметры)
        *   **General:** $$ score(h_t, \bar{h}_s) = h_t^T W_a \bar{h}_s $$
        *   **Scaled Dot-Product:** $$ score(h_t, \bar{h}_s) = \frac{h_t^T \bar{h}_s}{\sqrt{d_k}} $$ (где $d_k$ – размерность векторов, используется в Трансформерах)
2.  **Веса внимания (Attention Weights, $\alpha_{ts}$):**
    Оценки $score$ нормализуются с помощью функции softmax, чтобы получить распределение вероятностей (веса внимания) по скрытым состояниям кодировщика:
    $$ \alpha_{ts} = \frac{\exp(score(h_t, \bar{h}_s))}{\sum_{s'=1}^S \exp(score(h_t, \bar{h}_{s'}))} $$
3.  **Контекстный вектор (Context Vector, $c_t$):**
    Вычисляется как взвешенная сумма скрытых состояний кодировщика:
    $$ c_t = \sum_{s=1}^S \alpha_{ts} \bar{h}_s $$
4.  **Вектор внимания (Attention Vector, $a_t$) / Использование контекста:**
    Контекстный вектор $c_t$ используется вместе с текущим скрытым состоянием декодера $h_t$ для генерации выхода на шаге $t$. Например, они конкатенируются и пропускаются через полносвязный слой:
    $$ a_t = f(c_t, h_t) = \tanh(W_c[c_t; h_t]) $$
    Затем $a_t$ используется для предсказания следующего токена $y_t$.

**5.4. Похожесть слов (визуализация внимания)**
Матрица весов внимания $\alpha_{ts}$ показывает, на какие слова входной последовательности (ось s) модель "смотрела" при генерации каждого слова выходной последовательности (ось t). Это позволяет интерпретировать, как модель связывает слова в разных языках при переводе.

**5.5. Обсуждение механизма внимания**
*   Обучается так же, как и другие блоки нейронной сети (end-to-end с помощью обратного распространения ошибки).
*   Позволяет обрабатывать более длинные последовательности, так как снимает нагрузку с одного вектора контекста.
*   В целом, улучшает производительность моделей Seq2Seq.
*   Может применяться в различных архитектурах, не только RNN.
*   Добавляет больше параметров в модель.

**5.6. Differentiable Neural Computer (DNC)**
*   Сложная архитектура, сочетающая нейронную сеть (контроллер) с внешней памятью.
*   Контроллер может читать из памяти и писать в нее с помощью механизмов, похожих на внимание ("read heads" и "write heads").
*   Способна к более сложным формам рассуждений и манипулирования информацией, чем стандартные RNN.

---

## 6. Авторегрессионная генерация

Процесс генерации последовательности, где каждый следующий элемент генерируется на основе предыдущих сгенерированных элементов.

**6.1. Типы генерации**
*   **Генерация из вектора скрытого состояния:** Например, декодер в модели Seq2Seq без внимания генерирует последовательность, начиная с одного вектора контекста.
*   **Генерация из последовательности (продолжение):** Модели подается начальная часть последовательности (затравка, prompt), и она генерирует ее продолжение.

**6.2. Beam Search (Лучевой поиск)**
*   Алгоритм декодирования, используемый для генерации последовательностей (например, в машинном переводе, генерации текста).
*   Вместо того, чтобы на каждом шаге выбирать только один наиболее вероятный следующий токен (жадный поиск), Beam Search поддерживает $k$ (размер "луча", beam_width) наиболее вероятных частичных гипотез (последовательностей).
*   На каждом шаге для каждой из $k$ гипотез генерируются все возможные продолжения на один токен. Из всех полученных гипотез снова выбираются $k$ наиболее вероятных (обычно по сумме логарифмов вероятностей токенов).
*   Процесс продолжается до генерации токена конца последовательности (`<eos>`) или достижения максимальной длины.
*   Дает лучшие результаты, чем жадный поиск, но более вычислительно затратен.

**6.3. Выбор следующего токена (Sampling Strategies)**
На каждом шаге генерации модель (обычно после слоя Softmax) выдает распределение вероятностей по всему словарю для следующего токена.
*   **Жадный поиск (Greedy Search):** Выбирается токен с максимальной вероятностью.
*   **Сэмплирование (Sampling):** Токен выбирается случайным образом из распределения вероятностей.
    *   **Температура (Temperature Scaling):** Логиты перед Softmax делятся на параметр температуры $T$.
        *   $T \rightarrow 0$: распределение становится более "пиковым", приближаясь к жадному поиску.
        *   $T = 1$: стандартный Softmax.
        *   $T > 1$: распределение становится более "плоским", увеличивая случайность и разнообразие генерируемого текста.
*   **Top-k Sampling:** На каждом шаге рассматриваются только $k$ наиболее вероятных токенов, и следующий токен сэмплируется из их (перенормированного) распределения.
*   **Top-p (Nucleus) Sampling:** Выбирается минимальный набор токенов, чья суммарная вероятность превышает порог $p$. Следующий токен сэмплируется из этого набора.

---

# Лекция 8: Трансформеры

**План лекции:**
1.  Механизм внимания (краткое повторение)
2.  Архитектура трансформера
3.  Позиционное кодирование
4.  Большие языковые модели (LLM)

---

## 1. Механизм внимания (краткое повторение)

**1.1. Предпосылка**
(Как обсуждалось в Лекции 7) Кодирование длинного предложения одним вектором фиксированной длины (как в классических Seq2Seq моделях) является "бутылочным горлышком". Механизм внимания позволяет декодеру на каждом шаге генерации обращаться ко всем скрытым состояниям кодировщика, взвешивая их релевантность.

**1.2. DNC (Differentiable Neural Computer) - Напоминание**
(Слайд 4, Лекция 7, слайд 37) DNC использует контроллер (нейросеть) для взаимодействия с внешней памятью через механизмы чтения и записи, которые по своей сути являются формами внимания. Это подчеркивает важность "мягкого" доступа к информации.

**1.3. Мягкое чтение / Поиск (Soft Read / Search)**
Иллюстрация концепции внимания через аналогию с поиском в памяти.
*   **`read(query)` (Жесткий поиск):** Находит точное совпадение ключа с запросом и возвращает значение.
    ```python
    def read(query, memory):
        for key, value in memory:
            if key == query:
                return value
        return None # или ошибка
    ```
*   **`avg_read(query)` (Усреднение при точном совпадении):** Если есть несколько точных совпадений, возвращает среднее их значений.
    ```python
    def avg_read(query, memory):
        matching_values = []
        for key, value in memory:
            if key == query:
                matching_values.append(value)
        if not matching_values: return None
        return sum(matching_values) / len(matching_values)
    ```
*   **`soft_read(query)` (Мягкий поиск / Взвешенное чтение):**
    Каждой паре (key, value) в памяти присваивается вес $w$, зависящий от схожести `similarity(key, query)`. Возвращается взвешенная сумма значений. Это и есть суть внимания.
    ```python
    def soft_read(query, memory, similarity_func):
        wavg_value = 0
        sum_weights = 0
        for key, value in memory:
            weight = similarity_func(key, query) # Например, косинусное сходство или dot-product
            wavg_value += weight * value
            sum_weights += weight
        if sum_weights == 0: return None # или некоторое значение по умолчанию
        return wavg_value / sum_weights # Нормализация весов (аналог Softmax)
    ```

**1.4. Scaled Dot-Product Attention**
Ключевой компонент Трансформера.
*   **Вход:** Запросы (Queries, $Q$), Ключи (Keys, $K$), Значения (Values, $V$).
    *   Для **self-attention** (внимание к себе) $Q, K, V$ получаются из одной и той же входной последовательности $X$ путем линейных преобразований:
        $Q = XW^Q$, $K = XW^K$, $V = XW^V$.
    *   Для **cross-attention** (внимание между кодировщиком и декодировщиком) $Q$ приходит из декодера, а $K, V$ – из кодировщика.

*   **Матричный вариант (для одной "головы" внимания):**
    1.  Вычисление оценок схожести: $QK^T$
    2.  Масштабирование: $\frac{QK^T}{\sqrt{d_k}}$ (где $d_k$ – размерность векторов ключей/запросов). Масштабирование предотвращает слишком большие значения аргумента Softmax, что могло бы привести к очень малым градиентам.
    3.  Применение Softmax для получения весов внимания: $\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$
    4.  Взвешивание Значений: $Z = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$

*   **Векторный вариант (для одного элемента последовательности $x_i$):**
    *   $q_i = x_i W^Q$, $k_j = x_j W^K$, $v_j = x_j W^V$
    *   Оценка схожести $i$-го запроса с $j$-м ключом: $s_{i,j} = \langle q_i, k_j \rangle$ (или $\frac{\langle q_i, k_j \rangle}{\sqrt{d_k}}$)
    *   Веса внимания: $a_{i,j} = \text{Softmax}_j(s_{i,j})$ (Softmax по всем $j$ для данного $i$)
    *   Выходной вектор для $i$-го элемента: $z_i = \sum_j a_{i,j} v_j$

*   **Многоголовое внимание (Multi-Head Attention):**
    1.  Входные $Q, K, V$ линейно проецируются $H$ раз (для $H$ "голов") с разными, обучаемыми матрицами проекций:
        $Q_h = XW^Q_h$, $K_h = XW^K_h$, $V_h = XW^V_h$ для $h=1 \dots H$.
    2.  Scaled Dot-Product Attention применяется параллельно для каждой головы:
        $\text{head}_h = \text{Attention}(Q_h, K_h, V_h) = \text{Softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k/H}}\right) V_h$
        (Размерность $d_k/H$ если общая размерность $d_k$ делится между головами).
    3.  Результаты всех голов конкатенируются и снова линейно проецируются:
        $Z = \text{Concat}(\text{head}_1, \dots, \text{head}_H) W^O$
    *   **Идея:** Разные головы могут фокусироваться на разных аспектах информации или разных типах зависимостей в последовательности.

**1.5. Переиспользование параметров между головами**
Для уменьшения числа параметров и улучшения эффективности:
*   **Multi-Head Attention (MHA):** Каждая голова имеет свои $W^Q_h, W^K_h, W^V_h$.
*   **Grouped-Query Attention (GQA):** Несколько голов запросов (Queries) делят общие проекции для ключей и значений. Например, если 8 голов Q, может быть только 2 набора K и V.
*   **Multi-Query Attention (MQA):** Все головы запросов делят один общий набор проекций для ключей и значений.
Это особенно актуально при декодировании, где на каждом шаге генерируется один токен, и кеширование K и V из предыдущих шагов экономит вычисления.

**1.6. (Q * K^T) * V computation process with caching (слайд 8)**
При авторегрессионной генерации (например, в декодере GPT) на каждом новом шаге $N$:
*   **Queries:** Вычисляются только для текущего нового токена (1 вектор).
*   **Keys & Values:** Вычисляются для текущего нового токена и добавляются к кешированным $K$ и $V$ со всех предыдущих $N-1$ шагов.
*   Матрица $K^T$ содержит ключи всех предыдущих и текущего токенов.
*   Матрица $V$ содержит значения всех предыдущих и текущего токенов.
*   Это позволяет эффективно вычислять внимание для нового токена ко всей уже сгенерированной последовательности.

**1.7. Разные окна внимания (для длинных последовательностей)**
Механизм внимания имеет квадратичную сложность $O(n^2)$ по длине последовательности $n$. Для очень длинных последовательностей это становится проблемой.
*   **(a) Full $n^2$ attention:** Стандартное внимание, каждый токен взаимодействует с каждым.
*   **(b) Sliding window attention:** Каждый токен взаимодействует только с токенами в некотором локальном окне вокруг себя. (Как в Longformer)
*   **(c) Dilated sliding window attention:** Скользящее окно с "пропусками", чтобы увеличить рецептивное поле без увеличения числа вычислений. (Как в Longformer)
*   **(d) Global + sliding window attention:** Комбинация: некоторые токены (глобальные) могут взаимодействовать со всеми, остальные – только в скользящем окне.

**1.8. Иллюстрация работы Scaled Dot-Product Attention (jalammar.github.io)**
(Слайд 10-13) Пошаговая визуализация вычислений для механизма внимания, включая матричные операции и многоголовое внимание.

**1.9. Обсуждение внимания (повторение)**
*   Обучается так же, как и другие блоки.
*   Позволяет обрабатывать более длинные последовательности (по сравнению с RNN без внимания).
*   Улучшает производительность.
*   Может применяться в произвольных сетях.
*   Добавляет параметры (но часто более эффективно, чем увеличение скрытых состояний RNN).

---

## 2. Архитектура трансформера ("Attention Is All You Need", Vaswani et al., 2017)

Трансформер полностью отказывается от рекуррентных связей в пользу механизма внимания.

**2.1. Общая структура (Encoder-Decoder)**
*   Последовательность преобразуется несколько раз (слайд 16).
*   Состоит из стека **Кодировщиков (Encoders)** и стека **Декодировщиков (Decoders)**.
    *   Обычно 6 кодировщиков и 6 декодировщиков в оригинальной модели.

**2.2. Архитектура Кодировщика (Encoder Block)**
Каждый кодировщик состоит из двух основных подслоев:
1.  **Multi-Head Self-Attention:** Слой внимания, где $Q, K, V$ приходят из выхода предыдущего слоя кодировщика (для первого кодировщика – из входных эмбеддингов + позиционное кодирование).
2.  **Position-wise Feed-Forward Network (FFN):** Полносвязная сеть из двух линейных слоев с ReLU активацией между ними, применяемая независимо к каждой позиции (каждому токену) в последовательности.
    $$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$
*   **Add & Norm (Residual Connection + Layer Normalization):** После каждого из двух подслоев применяется остаточная связь (Add) и нормализация по слою (LayerNorm).
    $$ \text{LayerNorm}(\text{SublayerOutput} + \text{SublayerInput}) $$
    Layer Normalization нормализует активации по каждому примеру в батче и по всем нейронам/каналам для данной позиции, в отличие от Batch Normalization, которая нормализует по батчу для каждого нейрона/канала.

**2.3. Архитектура Декодировщика (Decoder Block)**
Каждый декодировщик состоит из трех основных подслоев:
1.  **Masked Multi-Head Self-Attention:** Слой self-attention для выходной последовательности. "Маскированный" означает, что при вычислении внимания для $i$-го токена, он может "смотреть" только на предыдущие токены ($0 \dots i-1$) и на себя, но не на будущие ($i+1 \dots T$). Это необходимо для авторегрессионной генерации.
2.  **Multi-Head Cross-Attention (Encoder-Decoder Attention):** Слой внимания, где $Q$ приходят из выхода предыдущего подслоя декодера (Masked Self-Attention), а $K$ и $V$ – из выхода всего стека кодировщиков. Этот слой позволяет декодеру "смотреть" на входную последовательность.
3.  **Position-wise Feed-Forward Network (FFN):** Аналогично кодировщику.
*   Также используются **Add & Norm** после каждого подслоя.
*   После стека декодировщиков обычно идет линейный слой и Softmax для получения вероятностей следующего токена.

**2.4. BERT и GPT**
Две влиятельные архитектуры, основанные на Трансформере:
*   **BERT (Bidirectional Encoder Representations from Transformers):** Использует только **стек Кодировщиков** Трансформера. Обучается на задачах предсказания замаскированных слов (Masked Language Model, MLM) и предсказания следующего предложения (Next Sentence Prediction, NSP). Является **двунаправленной** моделью, так как self-attention в кодировщике позволяет каждому токену "смотреть" на все остальные токены в последовательности. Хорошо подходит для задач понимания языка (NLU).
*   **GPT (Generative Pre-trained Transformer):** Использует только **стек Декодировщиков** Трансформера (с Masked Self-Attention). Обучается на задаче предсказания следующего слова (стандартная языковая модель). Является **однонаправленной (авторегрессионной)** моделью. Хорошо подходит для задач генерации текста (NLG).

**2.5. Маскирование в BERT (MLM)**
Во время предобучения BERT случайным образом заменяет некоторые токены во входной последовательности на специальный токен `[MASK]`. Задача модели – предсказать исходные токены на месте `[MASK]`, используя контекст остальных (незамаскированных) токенов.

**2.6. Masked Self-Attention (в декодере GPT и Трансформера)**
*   Перед применением Softmax к матрице скалярных произведений $QK^T / \sqrt{d_k}$ к элементам над главной диагональю прибавляется $-\infty$.
*   Это приводит к тому, что после Softmax соответствующие ячейки (соответствующие "будущим" токенам) будут иметь нулевой вес (вероятность).
*   Таким образом, $i$-й токен может обращать внимание только на токены с позициями от $0$ до $i$.

**2.7. Vision Transformer (ViT)**
Применение архитектуры Трансформера к задачам компьютерного зрения.
1.  Изображение делится на фиксированное количество непересекающихся **патчей** (например, 16x16 пикселей).
2.  Каждый патч "выпрямляется" в вектор и линейно проецируется в эмбеддинг.
3.  К этим эмбеддингам патчей добавляются **позиционные эмбеддинги**.
4.  Часто добавляется специальный обучаемый **`[CLS]` токен** (аналогично BERT), эмбеддинг которого после прохождения через Трансформер-кодировщик используется для классификации всего изображения.
5.  Полученная последовательность эмбеддингов патчей подается на вход стандартного Трансформер-кодировщика.
6.  Выход кодировщика (например, эмбеддинг `[CLS]` токена) подается на MLP-голову для классификации.

**2.8. Анализ Трансформера**
*   **Достоинства:**
    *   **Распараллеливаемость:** Вычисления для всех токенов в слое self-attention могут выполняться параллельно (в отличие от RNN, где обработка последовательная).
    *   **State-of-the-art качество:** Демонстрирует лучшие результаты на многих задачах NLP и других областях.
    *   **Потенциально интерпретируемые результаты:** Веса внимания могут дать некоторое представление о том, на какие части входных данных модель "обращает внимание".
*   **Недостатки:**
    *   **Очень много параметров:** Особенно для больших моделей.
    *   **Нестабильность обучения:** Требуют тщательной настройки гиперпараметров, скорости обучения (warmup), нормализации.
    *   **Фиксированная длина контекста:** Стандартный Трансформер обрабатывает последовательности фиксированной максимальной длины. Для более длинных последовательностей требуются модификации (например, окна внимания, Transformer-XL).
    *   **Квадратичная сложность внимания** по длине последовательности.

---

## 3. Позиционное кодирование (Positional Encoding)

Механизм self-attention по своей природе не учитывает порядок элементов в последовательности (обрабатывает ее как "мешок слов"). Чтобы сообщить модели информацию о порядке, к входным эмбеддингам токенов добавляются позиционные кодировки.

**3.1. Синусоидальное позиционное кодирование (оригинальный Трансформер)**
Для позиции $t$ и измерения $i$ в векторе позиционного кодирования $PE$:
$$ PE(t, 2i) = \sin(t / 10000^{2i/d_{model}}) $$
$$ PE(t, 2i+1) = \cos(t / 10000^{2i/d_{model}}) $$
где $d_{model}$ – размерность эмбеддингов.
*   Каждое измерение $PE$ соответствует синусоиде с разной частотой.
*   Это позволяет модели легко определять относительные позиции, так как $PE(t+k)$ может быть представлено как линейная функция от $PE(t)$.
*   Не обучаемое, фиксированное.

**3.2. Анализ позиционного кодирования**
*   Из-за операций суммирования (в self-attention и FFN) Трансформер обрабатывает последовательность как множество, если не добавить информацию о порядке.
*   Позиционное кодирование добавляет эту информацию.
*   Вектор позиционного кода **прибавляется** к эмбеддингу токена, а не конкатенируется, для экономии параметров.

**3.3. Rotary Position Embedding (RoPE)**
*   Альтернативный метод внедрения информации о позиции.
*   Вместо прибавления, части входного вектора (эмбеддинга токена) интерпретируются как двумерные подвекторы, которые **вращаются** на угол, зависящий от позиции токена. Матрица поворота для позиции $m$ и пары измерений $(2i, 2i+1)$ с частотой $\theta_i$:
    $$ R_{\Theta,m}^{(i)} = \begin{pmatrix} \cos m\theta_i & -\sin m\theta_i \\ \sin m\theta_i & \cos m\theta_i \end{pmatrix} $$
    Эти матрицы поворота применяются к парам измерений в векторах $Q$ и $K$ перед вычислением скалярного произведения в self-attention.
*   Сохраняет относительную информацию о позиции через свойства поворотов.

**3.4. Attention with Linear Bias (ALiBi)**
*   Еще один способ информировать модель о позиции без явного добавления позиционных эмбеддингов.
*   При вычислении оценок внимания ($QK^T$) к каждой оценке $score(q_i, k_j)$ добавляется **линейный сдвиг (bias)**, зависящий от относительного расстояния $|i-j|$ между токенами $i$ и $j$. Этот сдвиг обычно имеет вид $m \cdot |i-j|$, где $m$ – это специфичный для каждой головы внимания скаляр (отрицательный, чтобы наказывать за большое расстояние).
*   Этот сдвиг добавляется **до** Softmax.
*   Позволяет модели экстраполировать на длины последовательностей, превышающие те, на которых она обучалась.

**3.5. Обучаемое позиционное кодирование**
*   Позицию можно рассматривать как категориальный признак.
*   Каждой позиции (от 0 до максимальной длины) сопоставляется обучаемый вектор эмбеддинга (аналогично эмбеддингам слов).
*   Эти обучаемые позиционные эмбеддинги прибавляются к эмбеддингам токенов.
*   Недостаток: теряется явная информация об относительном порядке, модели приходится "выучивать" ее заново из данных. Плохо экстраполирует на невиданные длины.

---

## 4. Большие языковые модели (Large Language Models, LLM)

Модели, основанные преимущественно на архитектуре Трансформера, обученные на огромных объемах текстовых данных.

**4.1. Рост числа параметров в LLM (слайд 30)**
График показывает экспоненциальный рост числа параметров в LLM с 2018 года (GPT, BERT) до современных моделей (GPT-4, PaLM, LLaMA, Bloom) с сотнями миллиардов и триллионами параметров.

**4.2. Законы масштабирования (Scaling Laws for LLMs)**
Эмпирические исследования (например, OpenAI, DeepMind) показали, что производительность LLM (измеряемая как функция потерь на тестовых данных) предсказуемо улучшается с увеличением:
*   **Размера модели (числа параметров, $N$)**
*   **Размера обучающего набора данных (числа токенов, $D$)**
*   **Объема вычислений (Compute)**
Потери $L$ обычно убывают как степенная функция от этих факторов, например:
$L(N) \approx (N/N_c)^{-\alpha_N}$, $L(D) \approx (D/D_c)^{-\alpha_D}$.
Это позволяет прогнозировать производительность более крупных моделей и оптимально распределять бюджет на вычисления, размер модели и данные.

**4.3. Выравнивание LLM (LLM Alignment)**
Процесс настройки LLM для соответствия человеческим намерениям, предпочтениям и ценностям.
*   **Supervised Fine-Tuning (SFT):** Дообучение предобученной LLM на небольшом наборе высококачественных примеров "запрос-ответ", созданных людьми.
*   **Reinforcement Learning from Human Feedback (RLHF):**
    1.  **Сбор данных сравнения:** Для одного и того же запроса генерируется несколько ответов LLM. Люди-оценщики ранжируют эти ответы от лучшего к худшему.
    2.  **Обучение модели вознаграждения (Reward Model, RM):** На основе данных сравнения обучается модель (обычно другая LLM), которая предсказывает "оценку качества" (вознаграждение) для любого ответа на данный запрос.
    3.  **Оптимизация политики с помощью RL:** Предобученная LLM (политика) дообучается с использованием алгоритмов обучения с подкреплением (например, PPO - Proximal Policy Optimization). Цель – максимизировать вознаграждение, получаемое от модели вознаграждения.
*   **Этапы (слайд 32):**
    *   Step 1: SFT
    *   Step 2: Collect comparison data and train a reward model.
    *   Step 3: Optimize a policy against the reward model using reinforcement learning.
*   **Проблемы выравнивания (слайд 33):**
    *   **Inner Alignment:** Соответствие внутренних целей модели ее явной функции потерь.
    *   **Outer Alignment:** Соответствие функции потерь истинным намерениям человека.
    *   **Mechanistic Interpretability:** Понимание того, как модель принимает решения.
    *   **Alignment Evaluation:** Оценка того, насколько хорошо модель выровнена (фактологичность, этичность, токсичность, предвзятость).
    *   **Attack on Alignment:** Попытки обойти выравнивание (privacy attacks, backdoor attacks, adversarial attacks).
*   **Пример оценки выровненных моделей (слайд 34):** Радарные диаграммы, показывающие производительность моделей по различным аспектам (полезность, ясность, вовлеченность, глубина, фактологичность).

**4.4. Retrieval Augmented Generation (RAG)**
Техника для улучшения фактологичности и актуальности ответов LLM путем предоставления ей доступа к внешней базе знаний во время генерации.
1.  **Пользовательский запрос (Query).**
2.  **Извлечение (Retrieval):** Запрос используется для поиска релевантной информации (контекста) в базе знаний (например, векторной базе данных документов). Документы предварительно индексируются с помощью эмбеддинг-модели.
3.  **Дополнение (Augmentation):** Извлеченный релевантный контекст объединяется с исходным запросом пользователя.
4.  **Генерация (Generation):** Дополненный запрос (запрос + контекст) подается на вход LLM для генерации ответа.
*   Это позволяет LLM использовать информацию, которой не было в ее обучающих данных, и ссылаться на источники.

**4.5. Chain-of-Thought (CoT) Prompting**
Техника промптинга (формулирования запросов к LLM), которая побуждает модель генерировать последовательность промежуточных шагов рассуждения перед тем, как дать окончательный ответ.
*   **Пример (слайд 36):**
    *   **Стандартный промпт:** "Q: Roger has 5 balls... A: 7."
    *   **CoT промпт:** "Q: Roger has 5 balls. He buys 2 more. How many balls does he have now? A: He started with 5 and buys 2 more. 5 + 2 = 7. The answer is 7."
*   Предоставление примеров с такими "цепочками мыслей" (в few-shot промптинге) или просто указание модели "Let's think step by step" (в zero-shot промптинге) может значительно улучшить ее способность решать задачи, требующие многошаговых рассуждений (например, арифметические, логические).

---

# Лекция 10: Алгебра и геометрия глубокого обучения

**План лекции:**
1.  Алгебраические типы данных (ADT)
2.  Построение архитектуры для различных типов (Vec ↔ Prod, Vec ↔ Sum)
3.  Частные случаи архитектур для ADT (объекты, пропуски, категории)
4.  Графы
5.  Частные случаи архитектур для графов (множества, таблицы, последовательности, изображения, код)

---

## 1. Алгебраические типы данных (ADT)

**1.1. Введение**
*   **Абстрактное описание данных:** ADT предоставляют способ формального описания структуры данных.
*   **Конструирование типов:** Типы создаются с помощью алгебраических операторов (произведение, сумма).
*   **Описание инвариантов:** Позволяют четко определить свойства и ограничения данных.
*   **Применение:**
    *   **Функциональное программирование:** Основа систем типов (Haskell, Scala, F#).
    *   **Аналитическая комбинаторика:** Используются для подсчета и анализа комбинаторных структур.
    *   **Базы данных:** Для описания схем данных.
    *   **Глубокое обучение:** Могут служить основой для проектирования архитектур нейронных сетей, адаптированных к структуре входных и выходных данных.

**1.2. Тип-произведение (Product Type)**
*   Кодирует **составной тип данных**, где значение состоит из комбинации значений нескольких других типов. Все компоненты должны присутствовать.
*   Обозначается операторами: `*` (звёздочка), `·` (точка), `×` (умножение), `&` (амперсанд).
*   В языках программирования соответствует:
    *   `struct` (C/C++)
    *   `class` (Java, C#, Python - в части полей данных)
    *   `record` (Pascal, SQL)
    *   `tuple` (Python, Haskell)
*   **Пример:** Описание ириса из известного набора данных.
    $$ \text{Iris} = \text{SepalLength} \times \text{SepalWidth} \times \text{PetalLength} \times \text{PetalWidth} $$
    Каждый экземпляр типа `Iris` должен содержать значения для всех четырех признаков.

**1.3. Тип-сумма (Sum Type / Tagged Union / Variant)**
*   Кодирует **объединение типов**, где значение принадлежит одному из нескольких возможных типов.
*   Обозначается операторами: `+` (плюс), `|` (вертикальная черта).
*   В языках программирования соответствует:
    *   `union` (C/C++ - но без тега, что небезопасно)
    *   `enum` (если значения являются типами, а не просто константами)
    *   `interface` (в контексте реализации одного из нескольких вариантов)
    *   Алгебраические типы данных в функциональных языках (например, `data Either a b = Left a | Right b` в Haskell).
*   **Пример:** Виды ирисов.
    $$ \text{IrisSpecies} = \text{Setosa} + \text{Versicolor} + \text{Virginica} $$
    Значение типа `IrisSpecies` может быть либо `Setosa`, либо `Versicolor`, либо `Virginica`.

**1.4. Базовые и коллекционные типы**
*   **Векторные типы ($Vec$ или $\mathbb{R}^n$):**
    *   Используются как **промежуточное состояние** (скрытые представления) в нейронных сетях.
    *   Используются как **вход** для простых архитектур (например, полносвязные сети).
    *   Используются как **выход**, если требуется предсказать непрерывные значения или логиты (значения перед Softmax).
*   **Пустой тип ($Eps$ или $\epsilon$):**
    *   Тип, не имеющий значений (или имеющий одно уникальное значение, например, `()` - unit type).
    *   Можно рассматривать как **0-мерный вектор**.
    *   Используется для обозначения отсутствия информации или как компонент в ADT (например, `Maybe a = Just a | Nothing` где `Nothing` можно представить как `Eps`).
*   **Последовательность ($Seq(X)$):**
    *   Упорядоченное множество элементов типа $X$. Порядок важен.
    *   Пример: $$ \text{Text} = \text{Seq}(\text{Words}) $$
*   **Множество ($Set(X)$):**
    *   Неупорядоченное множество элементов типа $X$. Порядок не важен, дубликаты обычно не учитываются или имеют кратность.
    *   Пример: $$ \text{MovieCast} = \text{Set}(\text{Actor}) $$
*   **Другие коллекционные типы:**
    *   **Цикл ($Cycle(X)$):** Последовательность, где конец соединен с началом.
    *   **Мультимножество ($MSet(X)$):** Множество, где элементы могут повторяться (важна кратность).
    *   **Степенное множество ($PSet(X)$ / Power Set):** Множество всех подмножеств.
    *   В теории эти типы также можно приспособить для построения архитектур.
*   **Двумерные множества:**
    *   **Изображение ($Img(X)$):** Двумерная решетка элементов типа $X$. Порядок строк и столбцов важен.
        *   Пример: $$ \text{CIFAR} = \text{Img}(\text{RGB}) $$ (где RGB - это тип-произведение Red $\times$ Green $\times$ Blue)
        *   Пример: $$ \text{SegmentationTarget} = \text{Img}(\text{Class}) $$ (каждый пиксель имеет метку класса)
    *   **Таблица ($Tab(X)$):** Двумерная структура, где порядок строк и столбцов может быть неважен (в зависимости от интерпретации). Часто рассматривается как множество строк, где каждая строка – это тип-произведение.
        *   Пример: $$ \text{DataSet} = \text{Tab}(\mathbb{R}) $$ (таблица вещественных чисел) или $$ \text{DataSet} = \text{Seq}(\text{RowFeatures}) $$

---

## 2. Построение архитектуры для различных типов (Vec ↔ Prod, Vec ↔ Sum)

Идея состоит в том, чтобы определить, как преобразовывать структурированные данные в векторное представление (кодировщик, $Type \rightarrow Vec$) и обратно (декодировщик, $Vec \rightarrow Type$) для использования в нейронных сетях.

**2.1. Преобразования Vec ↔ Prod (Вектор ↔ Тип-произведение)**

*   **Prod → Vec:** Преобразование типа-произведения $P = T_1 \times T_2 \times \dots \times T_k$ в вектор $\mathbb{R}^n$.
    1.  Заводим вспомогательные функции (кодировщики) $g_i: T_i \rightarrow \mathbb{R}^{m_i}$ для каждого компонентного типа $T_i$.
    2.  **Два основных подхода:**
        *   **Конкатенация:**
            $$ f(x_1, \dots, x_k) = \text{concat}(g_1(x_1), g_2(x_2), \dots, g_k(x_k)) $$
            При этом $\sum_{i=1}^k m_i = n$.
            *Пример:* В RNN вектор памяти (состояния) конкатенируется с вектором очередного входного элемента.
        *   **Суммирование (с одинаковой размерностью $m_i=n$):**
            $$ f(x_1, \dots, x_k) = \sum_{i=1}^k g_i(x_i) $$
            При этом все $m_i = n$.
            *Пример:* В трансформерах эмбеддинг токена складывается с вектором позиционного кодирования.

*   **Vec → Prod:** Преобразование вектора $\mathbb{R}^n$ в тип-произведение $P = T_1 \times \dots \times T_k$.
    1.  Заводим вспомогательные функции (декодировщики) $g_i: \mathbb{R}^{m_i} \rightarrow T_i$.
    2.  **Инвертирование конкатенации:**
        *   Применить общую функцию-декодер ко всему входному вектору $\mathbb{R}^n \rightarrow \mathbb{R}^n$.
        *   "Нарезать" результат на $k$ подвекторов размерностей $m_1, \dots, m_k$.
        *   Применить к каждому подвектору соответствующую функцию $g_i$.
    3.  **Инвертирование суммирования:**
        *   Применить $k$ различных функций-декодеров $g'_i: \mathbb{R}^n \rightarrow \mathbb{R}^{m_i}$ ко всему входному вектору $\mathbb{R}^n$.
        *   Затем применить $g_i: \mathbb{R}^{m_i} \rightarrow T_i$.
        *   Или напрямую применить $g_i: \mathbb{R}^n \rightarrow T_i$ к входному вектору $\mathbb{R}^n$ для получения каждого компонента $x_i$.

**2.2. Преобразования Vec ↔ Sum (Вектор ↔ Тип-сумма)**

*   **Sum → Vec:** Преобразование типа-суммы $S = T_1 + \dots + T_k$ в вектор $\mathbb{R}^n$.
    1.  Заводим вспомогательные функции (кодировщики) $g_i: T_i \rightarrow \mathbb{R}^n$ для каждого варианта $T_i$.
    2.  Если на вход пришло значение $x$ типа $T_j$ (т.е. $x: T_j$), то выходным вектором будет $g_j(x)$.
    3.  Выбор активной функции $g_j$ осуществляется на основе сопоставления с образцом (pattern matching) или тега типа-суммы.

*   **Vec → Sum:** Преобразование вектора $\mathbb{R}^n$ в тип-сумму $S = T_1 + \dots + T_k$.
    1.  Заводим:
        *   Функцию выбора/классификации $c: \mathbb{R}^n \rightarrow \{1, \dots, k\}$ (предсказывает, какой тип $T_j$ является наиболее вероятным).
        *   $k$ вспомогательных функций-декодеров $g_j: \mathbb{R}^n \rightarrow T_j$.
    2.  Предсказанный тип $T_j$ определяется как $j^* = \text{argmax}_j (c(\text{входной вектор}))_j$.
    3.  Выходное значение будет $g_{j^*}(\text{входной вектор})$.
    4.  **Проблема с $\text{argmax}$:** Эта операция "жёсткая" и недифференцируема.
        *   **Решение 1 (Soft Argmax / Взвешенное суммирование):**
            Использовать $w = \text{softargmax}(c(\text{входной вектор}))$ или $w = \text{softmax}(c(\text{входной вектор}))$.
            Затем вычислить несколько ветвей $g_j(\text{входной вектор})$ для всех $j$ и скомбинировать результаты с весами $w_j$. (Например, если $T_j$ - это векторы, то $\sum w_j g_j(\text{входной вектор})$).
        *   **Решение 2 (Обучение с подкреплением):**
            Пусть $c(x)$ предсказывает "качество" или вероятность выбора каждой ветви $T_j$. Выбор $j$ осуществляется стохастически (например, сэмплирование по предсказанным вероятностям или $\epsilon$-greedy). Модель $c(x)$ и декодеры $g_j$ обучаются с помощью методов RL.
        *   **Решение 3 (Классификация + Декодирование):**
            Если на этапе обучения известен истинный тип $T_j$ (верный выбор $j$), то можно напрямую обучать $c(x)$ как классификатор (например, с помощью CrossEntropy), а $g_j(x)$ - как условный декодер для этого класса.

**2.3. Преобразования для базовых и коллекционных типов**
*   **Vec → Vec:** Обычная архитектура нейронной сети (например, MLP, Трансформер).
*   **Vec → Eps:** Бесполезная функция (проекция на пустой тип, т.е. отбрасывание информации).
*   **Eps → Vec:** Обучаемый вектор констант (эмбеддинг для "ничего").
*   **Eps → Sum:** Если варианты в Sum - это действия, то это задача о многоруком бандите. Обучаемый вектор (Eps → Vec) отображается на оценки для каждого действия (варианта Sum).
*   **Для других типов (Seq, Set, Img, Tab):** Обычно сначала генерируется векторное представление (Eps → Vec), а затем из этого вектора получается нужный структурированный тип (Vec → Type).
*   **Для изображений (Eps → Img):** Можно подобрать архитектуру (например, генеративно-состязательные сети - GAN, декодеры вариационных автокодировщиков - VAE) так, чтобы она требовала меньше параметров, чем число пикселей в выходном изображении, используя свойства изображений (локальность, иерархичность).

**2.4. Преобразования для сложных типов (Img, Seq, Set, Tab)**
*   **Img и Seq:** Уже рассмотрены в предыдущих лекциях (CNN для Img, RNN/Transformers для Seq).
*   **Множество (Set):**
    *   **Set(X) → Vec (Агрегация):**
        1.  Перевести каждый элемент $x \in \text{Set}(X)$ в векторное представление с помощью функции $g: X \rightarrow \text{Vec}$.
        2.  Сагрегировать полученный набор векторов в один вектор с помощью инвариантной к порядку операции:
            *   Сумма: $\sum g(x_i)$
            *   Среднее: $\frac{1}{|S|} \sum g(x_i)$
            *   Средневзвешенное (если есть веса элементов)
            *   Максимум (покомпонентный)
            *   Использование механизма self-attention (например, как в Set Transformer).
    *   **Set(X) → Set(Y) (Поэлементное преобразование или более сложное):**
        1.  **Независимо:** Применить функцию $h: X \rightarrow Y$ к каждому элементу множества.
        2.  **С контекстом:** Применить функцию $h: X \rightarrow Z$ для получения эмбеддингов, затем агрегировать их в общий контекстный вектор (как в Set(X) → Vec), а затем использовать этот контекст для преобразования каждого элемента (например, $h': X \times Z \rightarrow Y$).
        3.  **Самовнимание (Self-Attention):** Элементы множества взаимодействуют друг с другом для получения контекстуализированных представлений, которые затем преобразуются в тип Y.
    *   **Vec → Set(X):** Можно использовать архитектуру для Vec → Seq(X) и затем рассматривать результат как множество (если порядок не важен). Или генерировать элементы множества один за другим, возможно, с условием на уже сгенерированные.
*   **Таблица (Tab):**
    Таблицу можно рассматривать как произведение множеств строк и множеств столбцов (или наоборот):
    $$ \text{Tab}(V) \approx \text{Set}_{\text{row}}(\text{Set}_{\text{col}}(V)) \times \text{Set}_{\text{col}}(\text{Set}_{\text{row}}(V)) $$
    Это означает, что можно применять агрегацию по строкам, а затем по столбцам (или наоборот), или использовать более сложные модели, учитывающие взаимодействия.
    Часто таблицы обрабатываются как последовательности строк (если порядок строк важен) или как множества строк. Для каждой строки (которая является типом-произведением признаков) можно применить Prod → Vec.

---

## 3. Частные случаи архитектур для ADT

**3.1. Объект как произведение признаков**
*   Тип объекта: $$ \text{Iris} = \text{SepalLength} \times \text{SepalWidth} \times \text{PetalLength} \times \text{PetalWidth} $$
*   Каждый признак $x_j$ (скаляр) умножается на соответствующий ему вектор весов $w_j$ (обучаемый эмбеддинг признака или столбец в матрице весов).
*   Результаты суммируются: $ \text{representation} = \sum_j x_j w_j $.
*   Если $x$ – это вектор-строка признаков, а $W$ – матрица, где столбцы – это $w_j$, то это эквивалентно умножению $xW$. Если каждый $x_j$ проецируется в вектор, а затем результаты суммируются, это похоже на линейный слой, где каждый входной признак имеет свой набор весов для каждого выходного нейрона.

**3.2. Объект как множество признаков**
*   Тип признака (категориальный):
    $$ \text{IrisFeature} = \text{SepalLengthValue} + \text{SepalWidthValue} + \dots $$
    (где `SepalLengthValue` – это тип, представляющий конкретное значение длины чашелистика, а не сам числовой тип `Real`).
*   Тип объекта: $$ \text{Iris} = \text{Set}(\text{IrisFeature}) $$ (каждый объект – это набор имеющихся у него признаков-значений).
*   Каждому возможному значению признака $f \in \text{IrisFeature}$ сопоставляется обучаемый вектор-эмбеддинг $v_f$.
*   Представление объекта – это сумма эмбеддингов тех признаков, которые у него есть: $ \sum_{f \in \text{object_features}} v_f $.
*   Это эквивалентно умножению матрицы эмбеддингов на one-hot (или multi-hot) вектор признаков объекта. Похоже на подход "мешок слов" (bag-of-words).

**3.3. Пропущенные значения**
*   Признак с возможным пропуском можно кодировать как тип-сумму:
    $$ \text{WeakSepalLength} = \text{SepalWidthValue} + \text{Eps} $$
    (где `SepalWidthValue` – тип для существующего значения, `Eps` – для пропуска).
*   **Подход (Sum → Vec):**
    *   Если значение есть (например, 5.1), то используется кодировщик $g_1: \text{SepalWidthValue} \rightarrow \mathbb{R}^n$. Например, $g_1(v_1) = v_1 \cdot a_1$.
    *   Если значение пропущено (Eps), то используется кодировщик $g_2: \text{Eps} \rightarrow \mathbb{R}^n$. Например, $g_2() = a_2$.
*   Это эквивалентно добавлению **индикатора пропуска**:
    *   Вход: $(v_1, \text{is_missing_indicator})$.
    *   Выход: $v_1 \cdot a_1 \cdot (1-\text{is_missing_indicator}) + a_2 \cdot \text{is_missing_indicator}$.
    *   На слайде: если значение 5.1, то $5.1 \cdot a_1 + 0 \cdot a_2$. Если пропуск (Eps), то $0 \cdot a_1 + 1 \cdot a_2$ (если $a_2$ - вектор для пропуска).

**3.4. Категории**
*   Категориальный признак можно закодировать как тип-сумму его возможных значений:
    $$ \text{IrisSpecies} = \text{Setosa} + \text{Versicolor} + \text{Virginica} $$
*   **Подход (Sum → Vec):** Каждому значению категории (Setosa, Versicolor, Virginica) сопоставляется свой обучаемый вектор-эмбеддинг.
    *   $g(\text{Setosa}) = v_1$
    *   $g(\text{Versicolor}) = v_2$
    *   $g(\text{Virginica}) = v_3$
*   Это эквивалентно стандартному подходу с **обучаемыми эмбеддингами** (embedding lookup): категориальное значение (или его целочисленный код) используется как индекс для выбора соответствующего вектора из матрицы эмбеддингов.

**3.5. Коллаборативная фильтрация**
*   Задача: предсказать взаимодействие (например, рейтинг) между пользователем (User) и элементом (Item).
*   Тип "Пара": $$ \text{Pair} = \text{User} \times \text{Item} $$
*   Тип "Пользователь" (категориальный): $$ \text{User} = \sum_i \text{User}_i $$
*   Тип "Элемент" (категориальный): $$ \text{Item} = \sum_j \text{Item}_j $$
*   Каждому User$_i$ сопоставляется эмбеддинг $u_i$, каждому Item$_j$ – эмбеддинг $v_j$.
*   Предсказание для пары $(User_i, Item_j)$ часто делается как скалярное произведение их эмбеддингов: $\langle u_i, v_j \rangle$.
*   Это соответствует подходу **матричного разложения** в рекомендательных системах.

**3.6. Конфигурация гиперпараметров (слайд 20)**
*   Пространство поиска гиперпараметров для алгоритма машинного обучения можно представить в виде дерева, где узлы соответствуют выбору компонента (например, типа ядра в SVM) или значения параметра.
*   **Вершина типа И (произведение):** Все дочерние узлы (параметры) должны быть выбраны. (Например, для knn нужны dist, window, k, h).
*   **Вершина типа ИЛИ (сумма):** Должен быть выбран один из дочерних узлов. (Например, для dist в knn выбирается manh, eucl или cheb).
*   **Лист:** Конкретное значение гиперпараметра.
*   Числа у узлов могут означать количество возможных конфигураций для данной подветви.
*   Поиск оптимальной конфигурации гиперпараметров (Hyperparameter Optimization, HPO) – сложная задача.

---

## 4. Графы

Графы – это общая структура данных, состоящая из вершин (узлов) и рёбер (связей между узлами). Многие типы данных можно представить в виде графов.

**4.1. Примеры графовых данных**
*   **Суперпиксели (Superpixels):** Группировка смежных пикселей изображения со схожими характеристиками. Суперпиксели можно рассматривать как узлы графа, а их смежность – как рёбра.
*   **Облако точек (Point Cloud):** Набор точек в 3D-пространстве (например, от LiDAR). Можно построить граф на основе близости точек.
*   **Молекулы:** Атомы – узлы, химические связи – рёбра.
*   **Социальные сети:** Люди – узлы, дружба/подписка – рёбра.
*   **Код программы:** Элементы синтаксического дерева или графа потока управления – узлы, зависимости – рёбра.

**4.2. Подходы к обработке графов с помощью нейронных сетей (Graph Neural Networks, GNNs)**

*   **Случайный обход (Random Walks):**
    *   Генерируются случайные пути (последовательности вершин) на графе.
    *   Эти последовательности затем обрабатываются методами для последовательностей (например, RNN, Word2Vec-подобные модели для получения эмбеддингов узлов, как в DeepWalk, node2vec).
    *   Получается множество последовательностей: $\text{Set}(\text{Seq}(X))$, где $X$ – тип узла.

*   **Передача сообщений (Message Passing):**
    *   Основная парадигма для многих GNN.
    *   Каждая вершина $v_i$ имеет векторное представление (эмбеддинг) $\text{vec}_l[v_i]$ на слое $l$.
    *   На каждом слое эмбеддинг вершины обновляется на основе агрегированной информации от ее соседей на предыдущем слое:
        $$ \text{vec}_{l}[v_i] = f_l \left( \sum_{(v_i, v_j) \in E} g_l(\text{vec}_{l-1}[v_j], \text{edge_features}(v_i,v_j)) , \text{vec}_{l-1}[v_i] \right) $$
        Или, как на слайде, более общая форма:
        $$ \text{vec}_l[v_i] = f_l \left( \text{AGGREGATE}_{ (v_j, v_i) \in E \text{ or } v_j \in \text{Neighbors}(v_i) } \{ g_l(\text{vec}_{l-1}[v_j]) \} \right) $$
        где $f_l$ – функция обновления, $g_l$ – функция сообщения (часто просто линейное преобразование или MLP), AGGREGATE – функция агрегации (сумма, среднее, макс).
    *   Предполагается, что всегда существует ребро из вершины в себя (для учета собственной информации).
    *   Рёбра тоже могут иметь метки (вектора признаков), которые учитываются функцией $g_l$.
    *   Для рёбер разного типа можно применять разные функции $g_l$.

*   **Добавление фиктивных рёбер:**
    *   Чтобы информация распространилась между дальними вершинами, число слоев (передач сообщений) должно быть не меньше диаметра графа.
    *   Для сокращения пути информации (уменьшения числа необходимых слоев) можно добавить фиктивные рёбра (например, между вершинами на расстоянии 2, или к "глобальному" узлу).

*   **Матричные методы (Спектральные GNN):**
    *   Используют матрицу смежности графа $A$ или матрицу Лапласа $L = D - A$ (где $D$ – диагональная матрица степеней вершин).
    *   Свёртка на графе может быть определена в спектральной области через собственные векторы Лапласиана (например, ChebNet, GCN).
    *   GCN (Graph Convolutional Network) слой:
        $$ H^{(l+1)} = \sigma(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)}) $$
        где $\tilde{A} = A + I$ (матрица смежности с добавленными петлями), $\tilde{D}$ – диагональная матрица степеней для $\tilde{A}$, $H^{(l)}$ – матрица активаций узлов на слое $l$, $W^{(l)}$ – матрица весов.

---

## 5. Частные случаи архитектур для графов

Различные типы данных можно представить как частные случаи графов, и GNN-подобные подходы могут быть адаптированы для них.

*   **Множество ($Set(X)$):**
    *   Можно закодировать как **полный граф (all-to-all)**, где каждый элемент связан с каждым другим.
    *   Или как **двудольный граф (all-to-one)**, где все элементы связаны с одним агрегирующим узлом "y".
    *   Или как **разреженный граф (sparse)**, если есть априорная информация о связях.
    *   При агрегации информации для вершины (если это all-to-all) используется агрегация множества (сумма, среднее, self-attention).

*   **Таблица ($Tab(X)$):**
    *   Может быть представлена как **двумерная решетка (grid graph)**, где узлы – ячейки таблицы, а рёбра соединяют соседние ячейки.
    *   Может быть представлена как **двудольный граф** между строками и столбцами.
    *   "all-to-all" на слайде (для таблицы) может означать, что каждая ячейка связана со всеми другими ячейками, или каждая строка/столбец агрегирует информацию со всех других строк/столбцов.
    *   "all-to-one" для таблицы: агрегация всех ячеек в одно представление.

*   **Последовательность ($Seq(X)$):**
    *   Линейный граф, где каждый элемент связан только со следующим (и/или предыдущим для двунаправленных моделей).
    *   "The quick brown fox jumps over the lazy dog" – каждый узел (слово) передает информацию следующему.

*   **Изображение ($Img(X)$):**
    *   Двумерный решетчатый граф.
    *   Если представить, что каждый пиксель связан с собой и 8-ю ближайшими соседями (как в ядре 3x3), то операция передачи сообщений на таком графе эквивалентна **свёртке 3x3**.
    *   Передача сообщений на графе изображения иногда так и называется "свёрткой на графе".
    *   "A pixel depends on 8 neighbors and itself" (для обновления состояния пикселя).
    *   "A pixel depends on 9 pixels from previous layer" (если предыдущий слой тоже решетка, и применяется ядро 3x3).

*   **Код программы:**
    *   Может быть представлен как абстрактное синтаксическое дерево (AST) или граф потока управления (CFG).
    *   Для кода часто недостаточно ациклического дерева, так как переменная определяется ее использованием (и наоборот).
    *   Это может потребовать двунаправленных рёбер или более сложных GNN архитектур (например, Gated Graph Neural Networks, GGNN).

---

# Лекция 11: Самообучение (Self-Supervised Learning)

**План лекции:**
1.  Введение в самообучение
2.  Автокодировщики
3.  Самообучение на изображениях (Pretext Tasks)
4.  Контрастное обучение
5.  Векторные представления слов (Word Embeddings)

---

## 1. Введение в самообучение

**1.1. Сценарий**
Проблема: Имеется **много объектов (данных), но мало размеченных меток**. Мы хотели бы эффективно **предобучиться** на объектах без меток, чтобы затем использовать полученные знания для задач с малым количеством размеченных данных.

**1.2. Определения**
*   **Самообучение (Self-Supervised Learning, SSL):** Подход в машинном обучении, при котором для обучения представлений используются задачи, где разметка (метки) генерируется автоматически из самих данных. Это позволяет использовать большие объемы неразмеченных данных. SSL является разновидностью обучения без учителя, но использует "псевдо-метки" для формулирования задачи как обучения с учителем.
*   **Предварительная задача (Pretext Task):** Задача с искусственно созданными метками (псевдо-метками), на которой обучается модель с целью выучить хорошие, обобщающие **представления (representations)** объектов. Сама по себе pretext task может не иметь практической ценности.
*   **Последующая задача (Downstream Task):** Целевая задача (например, классификация, детектирование), для решения которой используются представления, полученные на pretext task. Обычно к "замороженному" или дообучаемому экстрактору признаков (полученному на pretext task) добавляется простой дискриминатор (например, линейный слой или небольшая нейросеть), который обучается на малом количестве размеченных данных для downstream task.

**1.3. Связь с переносом знаний (Transfer Learning)**
(Слайд 5, такой же как в Лекции 1, слайд 41)
1.  **Общая предобучающая модель (Generic Network $A$):** Обучается на большом общем наборе данных $D_A$ для выполнения общей задачи $T_A$ (часто это pretext task в SSL).
2.  **Извлечение признаков (Pre-trained $A'$):** Часть модели $A$ (например, все слои кроме последнего классификационного) используется как экстрактор признаков.
3.  **Обучение на специфической задаче:** Экстрактор $A'$ комбинируется с новой обучаемой частью $B$ (Trainable) и дообучается (или обучается только $B$) на специфическом, часто меньшем, наборе данных $D_B$ для целевой задачи $T_B$.

**1.4. Основные концепции самообучения**
*   **Перенос знаний для извлечения признаков:** SSL – это способ реализовать первый этап переноса знаний (обучение $A$ на $D_A$) без необходимости в человеческой разметке для $D_A$.
*   **Дообучение (Fine-tuning):** Предобученная часть ($A'$) может быть "заморожена" (веса не меняются) или "разморожена" и дообучаться вместе с $B$ на задаче $T_B$ (часто с меньшей скоростью обучения для $A'$).
*   **Дистилляция знаний (Knowledge Distillation - напоминание, слайд 7, как в Лекции 2, слайд 32):**
    Метод, где меньшая "студенческая" модель обучается имитировать выходы (или внутренние представления) большей, предобученной "учительской" модели. Это может использоваться для сжатия моделей или переноса знаний.
    В SSL учительская модель может быть той же архитектуры, но с медленно обновляемыми весами (см. BYOL).
*   **Bootstrap Your Own Latent (BYOL, слайд 8):**
    Метод самообучения, использующий две нейронные сети: **online network** (ученик, параметры $\theta$) и **target network** (учитель, параметры $\xi$).
    1.  Из исходного изображения $x$ создаются два аугментированных представления (view) $v = t(x)$ и $v' = t'(x)$ с помощью функций аугментации $t, t'$.
    2.  **Online network** ($f_\theta, g_\theta, q_\theta$):
        *   $y_\theta = f_\theta(v)$ (representation)
        *   $z_\theta = g_\theta(y_\theta)$ (projection)
        *   $q_\theta(z_\theta)$ (prediction)
    3.  **Target network** ($f_\xi, g_\xi$):
        *   $y'_\xi = f_\xi(v')$
        *   $z'_\xi = g_\xi(y'_\xi)$
    4.  **Задача обучения:** Online network обучается предсказывать выход target network $z'_\xi$. Ошибка: $\text{loss} = ||q_\theta(z_\theta) - z'_\xi||_2^2$.
        Градиент **не** распространяется через target network (операция `sg` - stop-gradient на $z'_\xi$).
    5.  **Обновление учителя:** Параметры target network $\xi$ не обучаются градиентным спуском, а являются экспоненциально скользящим средним параметров online network $\theta$:
        $$ \xi \leftarrow \tau \xi + (1-\tau) \theta $$
        где $\tau$ - коэффициент затухания (например, 0.996).
    *   Архитектуры online и target сетей идентичны.
    *   BYOL не требует негативных примеров, в отличие от многих контрастных методов.

---

## 2. Автокодировщики (Autoencoders)

**2.1. Определение**
**Автокодировщик (autoencoder)** — это нейронная сеть, обучаемая задаче восстановления своего входа на выходе. Цель – выучить полезное **низкоразмерное представление (код, latent representation)** данных путем нелинейной трансформации.

**2.2. Основная идея**
Заставить сеть предсказывать (восстанавливать) то, что подается ей на вход, но с некоторым ограничением, которое не позволяет ей выучить тривиальное тождественное преобразование (просто скопировать вход на выход).

**2.3. Ограничение преобразования**
*   **Структурное ограничение (Недополненный автокодировщик / Undercomplete Autoencoder):**
    Между входным и выходным слоями имеется **скрытый слой с меньшей размерностью**, чем вход/выход. Этот слой называется **"бутылочным горлышком" (bottleneck)**. Сеть вынуждена сжимать информацию во входе в это низкоразмерное представление, сохраняя наиболее важные характеристики.
*   **Регуляризационное ограничение (Разреженный автокодировщик / Sparse Autoencoder):**
    К функции потерь добавляется член регуляризации, который штрафует активации нейронов в скрытом слое (коде), заставляя их быть разреженными (большинство активаций близки к нулю). Размерность кода может быть больше или равна размерности входа.

**2.4. Части автокодировщика**
*   **Кодировщик (Encoder):** Часть сети от входного слоя до "бутылочного горлышка" (скрытого кода $z$). Отображает вход $x$ в код $z = c(x)$.
*   **Декодировщик (Decoder):** Часть сети от "бутылочного горлышка" до выходного слоя. Отображает код $z$ в восстановленный вход $\hat{x} = d(z)$.
*   **Функция потерь:** Обычно среднеквадратичная ошибка между входом $x$ и восстановленным выходом $\hat{x}$: $L(x, \hat{x}) = ||x - d(c(x))||^2$.

**2.5. Модель свёрточного автокодировщика (Convolutional Autoencoder)**
*   Кодировщик состоит из свёрточных слоев и слоев пулинга, уменьшающих пространственную размерность и извлекающих признаки.
*   Декодировщик состоит из слоев, обратных свёртке (транспонированная свёртка или деконволюция) и пулингу (апсемплинг, unpooling), восстанавливающих изображение из латентного представления.

**2.6. Регуляризация для автокодировщика (кроме структурных ограничений)**
Можно добавить регуляризационный член к функции потерь, чтобы наложить дополнительные ограничения на код $c(x)$:
$$ \mathcal{L}(x, d(c(x))) = ||d(c(x)) - x||^2 + \tau \cdot \Omega(c(x)) $$
*   $c$ – кодировщик, $d$ – декодировщик.
*   $\Omega(c(x))$ – член регуляризации (например, $L_1$ норма активаций кода для разреженности, или регуляризатор, заставляющий производную кодировщика быть малой - см. сжимающий автокодировщик).
*   $\tau$ – коэффициент регуляризации.

**2.7. Вариации автокодировщиков**
*   **Разреженный (Sparse) автокодировщик:** Добавляет штраф за неразреженные активации в коде (например, KL-дивергенция между средней активацией нейрона и желаемым низким уровнем разреженности).
*   **Шумоподавляющий (Denoising) автокодировщик (DAE):**
    *   Вход $x$ сначала "портится" (добавляется шум) до $\tilde{x}$.
    *   Кодировщик получает $\tilde{x}$, а декодировщик пытается восстановить **оригинальный, чистый** $x$.
    *   $L(x, d(c(\tilde{x})))$.
    *   Заставляет модель учиться извлекать более робастные признаки, игнорируя шум.
*   **Сжимающий (Contractive) автокодировщик (CAE):**
    К функции потерь добавляется член, штрафующий норму Якобиана кодировщика по входу: $\Omega(c(x)) = ||\nabla_x c(x)||_F^2$.
    Заставляет кодировщик быть менее чувствительным к малым изменениям на входе, т.е. "сжимать" окрестности входных точек в латентном пространстве.
*   **Вариационный (Variational) автокодировщик (VAE):**
    Основан на вероятностных принципах и байесовском выводе. Кодировщик предсказывает параметры распределения (например, среднее $\mu_z$ и дисперсию $\sigma_z^2$ Гауссианы) в латентном пространстве. Код $z$ сэмплируется из этого распределения $q(z|x)$. Декодер $p(x|z)$ также вероятностный. Функция потерь включает член восстановления и член KL-дивергенции между $q(z|x)$ и априорным распределением $p(z)$ (обычно стандартная Гауссиана). (Будет рассмотрен подробнее в лекции о генеративных моделях).

---

## 3. Самообучение на изображениях (Pretext Tasks for Images)

Идея: создать pretext task, где метки можно автоматически сгенерировать из самого изображения.

**3.1. Определение поворота (Rotation Prediction)**
*   **Задача:** Изображение случайным образом поворачивается на один из фиксированных углов (например, 0°, 90°, 180°, 270°). Модель (обычно CNN) должна предсказать, какой поворот был применен.
*   **Псевдо-метка:** Примененный угол поворота (4 класса).
*   Модель учится понимать ориентацию объектов и их общую структуру.

**3.2. Предсказание контекста / Относительного положения патчей (Relative Patch Location)**
*   **Задача:** Из изображения вырезается центральный патч и еще один патч из одного из 8 окружающих его положений. Модель получает оба патча и должна предсказать относительное положение второго патча относительно центрального (8 классов).
*   Модель учится пространственным отношениям между частями объектов.

**3.3. Решение головоломки (Jigsaw Puzzle)**
*   **Задача:** Изображение делится на $N \times N$ патчей (например, 3x3). Патчи случайным образом перемешиваются. Модель должна предсказать исходную перестановку патчей (из $9!$ возможных перестановок, что слишком много; обычно выбирается подмножество перестановок).
*   Модель учится глобальной структуре изображения и взаимному расположению его частей.

**3.4. Что еще? (Другие Pretext Tasks)**
*   **Восстановление части изображения (Image Inpainting):** Часть изображения маскируется, модель должна ее восстановить.
*   **Раскрашивание изображений (Image Colorization):** Модели подается черно-белая версия изображения, она должна предсказать его цветную версию.
*   **Восстановление деталей / Супер-разрешение (Image Super-Resolution):** Модель получает изображение низкого разрешения и должна восстановить его версию высокого разрешения.
*   **Предсказывать слово по контексту (для текста):** См. Masked Language Model (BERT).
*   **Предсказывать контекст по слову (для текста):** См. Skip-gram (Word2Vec).

**3.5. Проблема артефактов: Хроматическая аберрация**
*   **Проблема:** Некоторые pretext tasks могут быть "обмануты" низкоуровневыми артефактами в данных, не связанными с семантикой. Например, из-за особенностей оптики камеры, цветовые каналы (R, G, B) могут быть немного смещены относительно друг друга (хроматическая аберрация). Модель может научиться определять относительное положение частей фотографии по этому сдвигу, а не по семантическому содержанию.
*   **Решение:** Для борьбы с этим можно применять аугментации, которые нивелируют такие артефакты, например, случайно сдвигать цветовые каналы друг относительно друга во время обучения.

---

## 4. Контрастное обучение (Contrastive Learning)

**4.1. Определение**
**Контрастное обучение** – это подход к самообучению, цель которого – научить такие представления (эмбеддинги) объектов, чтобы "похожие" объекты (например, разные аугментированные версии одного и того же объекта – **позитивные пары**) были близки в пространстве представлений, а "непохожие" объекты (случайные другие объекты из датасета – **негативные пары**) были далеки друг от друга.
Это обучение за счет объединения аугментированных или иным образом полученных трансформаций одного объекта в один "класс" с исходным объектом в парадигме метрического обучения или метрической классификации.

**4.2. Пример: использование кропов (вырезанных частей изображения)**
*   Из одного исходного изображения (например, кошки) вырезаются два случайных, но перекрывающихся кропа. Это **позитивная пара**. Их эмбеддинги должны быть "притянуты" (pull together) друг к другу.
*   Кроп из изображения кошки и кроп из совершенно другого изображения (например, собаки) образуют **негативную пару**. Их эмбеддинги должны быть "оттолкнуты" (push apart) друг от друга.
*   Цель: выучить такое векторное представление (эмбеддинг), чтобы части одного объекта были более похожи друг на друга (и на весь объект), чем на любой другой объект.

**4.3. Обучение на одном примере (One-Shot Learning) и нескольких примерах (Few-Shot Learning)**
*   **Сценарий:**
    *   Число классов может увеличиваться со временем (новые классы).
    *   В некоторых (новых) классах может быть очень мало размеченных примеров.
*   **One-Shot Learning:** Алгоритм должен уметь классифицировать объекты нового класса, имея всего **один** размеченный пример этого класса.
*   **Few-Shot Learning:** То же самое, но доступно **несколько** (мало) размеченных примеров нового класса.
*   Контрастное обучение хорошо подходит для предобучения представлений для таких задач, так как оно учит метрику схожести, которая может быть обобщена на новые классы.

**4.4. Добавление центроидов (для One-Shot/Few-Shot классификации)**
*   Обучение на одном примере часто основано на **метрической классификации:** новый объект относится к тому классу, чей "прототип" (центроид) наиболее близок к эмбеддингу этого объекта в выученном пространстве представлений.
*   Когда появляется новый класс с одним/несколькими примерами, эмбеддинг этого примера (или средний эмбеддинг) становится центроидом нового класса.
*   Важно, чтобы выученная метрика (благодаря контрастному или другому SSL обучению) позволяла хорошо разделять классы по их центроидам.

**4.5. Сиамская сеть (Siamese Network)**
*   Состоит из двух (или более) **идентичных подсетей** (с общими весами), которые параллельно обрабатывают два разных входа $x^{(1)}$ и $x^{(2)}$.
*   Каждая подсеть генерирует векторное представление (эмбеддинг) своего входа.
*   Эти два эмбеддинга затем используются для вычисления функции потерь, которая зависит от того, являются ли $x^{(1)}$ и $x^{(2)}$ "похожими" или "непохожими".

**4.6. Triplet Loss**
*   Функция потерь, часто используемая для обучения сиамских сетей или в задачах метрического обучения.
*   Использует тройки примеров:
    *   **Якорь (Anchor, $a$):** Базовый пример.
    *   **Позитивный (Positive, $p$):** Пример того же класса, что и якорь.
    *   **Негативный (Negative, $n$):** Пример другого класса.
*   Цель: расстояние между якорем и позитивным примером $dist(a,p)$ должно быть меньше, чем расстояние между якорем и негативным примером $dist(a,n)$, как минимум на некоторый зазор (margin, $\epsilon$).
    $$ \mathcal{L}(a,p,n) = \max(0, \text{dist}(a,p) - \text{dist}(a,n) + \epsilon) $$
    Если $dist(a,n) > dist(a,p) + \epsilon$, то потери равны нулю.
    В качестве $dist$ может использоваться евклидово расстояние, косинусное расстояние и т.д.

**4.7. Обучение и вывод с Triplet Loss / Сиамскими сетями**
*   **Обучение:** Сеть (кодировщик) обучается по батчам троек $(a,p,n)$ с помощью градиентного спуска, минимизируя Triplet Loss.
*   **Вывод (для классификации, например, One-Shot):**
    *   Для каждого известного класса хранится один или несколько эталонных объектов (центроидов) в виде их эмбеддингов.
    *   Для нового входного объекта вычисляется его эмбеддинг.
    *   Объект классифицируется по ближайшему центроиду (например, по косинусному расстоянию или евклидову).
    *   Чтобы добавить новый класс, его размеченный пример(ы) кодируется, и полученный эмбеддинг (или средний эмбеддинг) становится новым центроидом.

**4.8. Contrastive Language-Image Pre-training (CLIP)**
*   Модель, обучаемая на огромных парах (изображение, текст-описание).
*   Состоит из двух кодировщиков: один для изображений (Image Encoder, обычно CNN или ViT) и один для текста (Text Encoder, обычно Трансформер).
*   **Задача контрастного обучения:**
    *   Для батча из $N$ пар (изображение, текст) создается матрица схожести $N \times N$ между всеми эмбеддингами изображений и всеми эмбеддингами текстов (обычно косинусное сходство).
    *   Цель: максимизировать схожесть "правильных" пар (соответствующее изображение и его описание) и минимизировать схожесть "неправильных" пар (изображение и описание другого изображения).
    *   Функция потерь (симметричная):
        $$ \mathcal{L}(I, T) = -\frac{1}{N} \sum_i \log \frac{\exp(\langle I_i, T_i \rangle / \tau)}{\sum_j \exp(\langle I_i, T_j \rangle / \tau)} - \frac{1}{N} \sum_i \log \frac{\exp(\langle T_i, I_i \rangle / \tau)}{\sum_j \exp(\langle T_i, I_j \rangle / \tau)} $$
        где $\langle \cdot, \cdot \rangle$ - косинусное сходство, $\tau$ - температурный параметр.
        Это похоже на две кросс-энтропийные потери: одна для классификации правильного текста для каждого изображения, другая для классификации правильного изображения для каждого текста.
*   После обучения CLIP может использоваться для **zero-shot классификации изображений**:
    1.  Для каждого класса создается текстовое описание (например, "a photo of a dog").
    2.  Эти описания кодируются текстовым кодировщиком.
    3.  Входное изображение кодируется кодировщиком изображений.
    4.  Изображение классифицируется в тот класс, чье текстовое описание имеет наибольшее косинусное сходство с эмбеддингом изображения.

---

## 5. Векторные представления слов (Word Embeddings)

**5.1. Вопрос о представлении текста**
Модели машинного обучения работают с числовыми данными. Как представить текст (слова, предложения) в виде векторов?

**5.2. One-hot encoding**
*   Фиксируется словарь всех уникальных слов $|V|$.
*   Каждому слову $w$ присваивается уникальный индекс $i(w)$ от $0$ до $|V|-1$.
*   Слово $w$ представляется в виде вектора длины $|V|$, где на позиции $i(w)$ стоит 1, а на всех остальных – 0.
    $$ \text{vector}(w) = (0, \dots, 0, 1_{i(w)}, 0, \dots, 0) $$
*   **Недостатки:**
    *   Очень высокая размерность для больших словарей.
    *   Разреженные векторы.
    *   Не несет семантической информации (все слова ортогональны, т.е. "одинаково не похожи").

**5.3. Основная идея Word2Vec (и других распределенных представлений)**
*   **Дистрибутивная гипотеза (Zellig Harris, 1954):** Слова, встречающиеся в похожих контекстах, вероятно, будут иметь похожий смысл. ("You shall know a word by the company it keeps" - J.R. Firth).
*   **Основная идея:** Характеризовать (представлять) слова через их контексты.

**5.4. Word2Vec: Continuous Bag of Words (CBOW)**
*   **Задача (pretext task):** Предсказать текущее (центральное) слово по его окружающим словам (контексту).
*   **Архитектура:**
    1.  Контекстные слова (например, $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$) представляются своими векторами-эмбеддингами.
    2.  Эмбеддинги контекстных слов агрегируются (например, суммируются или усредняются) для получения вектора контекста.
    3.  Этот вектор контекста используется для предсказания эмбеддинга центрального слова $w_t$.
    4.  Обучаются эмбеддинги слов.
*   **Функция потерь:** Минимизировать отрицательный логарифм вероятности правильного центрального слова при данном контексте:
    $$ L = - \log P(w_i | \text{context}(w_i)) $$

**5.5. Word2Vec: Skip-gram**
*   **Задача (pretext task):** Предсказать окружающие слова (контекст) по текущему (центральному) слову.
*   **Архитектура:**
    1.  Центральное слово $w_t$ представляется своим вектором-эмбеддингом.
    2.  Этот эмбеддинг используется для предсказания эмбеддингов нескольких контекстных слов (например, $w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}$).
    3.  Обучаются эмбеддинги слов.
*   **Функция потерь:** Минимизировать сумму отрицательных логарифмов вероятностей правильных контекстных слов при данном центральном слове:
    $$ L = - \sum_{c \in \text{context}(w_i)} \log P(c | w_i) $$
    Обычно Skip-gram работает лучше для больших корпусов и редких слов.

**5.6. Как обучить Word2Vec? (Оптимизации)**
Вычисление Softmax по всему словарю (для $P(w_j | \dots)$) очень затратно.
*   **Иерархический Softmax (Hierarchical Softmax):** Словарь организуется в виде двоичного дерева (например, дерево Хаффмана). Вероятность слова вычисляется как произведение вероятностей прохождения по путям в дереве.
*   **Негативное сэмплирование (Negative Sampling):**
    *   Для каждой пары (центральное слово, контекстное слово) из обучающих данных (позитивный пример) генерируется несколько **негативных примеров** путем замены контекстного слова на случайное слово из словаря (не из текущего контекста).
    *   Модель обучается отличать позитивные пары от негативных (бинарная классификация).
    *   Функция потерь для каждой пары $(w, c_{pos})$ и $k$ негативных сэмплов $c_{neg,j}$:
        $$ L = -\log \sigma(\text{score}(w, c_{pos})) - \sum_{j=1}^k \log(1 - \sigma(\text{score}(w, c_{neg,j}))) $$
        где $\text{score}(w,c)$ - это, например, скалярное произведение эмбеддингов $w$ и $c$.
*   **Субсэмплирование часто встречаемых слов:** Очень частые слова (например, "the", "a") несут мало информации о контексте. Они субсэмплируются (выкидываются с некоторой вероятностью), чтобы ускорить обучение и улучшить качество эмбеддингов для более редких слов.

**5.7. Свойства Word2Vec (и других распределенных эмбеддингов)**
*   Слова с похожими значениями имеют близкие векторы в пространстве эмбеддингов (например, "король" и "королева", "собака" и "кот").
*   Обнаруживаются **аналогии** в виде векторной арифметики:
    *   $\text{vec}(\text{"king"}) - \text{vec}(\text{"man"}) + \text{vec}(\text{"woman"}) \approx \text{vec}(\text{"queen"})$
    *   $\text{vec}(\text{"Paris"}) - \text{vec}(\text{"France"}) + \text{vec}(\text{"Italy"}) \approx \text{vec}(\text{"Rome"})$
*   Эти эмбеддинги используются как входные признаки для многих NLP моделей.

---

# Лекция 12: Генеративные модели

**План лекции:**
1.  Задача генерации новых объектов
2.  Авторегрессионные модели
3.  Вариационные автокодировщики (VAE)
4.  Генеративно-состязательные модели (GAN)
5.  Интересные идеи в GAN-ах
6.  Диффузные модели

---

## 1. Задача генерации новых объектов

**1.1. Какой это тип задачи?**
Генерация новых объектов, похожих на существующие, но не являющихся их точными копиями, может рассматриваться в контексте разных парадигм МО:
*   **С учителем (Supervised Learning):** Если есть явные метки или условия для генерации (например, "сгенерируй изображение кошки"). Conditional GANs, Text-to-Image модели.
*   **Без учителя (Unsupervised Learning):** Наиболее частый случай. Модель обучается на неразмеченных данных и пытается уловить их внутреннюю структуру и распределение для генерации новых образцов. VAE, большинство GAN, авторегрессионные модели.
*   **Частичное с учителем (Semi-Supervised Learning):** Используются и размеченные, и неразмеченные данные.
*   **С подкреплением (Reinforcement Learning):** Генератор может рассматриваться как агент, получающий вознаграждение за "хорошие" сгенерированные объекты.

**1.2. Определение препятствий для создания новых объектов**
*   **Новизна и принадлежность к множеству:** Сложность зависит от того, как мы определяем "новизну" объекта и как измеряем его "похожесть" на желаемое распределение данных.
*   **Эмпирические правила:** Если можно создать набор правил для генерации (например, процедурная генерация), то машинное обучение может не потребоваться.
*   **Контекст машинного обучения:** Мы хотим, чтобы:
    *   Сгенерированный объект был валидным (например, сгенерированное изображение было осмысленным изображением).
    *   Сгенерированный объект принадлежал к той же **предметной области** (data manifold), что и обучающие данные.
        *   Все детали должны соответствовать предметной области.
        *   Взаимодействие между деталями должно быть реалистичным.

**1.3. Препятствие в контексте машинного обучения**
*   **Проблема обобщения:** Если модель просто запомнит обучающие примеры, учёт (генерация) действительно **новых** объектов маловероятен.
*   **Скрытая структура:** Мы хотим создать объект, который правдоподобен относительно некоторой **скрытой структуры** или **распределения вероятностей** $p_{data}(x)$, лежащего в основе обучающих данных.
*   **Задача обучения без учителя:**
    1.  Изучить скрытую структуру объектов из обучающей выборки.
    2.  Выучить (аппроксимировать) распределение $p_{data}(x)$.
    3.  Сэмплировать (выбирать) новые объекты из выученного распределения $p_{model}(x)$.
    Цель: $p_{model}(x)$ должно быть похоже на $p_{data}(x)$.

**1.4. Как измерить сходство распределений?**
*   **Дивергенция Кульбака-Лейблера (KL-Divergence):**
    $$ D_{KL}(P || Q) = \int_{-\infty}^{\infty} p(x) \log \frac{p(x)}{q(x)} dx $$
    (Для дискретных распределений: $\sum_x p(x) \log \frac{p(x)}{q(x)}$).
    *   $p(x)$ – истинное распределение данных, $q(x)$ – распределение, моделируемое генератором.
    *   Также называется **относительной энтропией** $P$ по отношению к $Q$.
    *   **Несимметрична:** $D_{KL}(P || Q) \ne D_{KL}(Q || P)$. Поэтому это не расстояние в математическом смысле.
    *   $D_{KL}(P || Q) \ge 0$, и $D_{KL}(P || Q) = 0$ только если $P=Q$.
*   Другие меры: расстояние Вассерштейна, Jensen-Shannon дивергенция.

**1.5. Сложности генеративного моделирования**
*   **Оценка плотности $p_{data}(x)$:** Ключевая проблема в обучении без учителя.
    *   **Явная (Explicit) оценка плотности:** Модель напрямую определяет и обучается параметрической форме для $p_{model}(x)$ (например, авторегрессионные модели, VAE).
    *   **Неявная (Implicit) оценка плотности:** Модель обучается генерировать выборку из $p_{model}(x)$ без явного определения самой функции плотности (например, GAN).
*   **Проклятие размерности:** Восстанавливать многомерные распределения (например, для изображений высокого разрешения) очень сложно.

**1.6. Таксономия генеративных моделей (слайд 11)**
Иерархическая классификация подходов:
*   **Верхний уровень:** Maximum Likelihood (явные и неявные модели плотности).
*   **Явные модели плотности (Explicit Density):**
    *   **Вычислимые (Tractable Density):** Плотность можно вычислить аналитически.
        *   Fully visible belief nets (FVBNs, например, авторегрессионные модели как PixelRNN, NADE).
        *   Модели с изменением переменных (Change of variables models / Normalizing Flows).
    *   **Аппроксимированные (Approximate Density):** Плотность аппроксимируется.
        *   **Вариационные (Variational):** VAE.
        *   **Марковские цепи (Markov Chain):** Машины Больцмана.
*   **Неявные модели плотности (Implicit Density):**
    *   **Прямые (Direct):** GAN.
    *   **Марковские цепи (Markov Chain):** GSN (Generative Stochastic Networks).

**1.7. Оценка качества генерации**
*   **Inception Score (IS):**
    *   Использует предобученную классификационную модель Inception.
    *   Для сгенерированных изображений:
        1.  $p(y|x)$: предсказание класса $y$ для сгенерированного изображения $x$.
        2.  $p(y) = \int p(y|x)p_{model}(x)dx$: маргинальное распределение классов.
    *   $IS = \exp(\mathbb{E}_{x \sim p_{model}} [D_{KL}(p(y|x) || p(y))])$.
    *   **Высокий IS означает:**
        *   Низкая энтропия $p(y|x)$ (уверенность в предсказании класса для каждого изображения, т.е. изображения четкие и узнаваемые).
        *   Высокая энтропия $p(y)$ (большое разнообразие генерируемых классов).
*   **Fréchet Inception Distance (FID):**
    *   Сравнивает распределения активаций из некоторого слоя Inception-модели для реальных и сгенерированных изображений.
    *   Предполагается, что эти активации (признаки) следуют многомерному нормальному распределению.
    *   $FID = ||\mu_r - \mu_g||^2 + \text{Tr}(\Sigma_r + \Sigma_g - 2(\Sigma_r \Sigma_g)^{1/2})$.
    *   $\mu_r, \Sigma_r$ – среднее и ковариация для реальных данных, $\mu_g, \Sigma_g$ – для сгенерированных.
    *   **Меньшее значение FID указывает на большее сходство** распределений и лучшее качество генерации. Считается более робастной метрикой, чем IS.

---

## 2. Авторегрессионные модели

**2.1. Глубокая сеть доверия (Deep Belief Network - DBN, исторический контекст)**
Хотя DBN не являются авторегрессионными в современном смысле для генерации изображений, идея разложения совместной вероятности важна.
*   Объект $x$ (например, изображение) разбивается на части $x_i$ (например, пиксели).
*   **Явная модель плотности:** Используется цепное правило вероятностей для разложения $p(x)$:
    $$ p(x) = p(x_1) p(x_2|x_1) p(x_3|x_1, x_2) \dots p(x_n|x_1, \dots, x_{n-1}) = \prod_{i=1}^n p(x_i | x_1, \dots, x_{i-1}) $$
*   **Генерация:** Последовательно сэмплируется $x_1 \sim p(x_1)$, затем $x_2 \sim p(x_2|x_1)$ и т.д.
*   **Обучение:** Максимизируется правдоподобие наблюдаемых данных.
*   Условные вероятности $p(x_i | x_{<i})$ моделируются нейронными сетями.

**2.2. PixelRNN**
*   Авторегрессионная модель для генерации изображений пиксель за пикселем.
*   Генерация пикселей изображения обычно начинается с левого верхнего угла и идет по строкам (или по диагоналям).
*   Зависимость текущего пикселя $x_i$ от всех **предыдущих** сгенерированных пикселей $x_{<i}$ моделируется с помощью RNN (например, LSTM, часто двумерной LSTM, сканирующей изображение по двум направлениям).
*   На выходе RNN предсказывает параметры распределения для значения текущего пикселя (например, для RGB – 3 $\times$ 256 классов для каждого канала).

**2.3. PixelCNN**
*   **Проблема PixelRNN:** Последовательная генерация через RNN очень медленная.
*   **Идея PixelCNN:** Использовать свёрточные нейронные сети (CNN) вместо RNN для моделирования зависимостей от предыдущих пикселей.
*   **Маскированные свёртки (Masked Convolutions):** Ядра свёрток конструируются таким образом, чтобы при вычислении активации для пикселя $x_i$ они "видели" только те пиксели, которые были сгенерированы до $x_i$ в выбранном порядке (например, пиксели выше и левее).
*   Генерация по-прежнему последовательная (пиксель за пикселем), но вычисление условных вероятностей на этапе обучения может быть распараллелено (с помощью CNN).

**2.4. Анализ PixelRNN/PixelCNN (Pixel*NN)**
*   **Преимущества:**
    *   Могут явно вычислить правдоподобие $p(x)$ для любого изображения $x$.
    *   Явное правдоподобие обучающих данных дает хорошую и интерпретируемую метрику качества модели (можно сравнивать $log p(x)$).
    *   Генерируют хорошие, четкие образцы (особенно PixelCNN++).
*   **Недостатки:**
    *   **Последовательная генерация очень медленная**, так как каждый пиксель генерируется по одному.

**2.5. Повышение производительности PixelCNN**
*   **Закрытые свёрточные слои (Gated Convolutional Layers):** Использование вентильных механизмов (по аналогии с LSTM/GRU) в свёрточных слоях для лучшего моделирования зависимостей.
*   **Укороченные соединения (Short-cut / Residual Connections):** Облегчают обучение глубоких сетей.
*   **Дискретные логистические потери (Discretized Logistic Mixture Likelihood):** Вместо предсказания 256 классов для каждого пикселя, предсказываются параметры смеси логистических распределений, что лучше моделирует непрерывную природу значений пикселей.
*   **"Мультимасштаб":** Использование признаков с разных уровней разрешения.
*   Различные трюки в процессе обучения (например, планировщики скорости обучения).

---

## 3. Вариационные автокодировщики (Variational Autoencoders, VAE)

(Напоминание структуры автокодировщика: слайд 22)

**3.1. Основная вероятностная идея**
Вместо прямого моделирования сложного распределения данных $p_{data}(x)$, VAE вводит **скрытые (латентные) переменные** $z$, которые, как предполагается, порождают наблюдаемые данные $x$.
$$ p_\theta(x) = \int p_\theta(x|z) p(z) dz $$
*   $p(z)$: Априорное распределение латентных переменных (обычно простое, например, стандартное нормальное $N(0,I)$).
*   $p_\theta(x|z)$: Условное распределение данных при данном $z$ (вероятностная модель декодера), параметризованное $\theta$.
*   Интеграл обычно неразрешим аналитически.

**3.2. Генеративный процесс в VAE**
1.  Сэмплируем латентный вектор $z^{(i)}$ из априорного распределения $p(z)$.
2.  Сэмплируем наблюдаемый вектор $x^{(i)}$ из условного распределения $p_\theta(x|z^{(i)})$ (с помощью декодера).
Декодер $p_\theta(x|z)$ (нейронная сеть) отображает $z$ в параметры распределения для $x$ (например, среднее и дисперсию Гауссианы, если $x$ непрерывный, или параметры Бернулли, если $x$ бинарный).

**3.3. Проблема обучения параметров $\theta$**
Мы хотим максимизировать правдоподобие данных: $\theta^* = \arg \max_\theta \sum_i \log p_\theta(x^{(i)})$.
$\log p_\theta(x) = \log \int p_\theta(x|z) p(z) dz$. Этот интеграл трудноразрешим.
Также трудноразрешима апостериорная вероятность $p_\theta(z|x) = \frac{p_\theta(x|z)p(z)}{p_\theta(x)}$.

**3.4. Введение сети кодировщика (аппроксимация апостериорного распределения)**
Идея: ввести вторую нейронную сеть – **кодировщик (encoder) $q_\phi(z|x)$** – которая аппроксимирует истинное, но трудновычислимое апостериорное распределение $p_\theta(z|x)$.
*   Кодировщик $q_\phi(z|x)$ отображает вход $x$ в параметры распределения для $z$ (например, $\mu_z(x)$ и $\Sigma_z(x)$ для Гауссианы).

**3.5. Сэмплирование с кодировщиком и декодировщиком (во время обучения)**
1.  Для данного $x^{(i)}$ из обучающей выборки, кодировщик $q_\phi(z|x^{(i)})$ выдает параметры $(\mu_z^{(i)}, \Sigma_z^{(i)})$.
2.  Сэмплируем $z^{(i)} \sim N(\mu_z^{(i)}, \Sigma_z^{(i)})$.
3.  Декодер $p_\theta(x|z^{(i)})$ используется для восстановления $\hat{x}^{(i)}$.

**3.6. Вывод функции потерь (Evidence Lower Bound, ELBO)**
Рассмотрим $\log p_\theta(x^{(i)})$. Используя $q_\phi(z|x^{(i)})$, можно показать:
$$ \log p_\theta(x^{(i)}) = \mathbb{E}_{z \sim q_\phi(z|x^{(i)})} [\log p_\theta(x^{(i)}|z)] - D_{KL}(q_\phi(z|x^{(i)}) || p(z)) + D_{KL}(q_\phi(z|x^{(i)}) || p_\theta(z|x^{(i)})) $$
Так как $D_{KL}(\cdot || \cdot) \ge 0$, то:
$$ \log p_\theta(x^{(i)}) \ge \underbrace{\mathbb{E}_{z \sim q_\phi(z|x^{(i)})} [\log p_\theta(x^{(i)}|z)] - D_{KL}(q_\phi(z|x^{(i)}) || p(z))}_{\mathcal{L}(x^{(i)}, \theta, \phi) \text{ - ELBO}} $$
Эта нижняя граница (ELBO) и максимизируется при обучении VAE.
*   **Первый член (реконструкция):** $\mathbb{E}_{z \sim q_\phi(z|x^{(i)})} [\log p_\theta(x^{(i)}|z)]$. Это ожидаемое логарифмическое правдоподобие восстановления $x^{(i)}$ из $z$, сэмплированного из $q_\phi(z|x^{(i)})$. Поощряет точное восстановление.
*   **Второй член (регуляризация):** $D_{KL}(q_\phi(z|x^{(i)}) || p(z))$. KL-дивергенция между аппроксимированным апостериорным распределением $q_\phi(z|x^{(i)})$ и априорным $p(z)$. Заставляет $q_\phi(z|x^{(i)})$ быть близким к $p(z)$ (например, к стандартной Гауссиане), что делает латентное пространство более гладким и структурированным.

**3.7. Обучение VAE**
*   Максимизируем ELBO $\mathcal{L}(x^{(i)}, \theta, \phi)$ по параметрам $\theta$ (декодер) и $\phi$ (кодировщик).
*   **Репараметризационный трюк (Reparameterization Trick):** Чтобы градиент мог проходить через процесс сэмплирования $z \sim q_\phi(z|x)$, если $q_\phi(z|x)$ – Гауссиана $N(\mu_z(x), \Sigma_z(x))$, то $z$ сэмплируется как $z = \mu_z(x) + \epsilon \cdot \sqrt{\Sigma_z(x)}$, где $\epsilon \sim N(0,I)$. Теперь $\mu_z$ и $\Sigma_z$ детерминистически зависят от $x$ и $\phi$.
*   Для каждого мини-батча: прямой проход для вычисления ELBO, затем обратный проход для обновления $\theta, \phi$.

**3.8. Создание данных с помощью обученного VAE**
Используется только декодер:
1.  Сэмплируем $z \sim p(z)$ (например, из $N(0,I)$).
2.  Генерируем $\hat{x} \sim p_\theta(x|z)$ (пропускаем $z$ через декодер, чтобы получить параметры распределения для $x$, и сэмплируем $x$).

**3.9. Анализ VAE**
*   **Преимущества:**
    *   Принципиальный вероятностный подход к генеративным моделям.
    *   Дает возможность получить осмысленное латентное пространство $q_\phi(z|x)$, которое может быть полезно для других задач (например, как экстрактор признаков).
    *   Позволяет явно вычислять (аппроксимацию) правдоподобия данных.
*   **Недостатки:**
    *   Максимизация ELBO – это не то же самое, что максимизация истинного правдоподобия. Качество оценки правдоподобия не такое высокое, как у PixelRNN/PixelCNN.
    *   Генерируемые образцы часто **более размытые** и менее качественные по сравнению с GAN. Это связано с тем, что член реконструкции в ELBO (часто MSE для непрерывных данных) поощряет усреднение, а KL-член может слишком сильно "сглаживать" латентное пространство.

---

## 4. Генеративно-состязательные модели (Generative Adversarial Networks, GAN)

**4.1. Основные идеи**
*   Не пытаться непосредственно моделировать или аппроксимировать распределение плотности $p_{data}(x)$.
*   Обучать **преобразование (генератор)**, которое отображает случайный шум $z$ (из простого априорного распределения, например, $N(0,I)$) в объекты $x$, похожие на реальные.
*   Использовать **теоретико-игровой подход**: состязание двух нейронных сетей.

**4.2. Генератор vs Дискриминатор**
*   **Генератор ($G$):** Нейронная сеть, которая генерирует объекты $x_{gen} = G(z)$, пытаясь "обмануть" дискриминатор, чтобы он принял их за настоящие.
*   **Дискриминатор ($D$):** Нейронная сеть, которая обучается отличать настоящие объекты $x_{real}$ (из обучающей выборки) от сгенерированных объектов $x_{gen}$. Выдает вероятность того, что входной объект является настоящим.
*   Обучение построено как противоборство (минимаксная игра) этих двух "игроков".

**4.3. Обучение GAN-ов (слайд 38)**
*   Генератор получает на вход случайный шум $z$ и производит сэмпл $G(z)$.
*   Дискриминатор получает на вход либо реальные изображения из датасета, либо сгенерированные генератором.
*   Обе сети являются дифференцируемыми модулями.

**4.4. Минимаксная игра**
Целевая функция (value function $V(D,G)$) для GAN:
$$ \min_G \max_D V(D,G) = \mathbb{E}_{x \sim p_{data}(x)}[\log D(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D(G(z)))] $$
*   $D(x)$: вероятность того, что дискриминатор считает $x$ настоящим.
*   $G(z)$: объект, сгенерированный из шума $z$.
*   $p_{data}(x)$: распределение реальных данных.
*   $p_z(z)$: априорное распределение шума (например, Гауссиана).

*   **Дискриминатор $D$ (с параметрами $\theta_d$) пытается максимизировать эту функцию:**
    *   $D(x_{real}) \rightarrow 1$ (правильно классифицировать реальные как реальные).
    *   $D(G(z)) \rightarrow 0$ (правильно классифицировать сгенерированные как поддельные).
*   **Генератор $G$ (с параметрами $\theta_g$) пытается минимизировать эту функцию** (эквивалентно, заставить $D(G(z)) \rightarrow 1$, т.е. обмануть дискриминатор).

**4.5. Поочередное обучение**
На практике $D$ и $G$ обучаются поочередно:
1.  **Градиентный подъем по дискриминатору:**
    Фиксируем $G$, обновляем $D$ для максимизации $V(D,G)$.
    $$ \max_{\theta_d} \left( \mathbb{E}_{x \sim p_{data}}[\log D_{\theta_d}(x)] + \mathbb{E}_{z \sim p_z(z)}[\log(1 - D_{\theta_d}(G_{\theta_g}(z)))] \right) $$
    Обычно делается несколько шагов обновления $D$ на один шаг $G$.
2.  **Градиентный спуск по генератору:**
    Фиксируем $D$, обновляем $G$ для минимизации $V(D,G)$.
    $$ \min_{\theta_g} \mathbb{E}_{z \sim p_z(z)}[\log(1 - D_{\theta_d}(G_{\theta_g}(z)))] $$

**4.6. Проблема с обучением генератора (насыщение градиента)**
*   Когда дискриминатор очень хорошо отличает подделки ($D(G(z)) \approx 0$), то $\log(1 - D(G(z)))$ очень близок к $\log(1) = 0$, и его градиент по $\theta_g$ становится очень маленьким (насыщается). Генератор плохо обучается.
*   **Решение (изменение обучения генератора):** Вместо минимизации $\mathbb{E}[\log(1 - D(G(z)))]$, генератор максимизирует $\mathbb{E}[\log D(G(z))]$.
    $$ \max_{\theta_g} \mathbb{E}_{z \sim p_z(z)}[\log D_{\theta_d}(G_{\theta_g}(z))] $$
    Эта функция потерь имеет более сильные градиенты, когда $G$ плохо справляется (т.е. когда $D(G(z))$ мало).
*   Графики (слайды 41, 42) показывают, что $\log D(G(z))$ (или $-\log D(G(z))$ при минимизации) дает лучший сигнал градиента, когда $D(G(z))$ мало (sample is likely fake), по сравнению с $\log(1-D(G(z)))$.

**4.7. Шаги обучения (визуализация)**
*   **Шаг обучения дискриминатора (слайд 43):** Параметры генератора "заморожены". Дискриминатор обучается на реальных и сгенерированных примерах. Обратное распространение ошибки для обновления весов дискриминатора.
*   **Шаг обучения генератора (слайд 44):** Параметры дискриминатора "заморожены". Генератор производит образцы, они проходят через дискриминатор. Обратное распространение ошибки (через дискриминатор до генератора) для обновления весов генератора, чтобы он лучше "обманывал" дискриминатор.

**4.8. Динамика обучения (слайд 45)**
Обучение GAN – это поиск равновесия Нэша в игре между $G$ и $D$. В идеале $p_{model}$ (распределение, генерируемое $G$) сходится к $p_{data}$, и $D(x) = 0.5$ везде. Однако на практике сходимость не гарантирована.

**4.9. Недостатки GANs**
*   **Нет гарантии равновесия/сходимости:** Обучение может быть нестабильным.
*   **Схлопывание мод (Mode Collapse):** Генератор может научиться производить только очень ограниченное разнообразие образцов (например, только один или несколько типов изображений из многообразного датасета), которые хорошо обманывают дискриминатор. Он "забывает" о других модах распределения данных.
*   **Осцилляции:** Параметры $G$ и $D$ могут осциллировать, не достигая стабильной точки.
*   **Нет индикатора, когда останавливаться:** Сложно определить, когда модель достаточно обучилась, так как функция потерь не всегда коррелирует с качеством генерируемых изображений. Требуется визуальная оценка или использование метрик (IS, FID).

**4.10. Mode collapse и осцилляции (визуализация, слайд 47)**
Показано, как генератор может начать с воспроизведения шума, затем сфокусироваться на одной моде данных, а затем осциллировать между разными модами или генерировать некачественные образцы.

---

## 5. Интересные идеи в GAN-ах

**5.1. Conditional GANs (cGANs)**
*   **Идея:** Добавить дополнительную информацию (условие, метку класса $y$) как генератору, так и дискриминатору.
    *   Генератор: $G(z,y)$ генерирует объект класса $y$ из шума $z$.
    *   Дискриминатор: $D(x,y)$ оценивает, является ли $x$ реальным объектом класса $y$.
*   Это позволяет генерировать объекты, принадлежащие к определенному классу или обладающие определенными атрибутами.
*   **Целевая функция для cGAN:**
    $$ \min_G \max_D V(D,G) = \mathbb{E}_{(x,y) \sim p_{data}(x,y)}[\log D(x,y)] + \mathbb{E}_{z \sim p_z(z), y \sim p_y(y)}[\log(1 - D(G(z,y),y))] $$
    (Предполагается, что $y$ для генератора сэмплируется из того же распределения, что и метки реальных данных).
*   Дискриминатор теперь должен не только отличать реальное от поддельного, но и проверять соответствие объекта метке $y$.

**5.2. Задача оптимального перемещения и Расстояние Вассерштейна (Wasserstein Distance)**
*   KL-дивергенция и Jensen-Shannon дивергенция (на которой основана оригинальная функция потерь GAN) могут быть не лучшими способами измерения "расстояния" между распределениями, особенно если их носители не пересекаются (что часто бывает на ранних этапах обучения GAN). Это может приводить к проблемам с градиентами.
*   **Расстояние Вассерштейна (Earth Mover's Distance):** Интуитивно, минимальная "стоимость" превращения одного распределения в другое (как перемещение кучи земли).
    $$ W(p_{data}, p_{gen}) = \inf_{\gamma \in \Pi(p_{data}, p_{gen})} \mathbb{E}_{(x,y) \sim \gamma} [||x-y||] $$
    где $\Pi(p_{data}, p_{gen})$ – множество всех совместных распределений $\gamma(x,y)$, чьи маргинальные распределения равны $p_{data}$ и $p_{gen}$ соответственно.
*   **WGAN (Wasserstein GAN):**
    *   Использует расстояние Вассерштейна в качестве функции потерь.
    *   Благодаря двойственности Канторовича-Рубинштейна, $W(p_{data}, p_{gen})$ можно оценить как:
        $$ W(p_{data}, p_{gen}) = \sup_{||f||_L \le 1} ( \mathbb{E}_{x \sim p_{data}}[f(x)] - \mathbb{E}_{x \sim p_{gen}}[f(x)] ) $$
        где супремум берется по всем 1-Липшицевым функциям $f$.
    *   Дискриминатор (называемый **критиком** в WGAN) обучается аппроксимировать эту функцию $f$.
    *   Для обеспечения Липшицевости, веса критика "обрезаются" (weight clipping) до небольшого диапазона, или используется градиентный штраф (gradient penalty, WGAN-GP).
    *   WGAN часто приводит к более стабильному обучению и помогает бороться с mode collapse.

**5.3. Прогрессивное обучение GAN (Progressive GAN, ProGAN)**
*   **Идея:** Начинать генерацию изображений очень низкого разрешения (например, 4x4), а затем постепенно добавлять слои как в генератор, так и в дискриминатор, увеличивая разрешение генерируемых изображений (8x8, 16x16, ..., 1024x1024).
*   Это позволяет модели сначала научиться глобальной структуре, а затем добавлять детали.
*   Приводит к генерации высококачественных изображений высокого разрешения. (Слайд 52, 53 - примеры лиц, сгенерированных StyleGAN, наследником ProGAN).

**5.4. CycleGAN (Обучение на непарных данных)**
*   **Задача:** Преобразование изображений из одного домена в другой без наличия парных обучающих примеров (например, превратить фото лошади в зебру, лето в зиму).
*   Использует два генератора ($G_{A \rightarrow B}$ и $G_{B \rightarrow A}$) и два дискриминатора ($D_A$ и $D_B$).
*   **Ключевая идея: Циклическая согласованность (Cycle Consistency Loss).**
    Если изображение из домена A перевести в домен B ($X_B = G_{A \rightarrow B}(X_A)$), а затем обратно в домен A ($\hat{X}_A = G_{B \rightarrow A}(X_B)$), то результат $\hat{X}_A$ должен быть близок к исходному $X_A$. И наоборот.
    $$ L_{cyc}(G_{A \rightarrow B}, G_{B \rightarrow A}) = \mathbb{E}_{x_A \sim p_A}[||G_{B \rightarrow A}(G_{A \rightarrow B}(x_A)) - x_A||_1] + \mathbb{E}_{x_B \sim p_B}[||G_{A \rightarrow B}(G_{B \rightarrow A}(x_B)) - x_B||_1] $$
*   Общая функция потерь включает стандартные состязательные потери для $G_{A \rightarrow B}$ (с $D_B$) и $G_{B \rightarrow A}$ (с $D_A$), а также циклическую потерю.

**5.5. Доменная адаптация с помощью состязательного обучения (слайд 55)**
*   **Задача:** Адаптировать модель, обученную на одном (source) домене, для работы на другом, но похожем (target) домене, где размеченных данных мало или нет.
*   **Идея:** Обучить экстрактор признаков $G_f$ так, чтобы он генерировал представления, для которых **доменный классификатор $G_d$ не мог бы отличить, из какого домена (source или target) пришел исходный объект**.
*   Экстрактор признаков $G_f$ обучается минимизировать потери на основной задаче (например, классификации $G_y$ на source домене) и **максимизировать** потери доменного классификатора $G_d$ (через градиентный реверсивный слой, gradient reversal layer, который меняет знак градиента при обратном распространении).
*   Это заставляет $G_f$ учить признаки, инвариантные к домену.

---

## 6. Диффузные модели (Denoising Diffusion Probabilistic Models, DDPM)

**6.1. Общая схема обучения**
*   **Идея:** Основана на двух процессах:
    1.  **Прямой процесс (диффузия, фиксированный):** К исходному изображению $x_0$ итеративно (на протяжении $T$ шагов) добавляется небольшой Гауссовский шум, постепенно превращая его в чистый шум $x_T \sim N(0,I)$.
        $$ q(x_t | x_{t-1}) = N(x_t; \sqrt{1-\beta_t} x_{t-1}, \beta_t I) $$
        где $\beta_t$ – небольшой параметр, контролирующий уровень шума на шаге $t$.
        Можно напрямую сэмплировать $x_t$ из $x_0$: $q(x_t|x_0) = N(x_t; \sqrt{\bar{\alpha}_t}x_0, (1-\bar{\alpha}_t)I)$, где $\alpha_t = 1-\beta_t$ и $\bar{\alpha}_t = \prod_{s=1}^t \alpha_s$.
    2.  **Обратный процесс (денормализация, обучаемый):** Нейронная сеть $p_\theta(x_{t-1}|x_t)$ обучается обращать этот процесс – предсказывать (восстанавливать) менее зашумленное изображение $x_{t-1}$ из более зашумленного $x_t$.
        Цель – научиться предсказывать шум $\epsilon_\theta(x_t, t)$, добавленный на шаге $t$, чтобы получить $x_0$.
*   **Обучение:** Модель (обычно U-Net архитектура) обучается предсказывать шум $\epsilon$, добавленный к $x_0$ для получения $x_t$, на каждом шаге $t$. Функция потерь:
    $$ L = \mathbb{E}_{t, x_0, \epsilon} [||\epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t}x_0 + \sqrt{1-\bar{\alpha}_t}\epsilon, t)||^2] $$
*   **Генерация (сэмплирование):**
    1.  Начинаем с чистого шума $x_T \sim N(0,I)$.
    2.  Итеративно (от $t=T$ до $1$) сэмплируем $x_{t-1}$ из $p_\theta(x_{t-1}|x_t)$, используя предсказанный моделью шум $\epsilon_\theta(x_t, t)$ для "очистки" $x_t$.
*   Можно добавить **условие $y$** (например, текстовое описание или метку класса) в модель $\epsilon_\theta(x_t, t, y)$ для условной генерации.

**6.2. Трилемма генеративного обучения (слайд 58)**
Разные классы генеративных моделей имеют разные компромиссы между тремя желаемыми свойствами:
*   **Высокое качество сэмплов (High Quality Samples):** Насколько реалистичны и детализированы генерируемые объекты.
    *   Хорошо: GAN, Диффузные модели.
    *   Хуже: VAE, некоторые авторегрессионные.
*   **Быстрое сэмплирование (Fast Sampling):** Насколько быстро можно генерировать новые объекты.
    *   Хорошо: GAN (один проход через генератор), VAE.
    *   Медленно: Авторегрессионные модели (последовательная генерация), Диффузные модели (требуют много итеративных шагов денормализации).
*   **Покрытие мод / Разнообразие (Mode Coverage / Diversity):** Насколько хорошо модель способна генерировать все типы объектов из обучающего распределения, а не только самые частые или простые.
    *   Хорошо: VAE, Нормализующие потоки, Авторегрессионные модели (благодаря явной оценке правдоподобия).
    *   Проблема: GAN (mode collapse). Диффузные модели обычно лучше GAN в этом аспекте.

*   **GANs:** Хорошее качество, быстрое сэмплирование, но проблемы с покрытием мод и стабильностью обучения.
*   **Denoising Diffusion Models:** Очень высокое качество, хорошее покрытие мод, но медленное сэмплирование.
*   **Variational Autoencoders, Normalizing Flows:** Хорошее покрытие мод, быстрое сэмплирование (для VAE), но качество может быть ниже (размытость для VAE).

---

# Лекция 14: Активное обучение и разметка данных

**План лекции:**
1.  Обучение на частично размеченных данных (Semi-Supervised Learning)
2.  Активное обучение (Active Learning)
3.  Гауссовский процесс (Gaussian Process)
4.  Другие методы оценки дисперсии (неопределенности)
5.  Разметка данных

---

## 1. Обучение на частично размеченных данных (Semi-Supervised Learning, SSL)

**1.1. Постановка задачи**
*   Тренировочный набор данных состоит из двух частей:
    1.  **$D^l$ (Labeled Data):** Размеченная часть. Объекты содержат как признаки $X$, так и целевую метку $Y$.
    2.  **$D^u$ (Unlabeled Data):** Неразмеченная часть. Объекты содержат только признаки $X$.
*   **Ключевая особенность:** $|D^l| \ll |D^u|$ (размеченных объектов значительно меньше, чем неразмеченных).
*   **Наивное решение:** Выкинуть $D^u$ и обучать модель только на $D^l$. Это один из вариантов работы с пропусками в метках, но он не использует потенциально полезную информацию из $D^u$.
*   **Цель SSL:** Использовать информацию из $D^u$ для улучшения качества модели по сравнению с обучением только на $D^l$.

**1.2. Решения при помощи подходов обучения с учителем**

*   **Самообучение (Self-Training / Pseudo-Labeling):**
    1.  Обучить модель на размеченных данных $D^l$.
    2.  Использовать эту модель для предсказания меток на неразмеченных данных $D^u$.
    3.  Выбрать объекты из $D^u$, для которых модель наиболее уверена в своих предсказаниях (например, предсказания с высокой вероятностью).
    4.  Добавить эти объекты с их "псевдо-метками" в $D^l$.
    5.  Переобучить модель на расширенном $D^l$. Повторять шаги 2-5.
    *   Класс добавляемого объекта определяется предсказанием самого алгоритма.
    *   Риск: если модель ошибается с высокой уверенностью, ошибки могут накапливаться.

*   **Сообучение (Co-Training):**
    *   Используется, если признаки можно разделить на два (или более) **независимых** набора представлений (views), каждый из которых достаточен для классификации.
    1.  Обучаются два (или более) **разных** классификатора на $D^l$, каждый на своем наборе представлений.
    2.  Каждый классификатор делает предсказания на $D^u$.
    3.  Наиболее уверенные предсказания одного классификатора (на его "view") используются для добавления примеров в обучающее множество другого классификатора (на его "view").
    4.  Процесс повторяется.
    *   Идея: разные классификаторы могут "учить" друг друга, используя разные аспекты данных.

*   **Оптимизационный подход (регуляризация на основе неразмеченных данных):**
    Функция потерь модифицируется так, чтобы учитывать неразмеченные данные.
    $$ L(\theta) = L_{class}(a_\theta, D^l) + \lambda \cdot L_{clust}(a_\theta, D^u) $$
    *   $L_{class}$: стандартная функция потерь для обучения с учителем на $D^l$.
    *   $L_{clust}$: член, который поощряет выполнение некоторых предположений о структуре данных на $D^u$ (например, кластерное предположение – точки в одном кластере должны иметь одинаковые метки, или предположение о многообразии – разделяющая поверхность не должна проходить через области высокой плотности данных).
    *   $\lambda$: коэффициент, балансирующий вклад двух членов.

**1.3. Semi-Supervised Support Vector Machine (S3VM / Transductive SVM)**
*   Модификация SVM для SSL.
*   Цель: найти разделяющую гиперплоскость, которая не только хорошо разделяет размеченные данные $D^l$, но и проходит через области низкой плотности неразмеченных данных $D^u$ (максимизирует зазор и для неразмеченных данных).
*   Функция потерь (один из вариантов):
    $$ \sum_{i=1}^l (1 - M_i(w, w_0))_+ + \frac{1}{2C} ||w||^2 + \sum_{j=1}^u (1 - |M_j(w, w_0)|)_+ \rightarrow \min_{w,w_0} $$
    *   Первый член – стандартный hinge loss для $D^l$. $M_i(w, w_0) = y_i(\langle w, x_i \rangle + w_0)$.
    *   Второй член – регуляризация.
    *   Третий член – штраф для неразмеченных данных, заставляющий их быть как можно дальше от разделяющей гиперплоскости (т.е. $|M_j(w, w_0)| \ge 1$).
    *   $(x)_+ = \max(0,x)$.
*   Задача оптимизации для S3VM обычно невыпуклая и сложная.

**1.4. Решения при помощи обучения без учителя**

*   **Решение при помощи извлечения признаков (Feature Extraction):**
    1.  Обучить на **всех данных** ($D^l \cup D^u$) алгоритм извлечения признаков без учителя (например, автокодировщик, PCA, t-SNE для визуализации). Получаем новые представления $\hat{X}$ из исходных $X$.
    2.  Обучить на **размеченной части** $D^l$ (с новыми признаками $\hat{X}^l$) стандартный алгоритм обучения с учителем для отображения $\hat{X}^l \rightarrow Y$.
    *   Это более "умный" способ работы с пропущенными метками, так как структура неразмеченных данных используется для построения лучшего пространства признаков.

*   **Решение при помощи кластеризации:**
    1.  Обучить на **всех данных** ($D^l \cup D^u$) алгоритм кластеризации для получения меток кластеров $\hat{Y}$ (например, $X \rightarrow \hat{Y}$).
    2.  Обучить на **размеченной части** $D^l$ алгоритм обучения с учителем для отображения меток кластеров $\hat{Y}^l$ (или исходных признаков $X^l$ вместе с $\hat{Y}^l$) в истинные метки $Y$.
    *   Предположение: кластеры в пространстве признаков соответствуют классам.

**1.5. Пример для MNIST (слайд 8)**
*   Визуализация (например, t-SNE) данных MNIST без использования информации о классах показывает, что цифры образуют достаточно хорошо различимые кластеры.
*   Для определения, какому классу соответствует каждый кластер, достаточно нескольких размеченных объектов из этого кластера.

**1.6. Модификация методов кластеризации для SSL**
*   **EM-подобные алгоритмы (например, Gaussian Mixture Models):**
    *   На E-шаге (оценка скрытых переменных – принадлежности к компонентам смеси) для объектов из $D^l$ с известным классом, их принадлежность к компонентам (соответствующим их классу) фиксируется и не изменяется.
*   **Агломеративная кластеризация:**
    *   Можно ввести ограничение: не объединять кластеры, если они содержат размеченные объекты из разных классов.

---

## 2. Активное обучение (Active Learning)

**2.1. Сценарий**
*   Есть доступ к большому объему неразмеченных данных.
*   Разметка данных – дорогостоящий и/или медленный процесс (требует участия человека-эксперта, "Оракула").
*   Скорость обучения моделей может быть выше скорости разметки.

**2.2. Определение**
*   **Активное обучение:** Подход, в котором алгоритм сам выбирает, какие из неразмеченных объектов наиболее информативны для разметки, чтобы максимально улучшить качество модели при минимальных затратах на разметку.
*   Условия схожи с SSL, но здесь алгоритм может **задавать Оракулу вопросы** о метках выбранных объектов.
*   **Задача:** Найти оптимальную стратегию запросов к Оракулу, которая максимизирует качество аппроксимации $X \rightarrow Y$ при наименьшем числе обращений (размеченных примеров).
*   **Наивное решение:** Отправлять на разметку случайные объекты (это соответствует пассивному обучению / supervised learning с ограниченным бюджетом на разметку).

**2.3. Оценка алгоритмов активного обучения**
*   **Тестовое множество должно состоять из случайно выбранных объектов**, чтобы не было смещения в оценке.
*   **Метрики для сравнения:**
    *   Качество модели (например, точность) при **фиксированном бюджете** на разметку (фиксированном числе обращений к Оракулу).
    *   Число обращений (стоимость разметки), необходимое для достижения **заданного уровня качества**.
    *   **Площадь под кривой обучения (Area Under the Learning Curve, AULC):** Кривая показывает зависимость качества модели от числа размеченных примеров. Большая площадь означает более эффективное обучение.
*   Использование тренировочного множества (размеченного в процессе активного обучения) для оценки неинформативно, так как оно будет смещенным и разным для разных стратегий.

**2.4. Стратегии выбора объектов для разметки (Query Strategies)**

*   **Выбор по степени неуверенности (Uncertainty Sampling):**
    Идея: запрашивать метки для тех объектов, в предсказаниях которых модель наименее уверена.
    Пусть $P_\theta(y|x)$ – вероятность класса $y$ для объекта $x$ по текущей модели с параметрами $\theta$. Пусть $\hat{y} = \arg\max_y P_\theta(y|x)$ – наиболее вероятный класс.
    *   **Минимальная уверенность (Least Confident / Min Confidence):**
        Выбирается объект $x^*_{LC} = \arg\max_x (1 - P_\theta(\hat{y}|x))$.
        (Эквивалентно $\arg\min_x P_\theta(\hat{y}|x)$).
    *   **Отбор по минимальному отступу (Margin Sampling):**
        Выбирается объект, для которого разница между вероятностями двух наиболее вероятных классов $(\hat{y}_1, \hat{y}_2)$ минимальна.
        $$ x^*_{MS} = \arg\min_x (P_\theta(\hat{y}_1|x) - P_\theta(\hat{y}_2|x)) $$
        (Или $\arg\max_x$ для $-(P_\theta(\hat{y}_1|x) - P_\theta(\hat{y}_2|x))$ как на слайде).
    *   **Максимальная энтропия (Maximum Entropy):**
        Выбирается объект, для которого энтропия распределения вероятностей по классам максимальна (наибольшая неопределенность).
        $$ x^*_{ME} = \arg\max_x \left( - \sum_i P_\theta(y_i|x) \log P_\theta(y_i|x) \right) $$

*   **Пример для одномерного SVM (слайд 16):**
    На каждом шаге на разметку отправляется неразмеченный объект, наиболее близкий к текущей разделяющей гиперплоскости (т.е. где модель наименее уверена). Это похоже на бинарный поиск оптимальной границы.

*   **Отбор по несогласию в комитете (Query-by-Committee, QBC):**
    *   Используется комитет (ансамбль) из $C$ моделей $\{ \theta_{(1)}, \dots, \theta_{(|C|)} \}$, обученных на текущих размеченных данных (например, с помощью bagging или разных инициализаций).
    *   **Идея:** Запрашивать метки для тех объектов, по которым предсказания моделей в комитете наиболее сильно расходятся (наименьшее согласие).
    *   **Меры несогласия:**
        *   **Энтропия голосования (Vote Entropy):**
            $$ x^*_{VE} = \arg\max_{x \in H} \left( - \sum_i \frac{V(y_i)}{|C|} \log \frac{V(y_i)}{|C|} \right) $$
            где $V(y_i)$ – число голосов (моделей в комитете), предсказавших метку $y_i$ для объекта $x$.
        *   **KL-дивергенция:** Средняя KL-дивергенция между распределением вероятностей, предсказанным каждой моделью, и средним распределением вероятностей по комитету.

*   **Ожидаемое изменение модели (Expected Model Change):**
    *   **Идея:** Запрашивать метки для тех объектов, которые, как ожидается, приведут к наибольшему изменению параметров модели (или ее ошибки).
    *   Изменение модели можно оценивать, например, как ожидаемую норму градиента функции потерь, если бы этот объект был добавлен в обучающий набор:
        $$ x^*_{EGL} = \arg\max_x \sum_i P_\theta(y_i|x) ||\nabla \mathcal{L}(x, y_i; \theta)|| $$
        (Суммирование по всем возможным меткам $y_i$ для объекта $x$, взвешенное по их текущей вероятности).

*   **Ожидаемое сокращение ошибки (Expected Error Reduction):**
    *   **Идея:** Запрашивать метки для тех объектов, которые, как ожидается, приведут к наибольшему уменьшению ошибки на неразмеченных данных.
    *   **Минимизация ожидания 0/1 ошибки (точности):**
        $$ x^*_{0/1} = \arg\max_x \sum_i P_\theta(y_i|x) \sum_{x' \in D^u} (1 - P_{\theta+(x,y_i)}(\hat{y}|x')) $$
        где $P_{\theta+(x,y_i)}$ – модель, переобученная с добавлением $(x, y_i)$. Это очень ресурсоемко.
    *   **Минимизация ожидания перекрестной энтропии:**
        $$ x^*_{CE} = \arg\max_x \sum_i P_\theta(y_i|x) \left( - \sum_j \sum_{x' \in D^u} P_{\theta+(x,y_i)}(y_j|x') \log P_{\theta+(x,y_i)}(y_j|x') \right) $$

*   **Сэмплирование с учётом плотности (Density-Weighted Sampling):**
    *   **Проблема uncertainty sampling:** Может выбирать аномалии (outliers), которые малоинформативны для основной массы данных.
    *   **Идея:** Модифицировать меру неуверенности, домножая ее на оценку плотности распределения данных $p(x)$. Это позволяет выбирать объекты, которые не только неопределенны, но и репрезентативны (находятся в плотных областях).
    *   **Пакетная разметка (Batch Active Learning):**
        *   Отправлять на разметку по одному объекту может быть неэффективно (если Оракул может размечать пакетами).
        *   Если просто выбрать $k$ самых неуверенных объектов, они могут быть очень похожи друг на друга (находиться в одной области неопределенности).
        *   Учет плотности или меры разнообразия (diversity) помогает выбирать информативные и разнообразные объекты для пакетной разметки.

---

## 3. Гауссовский процесс (Gaussian Process, GP)

**3.1. Определение**
*   **Гауссовский процесс** – это стохастический процесс (коллекция случайных величин, индексированных временем или пространством), такой что любой конечный набор этих случайных величин имеет совместное многомерное нормальное (Гауссовское) распределение.
*   Любая конечная линейная комбинация этих случайных величин также нормально распределена.
*   GP является обобщением многомерного Гауссовского распределения на бесконечномерное пространство функций.
*   Полностью определяется **функцией среднего $m(x)$** и **ковариационной функцией (ядром) $k(x, x')$**:
    $$ f(x) \sim GP(m(x), k(x,x')) $$
    $m(x) = \mathbb{E}[f(x)]$
    $k(x,x') = \mathbb{E}[(f(x)-m(x))(f(x')-m(x'))]$
*   Традиционно используется в **байесовской оптимизации** в качестве суррогатной модели для аппроксимации дорогой целевой функции.

**3.2. Получение предсказания с помощью GP (для регрессии)**
Пусть у нас есть обучающий набор $(X, \mathbf{y}) = \{(x_i, y_i)\}_{i=1}^N$. Мы хотим предсказать значение $f(x^*)$ в новой точке $x^*$.
*   Совместное распределение $y_1, \dots, y_N, f(x^*)$ является Гауссовским.
*   Предсказание (среднее апостериорного распределения $p(f(x^*)|X, \mathbf{y}, x^*)$):
    $$ \mu(x^*) = m(x^*) + \mathbf{k}(x^*)^T (K + \sigma_n^2 I)^{-1} (\mathbf{y} - \mathbf{m}(X)) $$
    *   $K$: ковариационная матрица для обучающих точек, $K_{ij} = k(x_i, x_j)$.
    *   $\mathbf{k}(x^*)$: вектор ковариаций между $x^*$ и обучающими точками, $k_i(x^*) = k(x_i, x^*)$.
    *   $\sigma_n^2$: дисперсия шума в наблюдениях $y_i$ (если предполагается).
    *   $\mathbf{m}(X)$: вектор априорных средних для обучающих точек. Часто $m(x)=0$.
*   Дисперсия предсказания (неопределенность):
    $$ \sigma^2(x^*) = k(x^*, x^*) - \mathbf{k}(x^*)^T (K + \sigma_n^2 I)^{-1} \mathbf{k}(x^*) $$
*   Матрица $(K + \sigma_n^2 I)^{-1}$ не зависит от запроса $x^*$ и может быть предподсчитана.

**3.3. Ковариационные функции (Ядра)**
Определяют свойства функций, которые могут быть смоделированы GP (гладкость, периодичность и т.д.).
*   Похожи на ядра в методе опорных векторов (SVM).
*   **Константа:** $k(a,b) = C$.
*   **Линейная:** $k(a,b) = \langle a, b \rangle$. (Если данные центрированы, это обычная ковариация).
*   **Квадратичная экспоненциальная (Squared Exponential / RBF Kernel):**
    $$ k(a,b) = \sigma_f^2 \exp \left( -\frac{1}{2l^2} ||a-b||^2 \right) $$
    Или в более общем виде с матрицей масштабирования $\Theta$:
    $$ k(a,b) = \exp \left( -\frac{1}{2} (a-b)^T \Theta^{-2} (a-b) \right) $$
    *   $\sigma_f^2$: амплитуда.
    *   $l$ или $\Theta$: характерные масштабы длины.
*   Множество других: Матерн, периодические, рационально-квадратичные и т.д. Ядра можно комбинировать (сумма, произведение).

**3.4. Сэмплирование Томпсона (Thompson Sampling) с GP**
*   Гауссовский процесс определяет распределение над функциями.
*   На каждом шаге байесовской оптимизации или активного обучения можно **сэмплировать случайную функцию** $f_s$ из текущего апостериорного распределения GP.
*   Затем выбрать следующую точку для запроса, максимизируя эту сэмплированную функцию $f_s(x)$.
*   Это один из способов балансировки исследования и использования.

---

## 4. Другие методы оценки дисперсии (неопределенности)

Неопределенность предсказаний модели важна для многих стратегий активного обучения (например, UCB в байесовской оптимизации, uncertainty sampling).

**4.1. Оценка дисперсии ансамблем моделей**
*   Рассмотрим ансамбль из $T$ моделей: $a_1, a_2, \dots, a_T$.
*   Пусть предсказание ансамбля $y(x)$ вычисляется как среднее предсказаний отдельных моделей:
    $$ y(x) = \mathbb{E}_i [a_i(x)] = \frac{1}{T} \sum_{i=1}^T a_i(x) $$
*   Тогда дисперсию $\sigma^2(x)$ предсказания ансамбля можно оценить как выборочную дисперсию ответов моделей:
    $$ \sigma^2(x) = \mathbb{D}_i [a_i(x)] = \frac{1}{T-1} \sum_{i=1}^T (a_i(x) - y(x))^2 $$
    Чем больше разброс предсказаний моделей ансамбля, тем выше неопределенность.

**4.2. Случайный лес (Random Forest)**
*   Ансамбль деревьев принятия решений.
*   **Деревья принятия решений:**
    *   Умеют работать с объектами со смешанными типами признаков (числовые, категориальные).
    *   Быстро обучаются и делают предсказания.
    *   Не сильно переобучаются при большом числе признаков (особенно в составе леса).
    *   **Плохо экстраполируют** за пределы диапазона значений признаков, виденных на обучении.
*   Случайный лес может использоваться как суррогатная модель в байесовской оптимизации или для оценки неопределенности. Предсказание – среднее по деревьям, неопределенность – дисперсия по деревьям.
*   **Сравнение с GP (слайд 29):**
    *   Случайный лес хуже экстраполирует функцию по сравнению с GP.
    *   Оценка неопределенности случайным лесом также может быть хуже, особенно вне областей с данными.
    *   В середине (где мало точек) предсказания также могут быть хуже.

**4.3. Случайное отключение в нейронных сетях (Dropout для оценки неопределенности)**
*   **На стадии обучения:** Dropout случайным образом отключает нейроны (или их выходы).
*   Это можно представить как обучение ансамбля множества "прореженных" сетей с общими параметрами.
*   **На стадии предсказания (стандартно):** Dropout отключается, и используется вся сеть (с масштабированием весов или активаций).
*   **Для оценки дисперсии (Monte Carlo Dropout):**
    *   На стадии предсказания Dropout **не отключается**.
    *   Делается несколько (например, $T$) прогонов через сеть с **разными масками Dropout** для одного и того же входа $x$.
    *   Получаем $T$ разных предсказаний $\{y_1(x), \dots, y_T(x)\}$.
    *   Среднее этих предсказаний используется как итоговое предсказание.
    *   Выборочная дисперсия этих предсказаний используется как оценка неопределенности модели (эпистемической неопределенности).

---

## 5. Разметка данных

**5.1. Ограничения процесса разметки**
*   **Модальность данных:** Людям (разметчикам) трудно размечать абстрактные векторы чисел. Разметка обычно выполняется для понятных модальностей: картинки, звук, видео, текст.
*   **Квалификация и оборудование:** Для некоторых задач разметки требуется специальная квалификация (например, медицинская диагностика) и/или оборудование.
*   **Предварительная разметка:** Часть данных (золотой стандарт) должна быть предварительно размечена экспертами для обучения и контроля качества других разметчиков.
*   **Точность и стабильность:** Нет гарантии, что люди будут размечать абсолютно точно и стабильно (согласие между разметчиками - inter-annotator agreement).
*   **"Спорные" объекты:** При активном обучении на разметку часто отправляются "спорные" или пограничные объекты, которые сложны для разметки даже для человека.

**5.2. Инструменты для разметки данных**
Существует множество инструментов для различных типов данных и задач.
*   **`ipyannotate` (слайд 33):** Библиотека Python для создания простых виджетов разметки в Jupyter Notebook. Подходит для небольших задач и экспериментов.
*   **CVAT (Computer Vision Annotation Tool, слайд 34):** Мощный веб-инструмент с открытым исходным кодом для разметки изображений и видео (классификация, детектирование, сегментация, трекинг).
*   **YEDDA (слайд 35):** Легковесный инструмент для аннотирования текстовых фрагментов (span annotation) для задач типа извлечения именованных сущностей (NER).
*   **Stanford CoreNLP (слайд 36):** Набор инструментов для обработки естественного языка, включающий возможности для аннотирования (части речи, именованные сущности, синтаксические зависимости).
*   **EchoML (слайд 37):** Инструмент для разметки аудиоданных (например, для распознавания звуковых событий).
*   Множество коммерческих платформ (Toloka, Amazon Mechanical Turk, Labelbox, Scale AI и др.), которые предоставляют интерфейсы для разметки и доступ к пулу разметчиков.

**5.3. Слабая разметка (Weak Supervision)**
Использование менее точных или косвенных источников информации для получения меток, когда точная разметка дорога или невозможна.
*   **Слабая разметка** – это разметка для более простой, связанной задачи, которая может помочь в решении основной, более сложной задачи.
*   **Примеры:**
    *   Для задачи классификации: использовать вероятностные метки (soft labels) вместо жестких one-hot.
    *   Для задачи мультиклассовой (multi-label) классификации: использовать метки на уровне всего изображения (знать, что на картинке есть "кошка" и "собака", но не их точное положение) для обучения детектора объектов.
    *   Для задачи детекции объектов: использовать метки на уровне всего изображения для задачи сегментации (если известно, что объект есть, то хотя бы часть пикселей должна быть отнесена к нему).

**5.4. Уточнение разметки (Iterative Annotation / Refinement)**
Процесс улучшения качества разметки.
*   Сегментацию можно заменить на более простую детекцию (разметка ограничивающих рамок вместо попиксельных масок).
*   При выделении объекта можно просить разметчиков уточнять края (например, с помощью интерактивной сегментации).
*   Вместо построения обводки объекта с нуля, можно предложить разметчику скорректировать существующую (возможно, предсказанную моделью) обводку.

**5.5. Контроль качества разметки**
*   **Золотой стандарт (Gold Standard / Honeypots):** В задания для разметчиков подмешиваются объекты с уже известной (эталонной) разметкой. Это позволяет оценить качество работы каждого разметчика.
*   **Перекрытие (Overlap / Redundancy):** Один и тот же объект размечается несколькими (например, 3-5) разными людьми.
    *   Позволяет вычислить **межэкспертное согласие (inter-annotator agreement, IAA)**, например, с помощью каппы Коэна или альфы Криппендорфа.
    *   Помогает выявить спорные или неоднозначные объекты.
    *   Итоговая метка может быть выбрана голосованием большинства или более сложными методами агрегации.
*   **Задача проверки качества как задача разметки:** Проверка уже выполненной разметки может быть представлена как отдельная задача для других (возможно, более квалифицированных) разметчиков.

---

# Лекция 15: Обучение с подкреплением (Reinforcement Learning, RL)

**План лекции:**
1.  Введение в Обучение с подкреплением
2.  Задача о многоруком бандите
3.  Байесовская оптимизация (как связанная задача)
4.  Основные понятия Обучения с подкреплением (состояния, действия, награды, функции оценки)
5.  Задачи обучения с подкреплением (примеры)

---

## 1. Введение в Обучение с подкреплением (Reinforcement Learning, RL)

**1.1. Основная идея**
*   **Агент (Agent):** Обучаемая сущность, которая взаимодействует со **средой (Environment)**.
*   **Взаимодействие:**
    1.  Агент наблюдает текущее **состояние (State)** среды.
    2.  На основе состояния агент выбирает и совершает **действие (Action)**.
    3.  Среда реагирует на действие агента:
        *   Переходит в **новое состояние**.
        *   Выдает агенту **награду (Reward)** или штраф.
*   **Задача агента:** Научиться такой **стратегии (Policy)** выбора действий, которая максимизирует некоторую **суммарную (кумулятивную) награду** на протяжении времени.
*   RL больше похоже на то, как происходит обучение в реальном мире (методом проб и ошибок, с отложенной обратной связью).

**1.2. Дилемма исследования против использования (Exploration vs. Exploitation Tradeoff)**
Ключевая проблема в RL.
*   **Использование (Exploitation):** Агент выбирает действия, которые, по его текущим знаниям, принесут максимальную немедленную или ожидаемую будущую награду. Он использует уже известные хорошие стратегии.
*   **Исследование (Exploration):** Агент выбирает действия, о которых у него мало информации, или которые он ранее не пробовал. Это делается для того, чтобы собрать больше данных о среде и, возможно, найти еще лучшие стратегии, чем известные на данный момент.
*   **Баланс:**
    *   Слишком много использования: агент может застрять в локальном оптимуме, не найдя глобально лучшую стратегию.
    *   Слишком много исследования: агент может тратить много времени на неоптимальные действия, не получая высокой награды.
*   **Наивное решение:** Явное разделение процесса на фазу исследования (например, случайные действия для сбора данных) и фазу использования (применение лучшей найденной стратегии). Обычно менее эффективно, чем стратегии, совмещающие исследование и использование.

---

## 2. Задача о многоруком бандите (Multi-Armed Bandit, MAB)

**2.1. Постановка задачи**
*   **Частный случай RL:** Агент взаимодействует со средой, которая имеет только **одно состояние**.
*   Награда зависит **только от выбранного действия**.
*   **Действие (Action):** Выбор одной из $K$ "ручек" (рычагов) игрового автомата ("многорукого бандита").
*   **Награда (Reward):** Случайная величина, получаемая от выбранной ручки. Каждая ручка $i$ имеет свое (неизвестное агенту) распределение наград с некоторым средним значением $\mu_i$.
*   **Цель агента:** Максимизировать суммарный выигрыш за $T$ попыток (дерганий ручек).
*   **Дилемма Exploration vs. Exploitation в MAB:**
    *   **Exploitation:** Дергать ручку, которая на данный момент кажется наилучшей (имеет наибольшую оценку среднего выигрыша).
    *   **Exploration:** Дергать другие ручки, чтобы точнее оценить их средний выигрыш и, возможно, найти ручку лучше текущей оптимальной.

**2.2. Стратегии решения задачи MAB**

*   **Наивное решение (A/B-тестирование как MAB):**
    1.  **Фаза исследования:** Дёрнуть за каждую ручку определённое (одинаковое) число раз.
    2.  Вычислить для каждой ручки оценку среднего выигрыша.
    3.  **Фаза использования:** Оставшийся бюджет (попытки) потратить на ручку с наибольшим оценённым средним.
    *   График (слайд 8) показывает, что при A/B тестировании ресурсы тратятся на исследование всех опций (A, B, C) даже после того, как одна из них (Option A) показывает себя лучше. "Bandit Selection" быстрее переключается на лучшую опцию.

*   **Жадный алгоритм (Greedy Algorithm):**
    1.  Дёрнуть за каждую ручку по одному разу (для начальной оценки).
    2.  На каждом последующем шаге выбирать ручку с **максимальной текущей оценкой** среднего выигрыша.
    3.  После получения награды обновить оценку среднего выигрыша для выбранной ручки.
    *   **Недостаток:** Может "застрять" на субоптимальной ручке, если она случайно дала хороший выигрыш вначале, а истинно оптимальная ручка случайно дала плохой.

*   **$\epsilon$-жадный алгоритм ($\epsilon$-Greedy):**
    *   На каждом шаге:
        *   С вероятностью $1-\epsilon$: выбирается **жадное** действие (ручка с наилучшей текущей оценкой) – Exploitation.
        *   С вероятностью $\epsilon$: выбирается **случайная** ручка (из всех $K$ ручек равновероятно) – Exploration.
    *   Параметр $\epsilon$ (малое число, например, 0.1) контролирует баланс. Можно использовать убывающий $\epsilon(t)$.
    *   После получения награды оценка среднего обновляется.
    *   **Недостаток:** При исследовании выбирает случайную ручку, не учитывая, насколько плохи или хороши могут быть другие неоптимальные ручки.

*   **Стратегия SoftArgMax (Softmax Exploration / Boltzmann Exploration):**
    *   На каждом шаге ручка $i$ выбирается с вероятностью $p_i$, которая зависит от текущей оценки ее среднего выигрыша $\hat{\mu}_i$:
        $$ p_i = \frac{\exp(\hat{\mu}_i / \tau)}{\sum_{j=1}^K \exp(\hat{\mu}_j / \tau)} $$
        где $\tau > 0$ – **температурный параметр**.
    *   **Низкая температура ($\tau \rightarrow 0$):** Выбор приближается к жадному (argmax) – больше использования.
    *   **Высокая температура ($\tau \rightarrow \infty$):** Выбор приближается к равномерному случайному – больше исследования.
    *   Температуру $\tau$ можно изменять со временем (например, уменьшать).

*   **Верхний доверительный интервал (Upper Confidence Bound, UCB1):**
    *   Для каждой ручки $i$ вычисляется метрика, которая является суммой текущей оценки среднего выигрыша $\bar{q}_i$ и "бонуса за неопределенность":
        $$ \text{UCB}_i = \bar{q}_i + \lambda \cdot \sqrt{\frac{2 \ln(n)}{n_i}} $$
        *   $\bar{q}_i$: текущая оценка среднего выигрыша $i$-й ручки.
        *   $n$: общее число совершенных попыток (дерганий всех ручек).
        *   $n_i$: число раз, когда была выбрана $i$-я ручка.
        *   $\lambda$: константа, контролирующая вес неопределенности (исследования). Часто $\lambda = \sqrt{2}$ или настраивается.
    *   На каждом шаге выбирается ручка $i$ с **максимальной метрикой $\text{UCB}_i$**.
    *   Идея: ручки, которые дергались мало ($n_i$ мало) или показывают высокую вариативность, получат больший бонус за неопределенность и будут исследоваться чаще.
    *   Параметр $\lambda$ в формуле на слайде может быть также нужен для согласования размерностей, если $\bar{q}_i$ и член неопределенности имеют разные масштабы.

**2.3. Сравнение алгоритмов: Сожаление (Regret)**
*   **Сожаление (Regret):** Разница между суммарной наградой, полученной оптимальной стратегией (всегда дергать лучшую ручку $\mu^*$), и суммарной наградой, полученной данным алгоритмом за $T$ шагов.
    $$ R_T = T \mu^* - \sum_{t=1}^T r_t $$
    где $r_t$ – награда, полученная алгоритмом на шаге $t$.
*   Цель – минимизировать сожаление. Хорошие алгоритмы имеют сожаление, растущее медленнее, чем линейно (например, $O(\log T)$ или $O(\sqrt{T})$).
*   Графики (слайд 13) показывают, как суммарная награда алгоритма отстает от максимальной возможной. "Stuck at suboptimal arm" иллюстрирует проблему жадного алгоритма.

**2.4. Контекстные многорукие бандиты (Contextual Bandits)**
*   Обобщение MAB, где перед выбором действия агент наблюдает некоторое **состояние (контекст, $x$ )**.
*   Функция выигрыша для каждой ручки $a$ теперь зависит от контекста: $\mu_a(x)$.
*   Вместо оценки простого среднего выигрыша $\mu_a$, требуется строить **модель $f_a(x) \approx \mu_a(x)$**, аппроксимирующую зависимость выигрыша от контекста для каждой ручки $a$.
*   Для каждой "ручки" (действия) строится своя модель предсказания награды.
*   Обычная задача MAB – частный случай, когда контекст всегда один и тот же (или его нет), и модель $f_a(x)$ – это просто константа $\mu_a$.
*   **LinUCB (Linear UCB):** Если предполагается, что награда линейно зависит от контекста $\mu_a(x) = \langle x, \theta_a \rangle$, то UCB-подобная стратегия может быть использована:
    $$ \text{Action} = \arg\max_a \left( \langle x, \hat{\theta}_a \rangle + \alpha \sqrt{x^T (F_a^T F_a)^{-1} x} \right) $$
    (Формула на слайде $x^T (F_a^T F_a)^{-1} x$ представляет собой дисперсию предсказания линейной модели, где $F_a$ - матрица признаков для обучения $\theta_a$).

---

## 3. Байесовская оптимизация (Bayesian Optimization)

**3.1. Мотивация**
*   Задача: найти максимум (или минимум) "дорогой" целевой функции $f(x)$, т.е. функции, каждое вычисление которой занимает много ресурсов (времени, денег). Пример: подбор гиперпараметров нейронной сети, где $f(x)$ – это качество модели с гиперпараметрами $x$ после обучения.
*   **Идея:** Уменьшить число вычислений $f(x)$ путем построения **суррогатной модели (surrogate model)**, которая аппроксимирует $f(x)$ и является "дешевой" для вычисления.
*   Алгоритм оптимизации будет работать с суррогатной моделью для выбора следующей точки $x$ для вычисления $f(x)$.
*   Дорогие вызовы $f(x)$ используются для обучения/обновления суррогатной функции.

**3.2. Функции улучшения (Acquisition Functions)**
Алгоритм байесовской оптимизации на каждом шаге выбирает следующую точку $x$ для вычисления $f(x)$, максимизируя **функцию улучшения**. Эта функция балансирует между исследованием (области с высокой неопределенностью суррогатной модели) и использованием (области, где суррогатная модель предсказывает высокое значение).
Суррогатная модель (часто Гауссовский процесс) предоставляет предсказание среднего $\mu(x)$ и неопределенности (дисперсии $\sigma^2(x)$ или стандартного отклонения $\sigma(x)$) для $f(x)$ в любой точке $x$.

*   **Верхняя доверительная граница (Upper Confidence Bound, UCB):**
    (Уже обсуждалась в контексте GP, см. Лекцию 14)
    $$ \text{UCB}(x) = \mu(x) + \lambda \cdot \sigma(x) $$
    *   $\lambda$ – вес, контролирующий важность неопределенности.
    *   Теоретически $\lambda$ можно уменьшать для перехода от исследования к использованию. На практике часто используют $\lambda(t)$, растущую с номером шага $t$, например, $\lambda(t) = O(\sqrt{\log t})$.
    *   Пример графика (слайд 18) показывает, как UCB (синяя линия) выше в областях с высокой неопределенностью (широкий зеленый доверительный интервал) или высоким предсказанным средним (красная линия).

*   **Вероятность улучшения (Probability of Improvement, PI):**
    Пусть $f^*$ – текущее наилучшее наблюденное значение $f(x)$.
    Улучшение $I(x) = \max(f(x) - f^*, 0)$.
    Предполагая, что $f(x) \sim N(\mu(x), \sigma^2(x))$ (из GP), тогда $f(x) = \mu(x) + \sigma(x) \cdot z$, где $z \sim N(0,1)$.
    $$ PI(x) = P(I(x) > 0) = P(f(x) > f^*) $$
    Можно выразить через функцию стандартного нормального распределения $\Phi$:
    $$ PI(x) = P\left(z > \frac{f^* - \mu(x)}{\sigma(x)}\right) = 1 - \Phi\left(\frac{f^* - \mu(x)}{\sigma(x)}\right) = \Phi\left(\frac{\mu(x) - f^*}{\sigma(x)}\right) $$

*   **Ожидаемое улучшение (Expected Improvement, EI):**
    Математическое ожидание улучшения $I(x)$:
    $$ EI(x) = \mathbb{E}[I(x)] = \int_{-\infty}^{\infty} \max(f(x) - f^*, 0) p(f(x)|x) df(x) $$
    После вычислений (слайд 20), если $u = \frac{\mu(x) - f^*}{\sigma(x)}$:
    $$ EI(x) = (\mu(x) - f^*) \Phi(u) + \sigma(x) \phi(u) $$
    где $\phi(u)$ – плотность стандартного нормального распределения.
    Можно добавить параметр "исследования" $\xi \ge 0$:
    $$ EI_\xi(x) = (\mu(x) - f^* - \xi) \Phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right) + \sigma(x) \phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right) $$
    Большее $\xi$ способствует большему исследованию.

**3.3. Алгоритм байесовской оптимизации**
1.  **Инициализация:**
    a.  Сгенерировать несколько случайных точек $x_1, x_2, \dots, x_N$.
    b.  Вычислить для них реальную функцию качества $y_i = f(x_i)$.
    c.  Обучить на этих данных $(X_{meta}, Y_{meta}) = \{(x_i, y_i)\}$ суррогатную модель (например, GP).
2.  **На каждой итерации:**
    a.  Найти точку $x_{next}$, которая максимизирует выбранную функцию улучшения (UCB, PI, EI), используя текущую суррогатную модель. (Эта оптимизация "дешевая").
    b.  Вычислить реальную функцию качества $y_{next} = f(x_{next})$ (это "дорогой" шаг).
    c.  Обновить набор данных: $D_{new} = D_{old} \cup \{(x_{next}, y_{next})\}$.
    d.  Обновить (переобучить) суррогатную модель на $D_{new}$.
    Повторять до исчерпания бюджета вычислений $f(x)$.

---

## 4. Основные понятия Обучения с подкреплением (Markov Decision Process, MDP)

RL формализуется через Марковский процесс принятия решений (MDP).
*   $s \in S$: **Состояние** (State) среды.
*   $a \in A$: **Действие** (Action), которое может совершить агент.
*   $T(s'|s,a)$ или $P(s'|s,a)$: **Функция перехода** (Transition Function). Вероятность перехода в состояние $s'$ из состояния $s$ после совершения действия $a$. Предполагается **Марковское свойство**: следующее состояние зависит только от текущего состояния и действия, а не от всей предыдущей истории.
*   $\pi(a|s)$: **Стратегия (Политика, Policy)** агента. Вероятность совершения действия $a$ в состоянии $s$. Может быть детерминированной ($a = \pi(s)$) или стохастической.
*   $r(s,a)$ или $R(s,a,s')$: **Функция награды (Reward Function)**. Награда, получаемая агентом за совершение действия $a$ в состоянии $s$ (и, возможно, переход в $s'$).
*   $r_t$: Награда, полученная на шаге $t$.
*   $R_t$: **Выигрыш (Return)** или кумулятивная дисконтированная награда, начиная с шага $t$:
    $$ R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots $$
    Иногда $R_t = r_t + \gamma R_{t+1}$ (если $r_t$ награда за $(s_t, a_t)$).
*   $\gamma \in [0,1]$: **Коэффициент дисконтирования (Discount Factor)**. Определяет важность будущих наград по сравнению с немедленными. $\gamma < 1$ обеспечивает сходимость суммы для бесконечных эпизодов.
*   Иногда используется недисконтированный выигрыш для эпизодических задач $R_t = \sum_{k=0}^{K} r_{t+k+1}$, или средняя награда $R_t = \sum_{k=0}^{K} r_{t+k+1} / (K+1)$.

**Функции оценки (Value Functions):** Оценивают "хорошесть" состояний или пар состояние-действие.
*   $V_\pi(s) = \mathbb{E}_\pi [R_t | s_t = s]$: **Функция ценности состояния (State-Value Function)**. Ожидаемый выигрыш, начиная из состояния $s$ и следуя стратегии $\pi$.
*   $Q_\pi(s,a) = \mathbb{E}_\pi [R_t | s_t = s, a_t = a]$: **Функция ценности действия (Action-Value Function / Q-function)**. Ожидаемый выигрыш, начиная из состояния $s$, совершая действие $a$, и затем следуя стратегии $\pi$.

**Оптимальные функции оценки и стратегия:**
*   $V^*(s) = \max_\pi V_\pi(s)$: Оптимальная функция ценности состояния.
*   $Q^*(s,a) = \max_\pi Q_\pi(s,a)$: Оптимальная функция ценности действия.
*   $\pi^*(a|s)$: Оптимальная стратегия. Стратегия, которая максимизирует ожидаемый выигрыш.
    Если известна $Q^*(s,a)$, то оптимальное действие в состоянии $s$: $\pi^*(s) = \arg\max_a Q^*(s,a)$.

**4.1. Уравнения Беллмана**
Рекуррентные соотношения, связывающие ценности состояний (или пар состояние-действие) с ценностями последующих состояний.
*   **$Q_\pi$ через $V_\pi$:**
    $$ Q_\pi(s,a) = \mathbb{E}_\pi[r_{t+1} + \gamma V_\pi(s_{t+1}) | s_t=s, a_t=a] = \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V_\pi(s')) $$
    (Если награда $r(s,a)$ дается за действие $a$ в $s$, то $r(s,a)$).
*   **$V_\pi$ через $Q_\pi$:**
    $$ V_\pi(s) = \mathbb{E}_{a \sim \pi(a|s)}[Q_\pi(s,a)] = \sum_a \pi(a|s) Q_\pi(s,a) $$
*   **$V_\pi$ через $V_\pi$ (Уравнение Беллмана для $V_\pi$):**
    $$ V_\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V_\pi(s')) $$
*   **$Q_\pi$ через $Q_\pi$ (Уравнение Беллмана для $Q_\pi$):**
    $$ Q_\pi(s,a) = \sum_{s'} P(s'|s,a) \left(r(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q_\pi(s',a')\right) $$

*   **Оптимальные уравнения Беллмана (Bellman Optimality Equations):**
    $$ V^*(s) = \max_a \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V^*(s')) $$
    $$ Q^*(s,a) = \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma \max_{a'} Q^*(s',a')) $$

**4.2. Q-learning (Off-policy TD Control)**
Алгоритм обучения без модели (model-free), который напрямую аппроксимирует оптимальную функцию $Q^*(s,a)$.
*   Изначально таблица $Q(s,a)$ (для всех пар $(s,a)$) заполнена случайными числами (или нулями).
*   **Итерации (пока $Q(s,a)$ не сошлась):**
    Находясь в состоянии $s$:
    1.  **Выбрать действие $a$:** Обычно с использованием $\epsilon$-жадной стратегии на основе текущих $Q(s,a')$ (например, $a = \arg\max_{a'} Q(s,a')$ с вероятностью $1-\epsilon$, иначе случайное действие).
    2.  **Совершить действие $a$, наблюдать награду $r$ и новое состояние $s'$**. (Переход $T(s,a) \rightarrow s'$, награда $r(s,a)$).
    3.  **Обновить $Q(s,a)$:**
        $$ Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \left( r + \gamma \max_{a'} Q(s',a') \right) $$
        где $\alpha$ – скорость обучения.
        Это обновление основано на TD-ошибке (Temporal Difference error): $r + \gamma \max_{a'} Q(s',a') - Q(s,a)$.
    4.  **Перейти в новое состояние:** $s \leftarrow s'$.
    5.  Если $s$ – не терминальное состояние, вернуться к шагу 1.
*   Q-learning является **off-policy** алгоритмом: он учит оптимальную $Q^*$-функцию (соответствующую жадной стратегии) независимо от того, какая стратегия используется для выбора действий на этапе исследования (behavior policy).

**4.3. Более сложные алгоритмы RL**
*   **Проблема:** Множество состояний $S$ и действий $A$ может быть очень большим или непрерывным. Хранить таблицу $Q(s,a)$ невозможно.
*   **Решение:** Использовать **аппроксиматоры функций** (например, нейронные сети) для представления $V_\pi(s)$, $Q_\pi(s,a)$ или $\pi(a|s)$. Это область Deep Reinforcement Learning (DRL).
    *   Аналог контекстных многоруких бандитов: $Q(s,a) \approx f(s,a;\theta)$.
*   **Типы RL методов:**
    *   **Value-based методы:** Учат функцию ценности (V или Q), а стратегия выводится из нее (например, жадная). (Q-learning, DQN, SARSA).
    *   **Policy-based методы (Прямые RL методы):** Явно моделируют и оптимизируют стратегию $\pi(a|s;\theta)$ напрямую. (REINFORCE, A2C, A3C).
    *   **Actor-Critic методы:** Моделируют и стратегию (**Actor**, $\pi(a|s;\theta_\pi)$) и функцию ценности (**Critic**, $V(s;\theta_V)$ или $Q(s,a;\theta_Q)$). Актер выбирает действия, критик оценивает эти действия и помогает актеру улучшаться.
*   **Offline RL (Batch RL):** Обучение на основе заранее собранного ("чужого") набора траекторий (истории взаимодействий $(s,a,r,s')$), без возможности дальнейшего взаимодействия со средой.
*   **Off-policy алгоритмы:** Стратегия, используемая для генерации данных (behavior policy), может отличаться от стратегии, которая оценивается и улучшается (target policy). Q-learning – пример.
*   **On-policy алгоритмы:** Стратегия, используемая для генерации данных, та же самая, что и улучшаемая стратегия. (SARSA, A2C).

---

## 5. Задачи обучения с подкреплением (Примеры и Среды)

**Gymnasium (бывший OpenAI Gym)**
Популярная библиотека для разработки и сравнения алгоритмов RL. Предоставляет стандартизированный интерфейс к различным средам.
*   **GitHub:** [https://github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium)
*   **Классические задачи управления:** CartPole, MountainCar, Pendulum, Acrobot.
    [https://gymnasium.farama.org/](https://gymnasium.farama.org/)
*   **Игры Atari:** Большой набор классических игр Atari 2600, предоставляющих сложные среды с визуальным входом.
    [https://ale.farama.org/](https://ale.farama.org/)
*   Другие среды: робототехника (MuJoCo), настольные игры (Go, шахматы) и т.д.

---
