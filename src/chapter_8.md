

# Лекция 8: Трансформеры

- [Лекция 8: Трансформеры](#лекция-8-трансформеры)
  - [1. Механизм внимания (краткое повторение)](#1-механизм-внимания-краткое-повторение)
  - [2. Архитектура трансформера ("Attention Is All You Need", Vaswani et al., 2017)](#2-архитектура-трансформера-attention-is-all-you-need-vaswani-et-al-2017)
  - [3. Позиционное кодирование (Positional Encoding)](#3-позиционное-кодирование-positional-encoding)
  - [4. Большие языковые модели (Large Language Models, LLM)](#4-большие-языковые-модели-large-language-models-llm)

---

## 1. Механизм внимания (краткое повторение)

**1.1. Предпосылка**
(Как обсуждалось в Лекции 7) Кодирование длинного предложения одним вектором фиксированной длины (как в классических Seq2Seq моделях) является "бутылочным горлышком". Механизм внимания позволяет декодеру на каждом шаге генерации обращаться ко всем скрытым состояниям кодировщика, взвешивая их релевантность.

**1.2. DNC (Differentiable Neural Computer) - Напоминание**
(Слайд 4, Лекция 7, слайд 37) DNC использует контроллер (нейросеть) для взаимодействия с внешней памятью через механизмы чтения и записи, которые по своей сути являются формами внимания. Это подчеркивает важность "мягкого" доступа к информации.

**1.3. Мягкое чтение / Поиск (Soft Read / Search)**
Иллюстрация концепции внимания через аналогию с поиском в памяти.
*   **`read(query)` (Жесткий поиск):** Находит точное совпадение ключа с запросом и возвращает значение.
    ```python
    def read(query, memory):
        for key, value in memory:
            if key == query:
                return value
        return None # или ошибка
    ```
*   **`avg_read(query)` (Усреднение при точном совпадении):** Если есть несколько точных совпадений, возвращает среднее их значений.
    ```python
    def avg_read(query, memory):
        matching_values = []
        for key, value in memory:
            if key == query:
                matching_values.append(value)
        if not matching_values: return None
        return sum(matching_values) / len(matching_values)
    ```
*   **`soft_read(query)` (Мягкий поиск / Взвешенное чтение):**
    Каждой паре (key, value) в памяти присваивается вес $w$, зависящий от схожести `similarity(key, query)`. Возвращается взвешенная сумма значений. Это и есть суть внимания.
    ```python
    def soft_read(query, memory, similarity_func):
        wavg_value = 0
        sum_weights = 0
        for key, value in memory:
            weight = similarity_func(key, query) # Например, косинусное сходство или dot-product
            wavg_value += weight * value
            sum_weights += weight
        if sum_weights == 0: return None # или некоторое значение по умолчанию
        return wavg_value / sum_weights # Нормализация весов (аналог Softmax)
    ```

**1.4. Scaled Dot-Product Attention**
Ключевой компонент Трансформера.
*   **Вход:** Запросы (Queries, $Q$), Ключи (Keys, $K$), Значения (Values, $V$).
    *   Для **self-attention** (внимание к себе) $Q, K, V$ получаются из одной и той же входной последовательности $X$ путем линейных преобразований:
        $Q = XW^Q$, $K = XW^K$, $V = XW^V$.
    *   Для **cross-attention** (внимание между кодировщиком и декодировщиком) $Q$ приходит из декодера, а $K, V$ – из кодировщика.

*   **Матричный вариант (для одной "головы" внимания):**
    1.  Вычисление оценок схожести: $QK^T$
    2.  Масштабирование: $\frac{QK^T}{\sqrt{d_k}}$ (где $d_k$ – размерность векторов ключей/запросов). Масштабирование предотвращает слишком большие значения аргумента Softmax, что могло бы привести к очень малым градиентам.
    3.  Применение Softmax для получения весов внимания: $\text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)$
    4.  Взвешивание Значений: $Z = \text{Softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V$

*   **Векторный вариант (для одного элемента последовательности $x_i$):**
    *   $q_i = x_i W^Q$, $k_j = x_j W^K$, $v_j = x_j W^V$
    *   Оценка схожести $i$-го запроса с $j$-м ключом: $s_{i,j} = \langle q_i, k_j \rangle$ (или $\frac{\langle q_i, k_j \rangle}{\sqrt{d_k}}$)
    *   Веса внимания: $a_{i,j} = \text{Softmax}_j(s_{i,j})$ (Softmax по всем $j$ для данного $i$)
    *   Выходной вектор для $i$-го элемента: $z_i = \sum_j a_{i,j} v_j$

*   **Многоголовое внимание (Multi-Head Attention):**
    1.  Входные $Q, K, V$ линейно проецируются $H$ раз (для $H$ "голов") с разными, обучаемыми матрицами проекций:
        $Q_h = XW^Q_h$, $K_h = XW^K_h$, $V_h = XW^V_h$ для $h=1 \dots H$.
    2.  Scaled Dot-Product Attention применяется параллельно для каждой головы:
        $\text{head}_h = \text{Attention}(Q_h, K_h, V_h) = \text{Softmax}\left(\frac{Q_h K_h^T}{\sqrt{d_k/H}}\right) V_h$
        (Размерность $d_k/H$ если общая размерность $d_k$ делится между головами).
    3.  Результаты всех голов конкатенируются и снова линейно проецируются:
        $Z = \text{Concat}(\text{head}_1, \dots, \text{head}_H) W^O$
    *   **Идея:** Разные головы могут фокусироваться на разных аспектах информации или разных типах зависимостей в последовательности.

**1.5. Переиспользование параметров между головами**
Для уменьшения числа параметров и улучшения эффективности:
*   **Multi-Head Attention (MHA):** Каждая голова имеет свои $W^Q_h, W^K_h, W^V_h$.
*   **Grouped-Query Attention (GQA):** Несколько голов запросов (Queries) делят общие проекции для ключей и значений. Например, если 8 голов Q, может быть только 2 набора K и V.
*   **Multi-Query Attention (MQA):** Все головы запросов делят один общий набор проекций для ключей и значений.
Это особенно актуально при декодировании, где на каждом шаге генерируется один токен, и кеширование K и V из предыдущих шагов экономит вычисления.

**1.6. (Q * K^T) * V computation process with caching (слайд 8)**
При авторегрессионной генерации (например, в декодере GPT) на каждом новом шаге $N$:
*   **Queries:** Вычисляются только для текущего нового токена (1 вектор).
*   **Keys & Values:** Вычисляются для текущего нового токена и добавляются к кешированным $K$ и $V$ со всех предыдущих $N-1$ шагов.
*   Матрица $K^T$ содержит ключи всех предыдущих и текущего токенов.
*   Матрица $V$ содержит значения всех предыдущих и текущего токенов.
*   Это позволяет эффективно вычислять внимание для нового токена ко всей уже сгенерированной последовательности.

**1.7. Разные окна внимания (для длинных последовательностей)**
Механизм внимания имеет квадратичную сложность $O(n^2)$ по длине последовательности $n$. Для очень длинных последовательностей это становится проблемой.
*   **(a) Full $n^2$ attention:** Стандартное внимание, каждый токен взаимодействует с каждым.
*   **(b) Sliding window attention:** Каждый токен взаимодействует только с токенами в некотором локальном окне вокруг себя. (Как в Longformer)
*   **(c) Dilated sliding window attention:** Скользящее окно с "пропусками", чтобы увеличить рецептивное поле без увеличения числа вычислений. (Как в Longformer)
*   **(d) Global + sliding window attention:** Комбинация: некоторые токены (глобальные) могут взаимодействовать со всеми, остальные – только в скользящем окне.

**1.8. Иллюстрация работы Scaled Dot-Product Attention (jalammar.github.io)**
(Слайд 10-13) Пошаговая визуализация вычислений для механизма внимания, включая матричные операции и многоголовое внимание.

**1.9. Обсуждение внимания (повторение)**
*   Обучается так же, как и другие блоки.
*   Позволяет обрабатывать более длинные последовательности (по сравнению с RNN без внимания).
*   Улучшает производительность.
*   Может применяться в произвольных сетях.
*   Добавляет параметры (но часто более эффективно, чем увеличение скрытых состояний RNN).

---

## 2. Архитектура трансформера ("Attention Is All You Need", Vaswani et al., 2017)

Трансформер полностью отказывается от рекуррентных связей в пользу механизма внимания.

**2.1. Общая структура (Encoder-Decoder)**
*   Последовательность преобразуется несколько раз (слайд 16).
*   Состоит из стека **Кодировщиков (Encoders)** и стека **Декодировщиков (Decoders)**.
    *   Обычно 6 кодировщиков и 6 декодировщиков в оригинальной модели.

**2.2. Архитектура Кодировщика (Encoder Block)**
Каждый кодировщик состоит из двух основных подслоев:
1.  **Multi-Head Self-Attention:** Слой внимания, где $Q, K, V$ приходят из выхода предыдущего слоя кодировщика (для первого кодировщика – из входных эмбеддингов + позиционное кодирование).
2.  **Position-wise Feed-Forward Network (FFN):** Полносвязная сеть из двух линейных слоев с ReLU активацией между ними, применяемая независимо к каждой позиции (каждому токену) в последовательности.
    $$ \text{FFN}(x) = \max(0, xW_1 + b_1)W_2 + b_2 $$
*   **Add & Norm (Residual Connection + Layer Normalization):** После каждого из двух подслоев применяется остаточная связь (Add) и нормализация по слою (LayerNorm).
    $$ \text{LayerNorm}(\text{SublayerOutput} + \text{SublayerInput}) $$
    Layer Normalization нормализует активации по каждому примеру в батче и по всем нейронам/каналам для данной позиции, в отличие от Batch Normalization, которая нормализует по батчу для каждого нейрона/канала.

**2.3. Архитектура Декодировщика (Decoder Block)**
Каждый декодировщик состоит из трех основных подслоев:
1.  **Masked Multi-Head Self-Attention:** Слой self-attention для выходной последовательности. "Маскированный" означает, что при вычислении внимания для $i$-го токена, он может "смотреть" только на предыдущие токены ($0 \dots i-1$) и на себя, но не на будущие ($i+1 \dots T$). Это необходимо для авторегрессионной генерации.
2.  **Multi-Head Cross-Attention (Encoder-Decoder Attention):** Слой внимания, где $Q$ приходят из выхода предыдущего подслоя декодера (Masked Self-Attention), а $K$ и $V$ – из выхода всего стека кодировщиков. Этот слой позволяет декодеру "смотреть" на входную последовательность.
3.  **Position-wise Feed-Forward Network (FFN):** Аналогично кодировщику.
*   Также используются **Add & Norm** после каждого подслоя.
*   После стека декодировщиков обычно идет линейный слой и Softmax для получения вероятностей следующего токена.

**2.4. BERT и GPT**
Две влиятельные архитектуры, основанные на Трансформере:
*   **BERT (Bidirectional Encoder Representations from Transformers):** Использует только **стек Кодировщиков** Трансформера. Обучается на задачах предсказания замаскированных слов (Masked Language Model, MLM) и предсказания следующего предложения (Next Sentence Prediction, NSP). Является **двунаправленной** моделью, так как self-attention в кодировщике позволяет каждому токену "смотреть" на все остальные токены в последовательности. Хорошо подходит для задач понимания языка (NLU).
*   **GPT (Generative Pre-trained Transformer):** Использует только **стек Декодировщиков** Трансформера (с Masked Self-Attention). Обучается на задаче предсказания следующего слова (стандартная языковая модель). Является **однонаправленной (авторегрессионной)** моделью. Хорошо подходит для задач генерации текста (NLG).

**2.5. Маскирование в BERT (MLM)**
Во время предобучения BERT случайным образом заменяет некоторые токены во входной последовательности на специальный токен `[MASK]`. Задача модели – предсказать исходные токены на месте `[MASK]`, используя контекст остальных (незамаскированных) токенов.

**2.6. Masked Self-Attention (в декодере GPT и Трансформера)**
*   Перед применением Softmax к матрице скалярных произведений $QK^T / \sqrt{d_k}$ к элементам над главной диагональю прибавляется $-\infty$.
*   Это приводит к тому, что после Softmax соответствующие ячейки (соответствующие "будущим" токенам) будут иметь нулевой вес (вероятность).
*   Таким образом, $i$-й токен может обращать внимание только на токены с позициями от $0$ до $i$.

**2.7. Vision Transformer (ViT)**
Применение архитектуры Трансформера к задачам компьютерного зрения.
1.  Изображение делится на фиксированное количество непересекающихся **патчей** (например, 16x16 пикселей).
2.  Каждый патч "выпрямляется" в вектор и линейно проецируется в эмбеддинг.
3.  К этим эмбеддингам патчей добавляются **позиционные эмбеддинги**.
4.  Часто добавляется специальный обучаемый **`[CLS]` токен** (аналогично BERT), эмбеддинг которого после прохождения через Трансформер-кодировщик используется для классификации всего изображения.
5.  Полученная последовательность эмбеддингов патчей подается на вход стандартного Трансформер-кодировщика.
6.  Выход кодировщика (например, эмбеддинг `[CLS]` токена) подается на MLP-голову для классификации.

**2.8. Анализ Трансформера**
*   **Достоинства:**
    *   **Распараллеливаемость:** Вычисления для всех токенов в слое self-attention могут выполняться параллельно (в отличие от RNN, где обработка последовательная).
    *   **State-of-the-art качество:** Демонстрирует лучшие результаты на многих задачах NLP и других областях.
    *   **Потенциально интерпретируемые результаты:** Веса внимания могут дать некоторое представление о том, на какие части входных данных модель "обращает внимание".
*   **Недостатки:**
    *   **Очень много параметров:** Особенно для больших моделей.
    *   **Нестабильность обучения:** Требуют тщательной настройки гиперпараметров, скорости обучения (warmup), нормализации.
    *   **Фиксированная длина контекста:** Стандартный Трансформер обрабатывает последовательности фиксированной максимальной длины. Для более длинных последовательностей требуются модификации (например, окна внимания, Transformer-XL).
    *   **Квадратичная сложность внимания** по длине последовательности.

---

## 3. Позиционное кодирование (Positional Encoding)

Механизм self-attention по своей природе не учитывает порядок элементов в последовательности (обрабатывает ее как "мешок слов"). Чтобы сообщить модели информацию о порядке, к входным эмбеддингам токенов добавляются позиционные кодировки.

**3.1. Синусоидальное позиционное кодирование (оригинальный Трансформер)**
Для позиции $t$ и измерения $i$ в векторе позиционного кодирования $PE$:
$$ PE(t, 2i) = \sin(t / 10000^{2i/d_{model}}) $$
$$ PE(t, 2i+1) = \cos(t / 10000^{2i/d_{model}}) $$
где $d_{model}$ – размерность эмбеддингов.
*   Каждое измерение $PE$ соответствует синусоиде с разной частотой.
*   Это позволяет модели легко определять относительные позиции, так как $PE(t+k)$ может быть представлено как линейная функция от $PE(t)$.
*   Не обучаемое, фиксированное.

**3.2. Анализ позиционного кодирования**
*   Из-за операций суммирования (в self-attention и FFN) Трансформер обрабатывает последовательность как множество, если не добавить информацию о порядке.
*   Позиционное кодирование добавляет эту информацию.
*   Вектор позиционного кода **прибавляется** к эмбеддингу токена, а не конкатенируется, для экономии параметров.

**3.3. Rotary Position Embedding (RoPE)**
*   Альтернативный метод внедрения информации о позиции.
*   Вместо прибавления, части входного вектора (эмбеддинга токена) интерпретируются как двумерные подвекторы, которые **вращаются** на угол, зависящий от позиции токена. Матрица поворота для позиции $m$ и пары измерений $(2i, 2i+1)$ с частотой $\theta_i$:
    $$ R_{\Theta,m}^{(i)} = \begin{pmatrix} \cos m\theta_i & -\sin m\theta_i \\ \sin m\theta_i & \cos m\theta_i \end{pmatrix} $$
    Эти матрицы поворота применяются к парам измерений в векторах $Q$ и $K$ перед вычислением скалярного произведения в self-attention.
*   Сохраняет относительную информацию о позиции через свойства поворотов.

**3.4. Attention with Linear Bias (ALiBi)**
*   Еще один способ информировать модель о позиции без явного добавления позиционных эмбеддингов.
*   При вычислении оценок внимания ($QK^T$) к каждой оценке $score(q_i, k_j)$ добавляется **линейный сдвиг (bias)**, зависящий от относительного расстояния $|i-j|$ между токенами $i$ и $j$. Этот сдвиг обычно имеет вид $m \cdot |i-j|$, где $m$ – это специфичный для каждой головы внимания скаляр (отрицательный, чтобы наказывать за большое расстояние).
*   Этот сдвиг добавляется **до** Softmax.
*   Позволяет модели экстраполировать на длины последовательностей, превышающие те, на которых она обучалась.

**3.5. Обучаемое позиционное кодирование**
*   Позицию можно рассматривать как категориальный признак.
*   Каждой позиции (от 0 до максимальной длины) сопоставляется обучаемый вектор эмбеддинга (аналогично эмбеддингам слов).
*   Эти обучаемые позиционные эмбеддинги прибавляются к эмбеддингам токенов.
*   Недостаток: теряется явная информация об относительном порядке, модели приходится "выучивать" ее заново из данных. Плохо экстраполирует на невиданные длины.

---

## 4. Большие языковые модели (Large Language Models, LLM)

Модели, основанные преимущественно на архитектуре Трансформера, обученные на огромных объемах текстовых данных.

**4.1. Рост числа параметров в LLM (слайд 30)**
График показывает экспоненциальный рост числа параметров в LLM с 2018 года (GPT, BERT) до современных моделей (GPT-4, PaLM, LLaMA, Bloom) с сотнями миллиардов и триллионами параметров.

**4.2. Законы масштабирования (Scaling Laws for LLMs)**
Эмпирические исследования (например, OpenAI, DeepMind) показали, что производительность LLM (измеряемая как функция потерь на тестовых данных) предсказуемо улучшается с увеличением:
*   **Размера модели (числа параметров, $N$)**
*   **Размера обучающего набора данных (числа токенов, $D$)**
*   **Объема вычислений (Compute)**
Потери $L$ обычно убывают как степенная функция от этих факторов, например:
$L(N) \approx (N/N_c)^{-\alpha_N}$, $L(D) \approx (D/D_c)^{-\alpha_D}$.
Это позволяет прогнозировать производительность более крупных моделей и оптимально распределять бюджет на вычисления, размер модели и данные.

**4.3. Выравнивание LLM (LLM Alignment)**
Процесс настройки LLM для соответствия человеческим намерениям, предпочтениям и ценностям.
*   **Supervised Fine-Tuning (SFT):** Дообучение предобученной LLM на небольшом наборе высококачественных примеров "запрос-ответ", созданных людьми.
*   **Reinforcement Learning from Human Feedback (RLHF):**
    1.  **Сбор данных сравнения:** Для одного и того же запроса генерируется несколько ответов LLM. Люди-оценщики ранжируют эти ответы от лучшего к худшему.
    2.  **Обучение модели вознаграждения (Reward Model, RM):** На основе данных сравнения обучается модель (обычно другая LLM), которая предсказывает "оценку качества" (вознаграждение) для любого ответа на данный запрос.
    3.  **Оптимизация политики с помощью RL:** Предобученная LLM (политика) дообучается с использованием алгоритмов обучения с подкреплением (например, PPO - Proximal Policy Optimization). Цель – максимизировать вознаграждение, получаемое от модели вознаграждения.
*   **Этапы (слайд 32):**
    *   Step 1: SFT
    *   Step 2: Collect comparison data and train a reward model.
    *   Step 3: Optimize a policy against the reward model using reinforcement learning.
*   **Проблемы выравнивания (слайд 33):**
    *   **Inner Alignment:** Соответствие внутренних целей модели ее явной функции потерь.
    *   **Outer Alignment:** Соответствие функции потерь истинным намерениям человека.
    *   **Mechanistic Interpretability:** Понимание того, как модель принимает решения.
    *   **Alignment Evaluation:** Оценка того, насколько хорошо модель выровнена (фактологичность, этичность, токсичность, предвзятость).
    *   **Attack on Alignment:** Попытки обойти выравнивание (privacy attacks, backdoor attacks, adversarial attacks).
*   **Пример оценки выровненных моделей (слайд 34):** Радарные диаграммы, показывающие производительность моделей по различным аспектам (полезность, ясность, вовлеченность, глубина, фактологичность).

**4.4. Retrieval Augmented Generation (RAG)**
Техника для улучшения фактологичности и актуальности ответов LLM путем предоставления ей доступа к внешней базе знаний во время генерации.
1.  **Пользовательский запрос (Query).**
2.  **Извлечение (Retrieval):** Запрос используется для поиска релевантной информации (контекста) в базе знаний (например, векторной базе данных документов). Документы предварительно индексируются с помощью эмбеддинг-модели.
3.  **Дополнение (Augmentation):** Извлеченный релевантный контекст объединяется с исходным запросом пользователя.
4.  **Генерация (Generation):** Дополненный запрос (запрос + контекст) подается на вход LLM для генерации ответа.
*   Это позволяет LLM использовать информацию, которой не было в ее обучающих данных, и ссылаться на источники.

**4.5. Chain-of-Thought (CoT) Prompting**
Техника промптинга (формулирования запросов к LLM), которая побуждает модель генерировать последовательность промежуточных шагов рассуждения перед тем, как дать окончательный ответ.
*   **Пример (слайд 36):**
    *   **Стандартный промпт:** "Q: Roger has 5 balls... A: 7."
    *   **CoT промпт:** "Q: Roger has 5 balls. He buys 2 more. How many balls does he have now? A: He started with 5 and buys 2 more. 5 + 2 = 7. The answer is 7."
*   Предоставление примеров с такими "цепочками мыслей" (в few-shot промптинге) или просто указание модели "Let's think step by step" (в zero-shot промптинге) может значительно улучшить ее способность решать задачи, требующие многошаговых рассуждений (например, арифметические, логические).

---
