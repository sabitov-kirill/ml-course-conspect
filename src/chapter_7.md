# Лекция 7: Рекуррентные сети и работа с последовательностями

- [Лекция 7: Рекуррентные сети и работа с последовательностями](#лекция-7-рекуррентные-сети-и-работа-с-последовательностями)
  - [1. Рекуррентные нейронные сети (RNN)](#1-рекуррентные-нейронные-сети-rnn)
  - [2. Обработка последовательностей](#2-обработка-последовательностей)
  - [3. Продвинутые архитектуры RNN](#3-продвинутые-архитектуры-rnn)
  - [4. Больше связей в RNN](#4-больше-связей-в-rnn)
  - [5. Механизм внимания (Attention Mechanism)](#5-механизм-внимания-attention-mechanism)
  - [6. Авторегрессионная генерация](#6-авторегрессионная-генерация)

---

## 1. Рекуррентные нейронные сети (RNN)

**1.1. Рекурсия и биологическая аналогия**
*   Биологические нейронные сети обладают рекуррентными связями (нейроны могут влиять сами на себя или на предыдущие по ходу обработки нейроны через обратные связи).
*   Для моделирования процессов, аналогичных работе мозга (например, обработка временной информации, память), требуются рекурсивные функции.

**1.2. Проблема дифференцирования рекурсивных функций**
*   Стандартное автоматическое дифференцирование (AD) требует ациклического графа вычислений. Рекуррентные связи создают циклы.
*   **Решение:** "Разворот во времени" (Unrolling in Time) – рекурсивная функция вычисляется итеративно на протяжении конечного числа шагов, превращая циклический граф в глубокий ациклический граф. Каждый временной шаг становится слоем в этом развернутом графе.

**1.3. Временной ряд и последовательность**
*   Задачу обработки рекурсивной функции можно свести к обработке временнóго ряда.
*   Временнóй ряд (последовательность наблюдений, упорядоченных во времени) можно рассматривать как частный случай общей последовательности.

**1.4. Сеть Хопфилда (Hopfield Network)**
*   Один из ранних примеров рекуррентной нейронной сети (1982).
*   Представляет собой **ассоциативную память:** способна запоминать паттерны и восстанавливать их из зашумленных или неполных входных данных.
*   Нейроны бинарные (или биполярные). Связи симметричные ($w_{ij} = w_{ji}$), нет связей нейрона с самим собой ($w_{ii}=0$).
*   Динамика сети: нейроны асинхронно обновляют свои состояния до тех пор, пока сеть не сойдется к одному из запомненных паттернов (аттрактору).
*   Могут демонстрировать стабильное поведение (схождение к аттрактору), осцилляции или хаотическое поведение в зависимости от параметров и структуры.

**1.5. Разворот во времени (Unfolding/Unrolling in Time)**
*   Схема (слайд 5):
    *   Скрытое состояние $h(t)$ на шаге $t$ зависит от входа $x(t)$ и предыдущего скрытого состояния $h(t-1)$.
    *   $h(t+1)$ вычисляется блоком "Hidden Units" и передается на следующий временной шаг через блок "Delay".
    *   Выход "Outputs" генерируется на основе $h(t+1)$ (или $h(t)$).
*   При развороте (слайд 6) рекуррентный блок A копируется для каждого временного шага $t=0, 1, \dots, T$. Скрытое состояние $h_t$ передается от копии A на шаге $t$ к копии A на шаге $t+1$.
*   Это позволяет применять алгоритм обратного распространения ошибки (Backpropagation Through Time, BPTT).

*Неправильный разворот во времени (терминологический нюанс):*
Иногда говорят, что последовательности сводят к рекурсии, а затем обратно к последовательностям. Более точно, рекуррентное определение используется для обработки последовательностей.
Тип "последовательность от X", $Seq(X)$, можно определить рекурсивно:
$Seq(X) = \epsilon \lor (X \times Seq(X))$, где $\epsilon$ – пустая последовательность, $\lor$ – или, $\times$ – конкатенация/добавление элемента.

---

## 2. Обработка последовательностей

**2.1. Области применения RNN**
*   **Временные ряды:** Прогнозирование погоды, цен на акции, анализ медицинских сигналов (ЭКГ).
*   **Естественные языки (NLP):** Машинный перевод, генерация текста, анализ тональности, распознавание именованных сущностей.
*   **Речь:** Распознавание речи, синтез речи.
*   **Динамические системы:** Моделирование физических или биологических систем.
*   **Изображения и видео:** Описание изображений (image captioning), анализ видео (action recognition).
*   В целом, любые данные, имеющие последовательную структуру.

**2.2. Методы обработки последовательностей (кроме RNN)**
*   **Числовые последовательности (классические методы):**
    *   **Спектральные методы:** Анализ частотных характеристик (преобразование Фурье).
    *   **Временные методы:** Авторегрессионные модели (AR), скользящего среднего (MA), ARMA, ARIMA.
    *   **Частотно-временные методы:** Вейвлет-преобразование, кратковременное преобразование Фурье (STFT).
*   **Вероятностные графические модели:**
    *   Рассматривают временной ряд как результат стохастического процесса с условными независимостями.
    *   **Скрытые Марковские Модели (HMM):** Моделируют последовательности наблюдений, зависящие от скрытых состояний, которые образуют Марковскую цепь.
    *   **Динамические Байесовские Сети (DBN):** Обобщение HMM, позволяющее моделировать более сложные зависимости между переменными во времени.

**2.3. Обработка последовательности с помощью "простой" RNN**
На каждом временном шаге $i$:
$$ (h_i, y_i) = f_a(h_{i-1}, x_i) $$
*   $x_i$: элемент входной последовательности на шаге $i$.
*   $h_{i-1}$: скрытое состояние (память) с предыдущего шага.
*   $h_i$: новое скрытое состояние.
*   $y_i$: выход на шаге $i$.
*   $a$: обучаемые параметры функции $f_a$ (веса), общие для всех временных шагов.
Типичная реализация "простой" RNN ячейки:
$$ h_i = \tanh(W_{hh}h_{i-1} + W_{xh}x_i + b_h) $$
$$ y_i = W_{hy}h_i + b_y $$
(Функция активации для выхода может быть другой, например, softmax для классификации).

**2.4. Анализ "простой" RNN**
*   **Преимущества:**
    *   Способны аппроксимировать не просто статические функции, а динамические системы (функции с памятью).
    *   Являются частью "экосистемы" нейронных сетей, интегрируются с другими типами слоев.
*   **Недостатки:**
    *   **Проблема исчезающих/взрывающихся градиентов (Vanishing/Exploding Gradients):** При распространении градиента через много временных шагов (в BPTT) он может экспоненциально уменьшаться (исчезать) или увеличиваться (взрываться) из-за многократных умножений на матрицу весов $W_{hh}$. Это затрудняет обучение долговременных зависимостей.
    *   **Трудности с долговременными зависимостями:** Из-за проблемы градиентов простые RNN плохо запоминают информацию на длительных интервалах.
    *   Всегда смешивают предыдущий сигнал (память) с текущим входом, что может приводить к потере информации.
    *   Обучение может быть медленным.

---

## 3. Продвинутые архитектуры RNN

Разработаны для решения проблемы исчезающих/взрывающихся градиентов и улучшения способности запоминать долговременные зависимости.

**3.1. Долгая краткосрочная память (Long Short-Term Memory, LSTM)**
*   **Идея:** Ввести отдельный **блок памяти (cell state, $C_t$)**, который может хранить информацию на длительных интервалах. Информация в этот блок добавляется или удаляется с помощью специальных **вентилей (gates)**.
*   LSTM ячейка параметрическая и используется для хранения "глобального" состояния $C_t$ и генерации "локального" скрытого состояния $h_t$.

**Компоненты LSTM ячейки:**
*   **Лента (Conveyor Belt) / Состояние ячейки ($C_t$):** Основной поток информации. Операции с $C_t$ в основном аддитивные или поэлементное умножение на значения вентилей, что позволяет градиентам легко проходить.
    $$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$
*   **Фильтр забывания (Forget Gate, $f_t$):** Определяет, какую информацию из предыдущего состояния ячейки $C_{t-1}$ нужно "забыть".
    $$ f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) $$
    $\sigma$ – сигмоидная функция (выход в $[0,1]$). $[h_{t-1}, x_t]$ – конкатенация предыдущего скрытого состояния и текущего входа.
*   **Фильтр входа (Input Gate, $i_t$):** Определяет, какую новую информацию из кандидата на обновление $\tilde{C}_t$ нужно записать в состояние ячейки.
    $$ i_t = \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) $$
*   **Кандидат на обновление состояния ($\tilde{C}_t$):** Новая информация, которая может быть добавлена к состоянию ячейки.
    $$ \tilde{C}_t = \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) $$
    $\tanh$ – гиперболический тангенс (выход в $[-1,1]$).
*   **Обновление памяти (Состояния ячейки):** Комбинация старой информации (пропущенной через фильтр забывания) и новой информации (пропущенной через фильтр входа).
    $$ C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t $$
    $\odot$ – поэлементное умножение.
*   **Фильтр выхода (Output Gate, $o_t$):** Определяет, какая часть состояния ячейки $C_t$ будет использована для формирования скрытого состояния $h_t$.
    $$ o_t = \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) $$
*   **Обновление скрытого состояния ($h_t$):**
    $$ h_t = o_t \odot \tanh(C_t) $$

**LSTM со смотровыми глазками (Peephole Connections):**
Модификация, где вентили ($f_t, i_t, o_t$) также зависят от состояния ячейки $C_{t-1}$ (для $f_t, i_t$) или $C_t$ (для $o_t$).
$$ f_t = \sigma(W_f \cdot [C_{t-1}, h_{t-1}, x_t] + b_f) $$
$$ i_t = \sigma(W_i \cdot [C_{t-1}, h_{t-1}, x_t] + b_i) $$
$$ o_t = \sigma(W_o \cdot [C_t, h_{t-1}, x_t] + b_o) $$

**3.2. Gated Recurrent Unit (GRU)**
*   Упрощенная версия LSTM с меньшим количеством параметров и вентилей. Часто показывает сравнимую производительность.
*   Объединяет фильтры забывания и входа в один **фильтр обновления (update gate, $z_t$)**.
*   Использует **фильтр сброса (reset gate, $r_t$)**.
*   Не имеет отдельного состояния ячейки $C_t$, вся информация хранится в $h_t$.
*   Уравнения:
    $$ z_t = \sigma(W_z \cdot [h_{t-1}, x_t]) \quad \text{(Update gate)} $$
    $$ r_t = \sigma(W_r \cdot [h_{t-1}, x_t]) \quad \text{(Reset gate)} $$
    $$ \tilde{h}_t = \tanh(W \cdot [r_t \odot h_{t-1}, x_t]) \quad \text{(Candidate hidden state)} $$
    $$ h_t = (1 - z_t) \odot h_{t-1} + z_t \odot \tilde{h}_t $$

**3.3. Анализ LSTM/GRU**
*   **Достоинства:**
    *   Значительно лучше справляются с долговременными зависимостями по сравнению с простыми RNN.
    *   Менее подвержены проблеме исчезающих/взрывающихся градиентов благодаря вентильной структуре и аддитивным обновлениям состояния ячейки.
*   **Недостатки:**
    *   Более вычислительно сложные и требуют больше времени для обучения, чем простые RNN.
    *   Все еще могут забывать очень длинные зависимости, хотя и реже.

---

## 4. Больше связей в RNN

**4.1. Добавление обратного направления (Bidirectional RNN, BiRNN)**
*   Для многих задач (например, машинный перевод, анализ тональности) информация не только из прошлого, но и из будущего контекста может быть полезна для обработки текущего элемента последовательности.
    *   Пример: "He said 'Teddy bears are on **sail**!'" vs "He said 'Teddy Roosevelt was a great **President**!'" – значение "Teddy" зависит от последующих слов.
*   **BiRNN:** Состоит из двух независимых RNN, обрабатывающих последовательность в двух направлениях:
    *   **Прямая RNN (Forward RNN):** Обрабатывает последовательность от начала к концу ($x_0, x_1, \dots, x_T$), генерируя скрытые состояния $\overrightarrow{h_t}$.
    *   **Обратная RNN (Backward RNN):** Обрабатывает последовательность от конца к началу ($x_T, x_{T-1}, \dots, x_0$), генерируя скрытые состояния $\overleftarrow{h_t}$.
*   Выход BiRNN на шаге $t$ обычно является конкатенацией или другой комбинацией $\overrightarrow{h_t}$ и $\overleftarrow{h_t}$.
    $$ y_t = g([\overrightarrow{h_t}, \overleftarrow{h_t}]) $$
*   Подходит для задач, где доступна вся входная последовательность (не для онлайн-прогнозирования).

**4.2. Многослойная рекуррентная нейронная сеть (Stacked/Deep RNN)**
*   Несколько слоев RNN укладываются друг на друга.
*   Выходная последовательность скрытых состояний одного слоя RNN ($h^{(l)}_t$) становится входной последовательностью для следующего слоя RNN ($h^{(l+1)}_t$).
*   Позволяет сети изучать более сложные иерархические представления последовательностей.
*   На схеме (слайд 26):
    *   $h_t$ – скрытые состояния первого слоя.
    *   $z_t$ – скрытые состояния второго слоя.
    *   $x_t$ – вход, $O_t$ – выход.
    *   Матрицы $S, W1, G, W2, V$ – обучаемые веса.

**4.3. Классификация архитектур RNN по типу задачи (many-to-many, etc.)**
В зависимости от того, как входы и выходы соотносятся по длине последовательности:
*   **One-to-one:** Стандартная нейронная сеть (не рекуррентная), один вход – один выход. (Пример: классификация изображений).
*   **One-to-many:** Один вход (например, изображение или вектор) – последовательность на выходе. (Пример: генерация описания изображения, image captioning).
*   **Many-to-one:** Последовательность на входе – один выход. (Пример: анализ тональности текста, классификация последовательности).
*   **Many-to-many (синхронный):** Входная и выходная последовательности имеют одинаковую длину, выход на шаге $t$ зависит от входа на шаге $t$ (и предыдущих). (Пример: разметка частей речи для каждого слова, видеоклассификация по кадрам).
*   **Many-to-many (асинхронный / Seq2Seq):** Входная и выходная последовательности могут иметь разную длину. (Пример: машинный перевод, распознавание речи).

**4.4. Модель Seq2Seq (Sequence-to-Sequence)**
*   Архитектура для задач many-to-many, где длины входной и выходной последовательностей могут различаться.
*   Состоит из двух основных частей:
    1.  **Кодировщик (Encoder):** RNN (например, LSTM, GRU), которая обрабатывает входную последовательность ($A, B, C$) и сжимает ее в векторное представление фиксированной длины – **вектор контекста (context vector, $c$)**. Обычно это последнее скрытое состояние кодировщика.
    2.  **Декодировщик (Decoder):** Другая RNN, которая инициализируется вектором контекста $c$ и генерирует выходную последовательность ($W, X, Y, Z, \text{<eos>}$) шаг за шагом. На каждом шаге декодер получает предыдущий сгенерированный элемент и свое предыдущее скрытое состояние для генерации следующего элемента.
*   Между кодировщиком и декодировщиком передается вектор контекста, кодирующий всю входную последовательность.
*   Ячейки RNN (в кодировщике и декодировщике) могут быть любой сложности (простые RNN, LSTM, GRU).
*   **Проблема:** Вектор контекста фиксированной длины становится "бутылочным горлышком" для длинных последовательностей, так как ему сложно удержать всю информацию. Это решается механизмом внимания.

---

## 5. Механизм внимания (Attention Mechanism)

**5.1. Предпосылка**
*   Предложение (особенно длинное) очень сложно или невозможно адекватно закодировать одним вектором контекста фиксированной длины.
*   **Идея:** Вместо того, чтобы полагаться на один вектор контекста, декодер на каждом шаге генерации может "обращать внимание" на различные части входной последовательности, используя все скрытые состояния кодировщика.

**5.2. Использование векторов скрытых состояний кодировщика**
*   Декодеру на каждом шаге $t$ предоставляется доступ ко всем скрытым состояниям кодировщика $\{ \bar{h}_1, \bar{h}_2, \dots, \bar{h}_S \}$, где $S$ – длина входной последовательности.
*   Декодер вычисляет "веса внимания" $\alpha_{ts}$ для каждого скрытого состояния кодировщика $\bar{h}_s$. Эти веса показывают, насколько релевантно каждое $\bar{h}_s$ для генерации выхода на текущем шаге $t$ декодера.
*   На основе этих весов вычисляется **контекстный вектор $c_t$** как взвешенная сумма скрытых состояний кодировщика. Этот $c_t$ специфичен для каждого шага декодера $t$.

**5.3. Вычисление механизма внимания**
1.  **Оценка релевантности (Alignment Score Function, $score(h_t, \bar{h}_s)$):**
    Вычисляется "оценка" того, насколько скрытое состояние декодера на предыдущем шаге $h_{t-1}$ (или текущее $h_t$ перед генерацией выхода) "соответствует" каждому скрытому состоянию кодировщика $\bar{h}_s$.
    *   **Типы функций score:**
        *   **Content-based (Dot-Product):** $$ score(h_t, \bar{h}_s) = h_t^T \bar{h}_s $$
        *   **Additive (Bahdanau Attention):** $$ score(h_t, \bar{h}_s) = v_a^T \tanh(W_a[h_t; \bar{h}_s]) $$ (где $W_a, v_a$ – обучаемые параметры)
        *   **General:** $$ score(h_t, \bar{h}_s) = h_t^T W_a \bar{h}_s $$
        *   **Scaled Dot-Product:** $$ score(h_t, \bar{h}_s) = \frac{h_t^T \bar{h}_s}{\sqrt{d_k}} $$ (где $d_k$ – размерность векторов, используется в Трансформерах)
2.  **Веса внимания (Attention Weights, $\alpha_{ts}$):**
    Оценки $score$ нормализуются с помощью функции softmax, чтобы получить распределение вероятностей (веса внимания) по скрытым состояниям кодировщика:
    $$ \alpha_{ts} = \frac{\exp(score(h_t, \bar{h}_s))}{\sum_{s'=1}^S \exp(score(h_t, \bar{h}_{s'}))} $$
3.  **Контекстный вектор (Context Vector, $c_t$):**
    Вычисляется как взвешенная сумма скрытых состояний кодировщика:
    $$ c_t = \sum_{s=1}^S \alpha_{ts} \bar{h}_s $$
4.  **Вектор внимания (Attention Vector, $a_t$) / Использование контекста:**
    Контекстный вектор $c_t$ используется вместе с текущим скрытым состоянием декодера $h_t$ для генерации выхода на шаге $t$. Например, они конкатенируются и пропускаются через полносвязный слой:
    $$ a_t = f(c_t, h_t) = \tanh(W_c[c_t; h_t]) $$
    Затем $a_t$ используется для предсказания следующего токена $y_t$.

**5.4. Похожесть слов (визуализация внимания)**
Матрица весов внимания $\alpha_{ts}$ показывает, на какие слова входной последовательности (ось s) модель "смотрела" при генерации каждого слова выходной последовательности (ось t). Это позволяет интерпретировать, как модель связывает слова в разных языках при переводе.

**5.5. Обсуждение механизма внимания**
*   Обучается так же, как и другие блоки нейронной сети (end-to-end с помощью обратного распространения ошибки).
*   Позволяет обрабатывать более длинные последовательности, так как снимает нагрузку с одного вектора контекста.
*   В целом, улучшает производительность моделей Seq2Seq.
*   Может применяться в различных архитектурах, не только RNN.
*   Добавляет больше параметров в модель.

**5.6. Differentiable Neural Computer (DNC)**
*   Сложная архитектура, сочетающая нейронную сеть (контроллер) с внешней памятью.
*   Контроллер может читать из памяти и писать в нее с помощью механизмов, похожих на внимание ("read heads" и "write heads").
*   Способна к более сложным формам рассуждений и манипулирования информацией, чем стандартные RNN.

---

## 6. Авторегрессионная генерация

Процесс генерации последовательности, где каждый следующий элемент генерируется на основе предыдущих сгенерированных элементов.

**6.1. Типы генерации**
*   **Генерация из вектора скрытого состояния:** Например, декодер в модели Seq2Seq без внимания генерирует последовательность, начиная с одного вектора контекста.
*   **Генерация из последовательности (продолжение):** Модели подается начальная часть последовательности (затравка, prompt), и она генерирует ее продолжение.

**6.2. Beam Search (Лучевой поиск)**
*   Алгоритм декодирования, используемый для генерации последовательностей (например, в машинном переводе, генерации текста).
*   Вместо того, чтобы на каждом шаге выбирать только один наиболее вероятный следующий токен (жадный поиск), Beam Search поддерживает $k$ (размер "луча", beam_width) наиболее вероятных частичных гипотез (последовательностей).
*   На каждом шаге для каждой из $k$ гипотез генерируются все возможные продолжения на один токен. Из всех полученных гипотез снова выбираются $k$ наиболее вероятных (обычно по сумме логарифмов вероятностей токенов).
*   Процесс продолжается до генерации токена конца последовательности (`<eos>`) или достижения максимальной длины.
*   Дает лучшие результаты, чем жадный поиск, но более вычислительно затратен.

**6.3. Выбор следующего токена (Sampling Strategies)**
На каждом шаге генерации модель (обычно после слоя Softmax) выдает распределение вероятностей по всему словарю для следующего токена.
*   **Жадный поиск (Greedy Search):** Выбирается токен с максимальной вероятностью.
*   **Сэмплирование (Sampling):** Токен выбирается случайным образом из распределения вероятностей.
    *   **Температура (Temperature Scaling):** Логиты перед Softmax делятся на параметр температуры $T$.
        *   $T \rightarrow 0$: распределение становится более "пиковым", приближаясь к жадному поиску.
        *   $T = 1$: стандартный Softmax.
        *   $T > 1$: распределение становится более "плоским", увеличивая случайность и разнообразие генерируемого текста.
*   **Top-k Sampling:** На каждом шаге рассматриваются только $k$ наиболее вероятных токенов, и следующий токен сэмплируется из их (перенормированного) распределения.
*   **Top-p (Nucleus) Sampling:** Выбирается минимальный набор токенов, чья суммарная вероятность превышает порог $p$. Следующий токен сэмплируется из этого набора.

---
