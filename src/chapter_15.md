
# Лекция 15: Обучение с подкреплением (Reinforcement Learning, RL)

- [Лекция 15: Обучение с подкреплением (Reinforcement Learning, RL)](#лекция-15-обучение-с-подкреплением-reinforcement-learning-rl)
  - [1. Введение в Обучение с подкреплением (Reinforcement Learning, RL)](#1-введение-в-обучение-с-подкреплением-reinforcement-learning-rl)
  - [2. Задача о многоруком бандите (Multi-Armed Bandit, MAB)](#2-задача-о-многоруком-бандите-multi-armed-bandit-mab)
  - [3. Байесовская оптимизация (Bayesian Optimization)](#3-байесовская-оптимизация-bayesian-optimization)
  - [4. Основные понятия Обучения с подкреплением (Markov Decision Process, MDP)](#4-основные-понятия-обучения-с-подкреплением-markov-decision-process-mdp)
  - [5. Задачи обучения с подкреплением (Примеры и Среды)](#5-задачи-обучения-с-подкреплением-примеры-и-среды)

---

## 1. Введение в Обучение с подкреплением (Reinforcement Learning, RL)

**1.1. Основная идея**
*   **Агент (Agent):** Обучаемая сущность, которая взаимодействует со **средой (Environment)**.
*   **Взаимодействие:**
    1.  Агент наблюдает текущее **состояние (State)** среды.
    2.  На основе состояния агент выбирает и совершает **действие (Action)**.
    3.  Среда реагирует на действие агента:
        *   Переходит в **новое состояние**.
        *   Выдает агенту **награду (Reward)** или штраф.
*   **Задача агента:** Научиться такой **стратегии (Policy)** выбора действий, которая максимизирует некоторую **суммарную (кумулятивную) награду** на протяжении времени.
*   RL больше похоже на то, как происходит обучение в реальном мире (методом проб и ошибок, с отложенной обратной связью).

**1.2. Дилемма исследования против использования (Exploration vs. Exploitation Tradeoff)**
Ключевая проблема в RL.
*   **Использование (Exploitation):** Агент выбирает действия, которые, по его текущим знаниям, принесут максимальную немедленную или ожидаемую будущую награду. Он использует уже известные хорошие стратегии.
*   **Исследование (Exploration):** Агент выбирает действия, о которых у него мало информации, или которые он ранее не пробовал. Это делается для того, чтобы собрать больше данных о среде и, возможно, найти еще лучшие стратегии, чем известные на данный момент.
*   **Баланс:**
    *   Слишком много использования: агент может застрять в локальном оптимуме, не найдя глобально лучшую стратегию.
    *   Слишком много исследования: агент может тратить много времени на неоптимальные действия, не получая высокой награды.
*   **Наивное решение:** Явное разделение процесса на фазу исследования (например, случайные действия для сбора данных) и фазу использования (применение лучшей найденной стратегии). Обычно менее эффективно, чем стратегии, совмещающие исследование и использование.

---

## 2. Задача о многоруком бандите (Multi-Armed Bandit, MAB)

**2.1. Постановка задачи**
*   **Частный случай RL:** Агент взаимодействует со средой, которая имеет только **одно состояние**.
*   Награда зависит **только от выбранного действия**.
*   **Действие (Action):** Выбор одной из $K$ "ручек" (рычагов) игрового автомата ("многорукого бандита").
*   **Награда (Reward):** Случайная величина, получаемая от выбранной ручки. Каждая ручка $i$ имеет свое (неизвестное агенту) распределение наград с некоторым средним значением $\mu_i$.
*   **Цель агента:** Максимизировать суммарный выигрыш за $T$ попыток (дерганий ручек).
*   **Дилемма Exploration vs. Exploitation в MAB:**
    *   **Exploitation:** Дергать ручку, которая на данный момент кажется наилучшей (имеет наибольшую оценку среднего выигрыша).
    *   **Exploration:** Дергать другие ручки, чтобы точнее оценить их средний выигрыш и, возможно, найти ручку лучше текущей оптимальной.

**2.2. Стратегии решения задачи MAB**

*   **Наивное решение (A/B-тестирование как MAB):**
    1.  **Фаза исследования:** Дёрнуть за каждую ручку определённое (одинаковое) число раз.
    2.  Вычислить для каждой ручки оценку среднего выигрыша.
    3.  **Фаза использования:** Оставшийся бюджет (попытки) потратить на ручку с наибольшим оценённым средним.
    *   График (слайд 8) показывает, что при A/B тестировании ресурсы тратятся на исследование всех опций (A, B, C) даже после того, как одна из них (Option A) показывает себя лучше. "Bandit Selection" быстрее переключается на лучшую опцию.

*   **Жадный алгоритм (Greedy Algorithm):**
    1.  Дёрнуть за каждую ручку по одному разу (для начальной оценки).
    2.  На каждом последующем шаге выбирать ручку с **максимальной текущей оценкой** среднего выигрыша.
    3.  После получения награды обновить оценку среднего выигрыша для выбранной ручки.
    *   **Недостаток:** Может "застрять" на субоптимальной ручке, если она случайно дала хороший выигрыш вначале, а истинно оптимальная ручка случайно дала плохой.

*   **$\epsilon$-жадный алгоритм ($\epsilon$-Greedy):**
    *   На каждом шаге:
        *   С вероятностью $1-\epsilon$: выбирается **жадное** действие (ручка с наилучшей текущей оценкой) – Exploitation.
        *   С вероятностью $\epsilon$: выбирается **случайная** ручка (из всех $K$ ручек равновероятно) – Exploration.
    *   Параметр $\epsilon$ (малое число, например, 0.1) контролирует баланс. Можно использовать убывающий $\epsilon(t)$.
    *   После получения награды оценка среднего обновляется.
    *   **Недостаток:** При исследовании выбирает случайную ручку, не учитывая, насколько плохи или хороши могут быть другие неоптимальные ручки.

*   **Стратегия SoftArgMax (Softmax Exploration / Boltzmann Exploration):**
    *   На каждом шаге ручка $i$ выбирается с вероятностью $p_i$, которая зависит от текущей оценки ее среднего выигрыша $\hat{\mu}_i$:
        $$ p_i = \frac{\exp(\hat{\mu}_i / \tau)}{\sum_{j=1}^K \exp(\hat{\mu}_j / \tau)} $$
        где $\tau > 0$ – **температурный параметр**.
    *   **Низкая температура ($\tau \rightarrow 0$):** Выбор приближается к жадному (argmax) – больше использования.
    *   **Высокая температура ($\tau \rightarrow \infty$):** Выбор приближается к равномерному случайному – больше исследования.
    *   Температуру $\tau$ можно изменять со временем (например, уменьшать).

*   **Верхний доверительный интервал (Upper Confidence Bound, UCB1):**
    *   Для каждой ручки $i$ вычисляется метрика, которая является суммой текущей оценки среднего выигрыша $\bar{q}_i$ и "бонуса за неопределенность":
        $$ \text{UCB}_i = \bar{q}_i + \lambda \cdot \sqrt{\frac{2 \ln(n)}{n_i}} $$
        *   $\bar{q}_i$: текущая оценка среднего выигрыша $i$-й ручки.
        *   $n$: общее число совершенных попыток (дерганий всех ручек).
        *   $n_i$: число раз, когда была выбрана $i$-я ручка.
        *   $\lambda$: константа, контролирующая вес неопределенности (исследования). Часто $\lambda = \sqrt{2}$ или настраивается.
    *   На каждом шаге выбирается ручка $i$ с **максимальной метрикой $\text{UCB}_i$**.
    *   Идея: ручки, которые дергались мало ($n_i$ мало) или показывают высокую вариативность, получат больший бонус за неопределенность и будут исследоваться чаще.
    *   Параметр $\lambda$ в формуле на слайде может быть также нужен для согласования размерностей, если $\bar{q}_i$ и член неопределенности имеют разные масштабы.

**2.3. Сравнение алгоритмов: Сожаление (Regret)**
*   **Сожаление (Regret):** Разница между суммарной наградой, полученной оптимальной стратегией (всегда дергать лучшую ручку $\mu^*$), и суммарной наградой, полученной данным алгоритмом за $T$ шагов.
    $$ R_T = T \mu^* - \sum_{t=1}^T r_t $$
    где $r_t$ – награда, полученная алгоритмом на шаге $t$.
*   Цель – минимизировать сожаление. Хорошие алгоритмы имеют сожаление, растущее медленнее, чем линейно (например, $O(\log T)$ или $O(\sqrt{T})$).
*   Графики (слайд 13) показывают, как суммарная награда алгоритма отстает от максимальной возможной. "Stuck at suboptimal arm" иллюстрирует проблему жадного алгоритма.

**2.4. Контекстные многорукие бандиты (Contextual Bandits)**
*   Обобщение MAB, где перед выбором действия агент наблюдает некоторое **состояние (контекст, $x$ )**.
*   Функция выигрыша для каждой ручки $a$ теперь зависит от контекста: $\mu_a(x)$.
*   Вместо оценки простого среднего выигрыша $\mu_a$, требуется строить **модель $f_a(x) \approx \mu_a(x)$**, аппроксимирующую зависимость выигрыша от контекста для каждой ручки $a$.
*   Для каждой "ручки" (действия) строится своя модель предсказания награды.
*   Обычная задача MAB – частный случай, когда контекст всегда один и тот же (или его нет), и модель $f_a(x)$ – это просто константа $\mu_a$.
*   **LinUCB (Linear UCB):** Если предполагается, что награда линейно зависит от контекста $\mu_a(x) = \langle x, \theta_a \rangle$, то UCB-подобная стратегия может быть использована:
    $$ \text{Action} = \arg\max_a \left( \langle x, \hat{\theta}_a \rangle + \alpha \sqrt{x^T (F_a^T F_a)^{-1} x} \right) $$
    (Формула на слайде $x^T (F_a^T F_a)^{-1} x$ представляет собой дисперсию предсказания линейной модели, где $F_a$ - матрица признаков для обучения $\theta_a$).

---

## 3. Байесовская оптимизация (Bayesian Optimization)

**3.1. Мотивация**
*   Задача: найти максимум (или минимум) "дорогой" целевой функции $f(x)$, т.е. функции, каждое вычисление которой занимает много ресурсов (времени, денег). Пример: подбор гиперпараметров нейронной сети, где $f(x)$ – это качество модели с гиперпараметрами $x$ после обучения.
*   **Идея:** Уменьшить число вычислений $f(x)$ путем построения **суррогатной модели (surrogate model)**, которая аппроксимирует $f(x)$ и является "дешевой" для вычисления.
*   Алгоритм оптимизации будет работать с суррогатной моделью для выбора следующей точки $x$ для вычисления $f(x)$.
*   Дорогие вызовы $f(x)$ используются для обучения/обновления суррогатной функции.

**3.2. Функции улучшения (Acquisition Functions)**
Алгоритм байесовской оптимизации на каждом шаге выбирает следующую точку $x$ для вычисления $f(x)$, максимизируя **функцию улучшения**. Эта функция балансирует между исследованием (области с высокой неопределенностью суррогатной модели) и использованием (области, где суррогатная модель предсказывает высокое значение).
Суррогатная модель (часто Гауссовский процесс) предоставляет предсказание среднего $\mu(x)$ и неопределенности (дисперсии $\sigma^2(x)$ или стандартного отклонения $\sigma(x)$) для $f(x)$ в любой точке $x$.

*   **Верхняя доверительная граница (Upper Confidence Bound, UCB):**
    (Уже обсуждалась в контексте GP, см. Лекцию 14)
    $$ \text{UCB}(x) = \mu(x) + \lambda \cdot \sigma(x) $$
    *   $\lambda$ – вес, контролирующий важность неопределенности.
    *   Теоретически $\lambda$ можно уменьшать для перехода от исследования к использованию. На практике часто используют $\lambda(t)$, растущую с номером шага $t$, например, $\lambda(t) = O(\sqrt{\log t})$.
    *   Пример графика (слайд 18) показывает, как UCB (синяя линия) выше в областях с высокой неопределенностью (широкий зеленый доверительный интервал) или высоким предсказанным средним (красная линия).

*   **Вероятность улучшения (Probability of Improvement, PI):**
    Пусть $f^*$ – текущее наилучшее наблюденное значение $f(x)$.
    Улучшение $I(x) = \max(f(x) - f^*, 0)$.
    Предполагая, что $f(x) \sim N(\mu(x), \sigma^2(x))$ (из GP), тогда $f(x) = \mu(x) + \sigma(x) \cdot z$, где $z \sim N(0,1)$.
    $$ PI(x) = P(I(x) > 0) = P(f(x) > f^*) $$
    Можно выразить через функцию стандартного нормального распределения $\Phi$:
    $$ PI(x) = P\left(z > \frac{f^* - \mu(x)}{\sigma(x)}\right) = 1 - \Phi\left(\frac{f^* - \mu(x)}{\sigma(x)}\right) = \Phi\left(\frac{\mu(x) - f^*}{\sigma(x)}\right) $$

*   **Ожидаемое улучшение (Expected Improvement, EI):**
    Математическое ожидание улучшения $I(x)$:
    $$ EI(x) = \mathbb{E}[I(x)] = \int_{-\infty}^{\infty} \max(f(x) - f^*, 0) p(f(x)|x) df(x) $$
    После вычислений (слайд 20), если $u = \frac{\mu(x) - f^*}{\sigma(x)}$:
    $$ EI(x) = (\mu(x) - f^*) \Phi(u) + \sigma(x) \phi(u) $$
    где $\phi(u)$ – плотность стандартного нормального распределения.
    Можно добавить параметр "исследования" $\xi \ge 0$:
    $$ EI_\xi(x) = (\mu(x) - f^* - \xi) \Phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right) + \sigma(x) \phi\left(\frac{\mu(x) - f^* - \xi}{\sigma(x)}\right) $$
    Большее $\xi$ способствует большему исследованию.

**3.3. Алгоритм байесовской оптимизации**
1.  **Инициализация:**
    a.  Сгенерировать несколько случайных точек $x_1, x_2, \dots, x_N$.
    b.  Вычислить для них реальную функцию качества $y_i = f(x_i)$.
    c.  Обучить на этих данных $(X_{meta}, Y_{meta}) = \{(x_i, y_i)\}$ суррогатную модель (например, GP).
2.  **На каждой итерации:**
    a.  Найти точку $x_{next}$, которая максимизирует выбранную функцию улучшения (UCB, PI, EI), используя текущую суррогатную модель. (Эта оптимизация "дешевая").
    b.  Вычислить реальную функцию качества $y_{next} = f(x_{next})$ (это "дорогой" шаг).
    c.  Обновить набор данных: $D_{new} = D_{old} \cup \{(x_{next}, y_{next})\}$.
    d.  Обновить (переобучить) суррогатную модель на $D_{new}$.
    Повторять до исчерпания бюджета вычислений $f(x)$.

---

## 4. Основные понятия Обучения с подкреплением (Markov Decision Process, MDP)

RL формализуется через Марковский процесс принятия решений (MDP).
*   $s \in S$: **Состояние** (State) среды.
*   $a \in A$: **Действие** (Action), которое может совершить агент.
*   $T(s'|s,a)$ или $P(s'|s,a)$: **Функция перехода** (Transition Function). Вероятность перехода в состояние $s'$ из состояния $s$ после совершения действия $a$. Предполагается **Марковское свойство**: следующее состояние зависит только от текущего состояния и действия, а не от всей предыдущей истории.
*   $\pi(a|s)$: **Стратегия (Политика, Policy)** агента. Вероятность совершения действия $a$ в состоянии $s$. Может быть детерминированной ($a = \pi(s)$) или стохастической.
*   $r(s,a)$ или $R(s,a,s')$: **Функция награды (Reward Function)**. Награда, получаемая агентом за совершение действия $a$ в состоянии $s$ (и, возможно, переход в $s'$).
*   $r_t$: Награда, полученная на шаге $t$.
*   $R_t$: **Выигрыш (Return)** или кумулятивная дисконтированная награда, начиная с шага $t$:
    $$ R_t = \sum_{k=0}^{\infty} \gamma^k r_{t+k+1} = r_{t+1} + \gamma r_{t+2} + \gamma^2 r_{t+3} + \dots $$
    Иногда $R_t = r_t + \gamma R_{t+1}$ (если $r_t$ награда за $(s_t, a_t)$).
*   $\gamma \in [0,1]$: **Коэффициент дисконтирования (Discount Factor)**. Определяет важность будущих наград по сравнению с немедленными. $\gamma < 1$ обеспечивает сходимость суммы для бесконечных эпизодов.
*   Иногда используется недисконтированный выигрыш для эпизодических задач $R_t = \sum_{k=0}^{K} r_{t+k+1}$, или средняя награда $R_t = \sum_{k=0}^{K} r_{t+k+1} / (K+1)$.

**Функции оценки (Value Functions):** Оценивают "хорошесть" состояний или пар состояние-действие.
*   $V_\pi(s) = \mathbb{E}_\pi [R_t | s_t = s]$: **Функция ценности состояния (State-Value Function)**. Ожидаемый выигрыш, начиная из состояния $s$ и следуя стратегии $\pi$.
*   $Q_\pi(s,a) = \mathbb{E}_\pi [R_t | s_t = s, a_t = a]$: **Функция ценности действия (Action-Value Function / Q-function)**. Ожидаемый выигрыш, начиная из состояния $s$, совершая действие $a$, и затем следуя стратегии $\pi$.

**Оптимальные функции оценки и стратегия:**
*   $V^*(s) = \max_\pi V_\pi(s)$: Оптимальная функция ценности состояния.
*   $Q^*(s,a) = \max_\pi Q_\pi(s,a)$: Оптимальная функция ценности действия.
*   $\pi^*(a|s)$: Оптимальная стратегия. Стратегия, которая максимизирует ожидаемый выигрыш.
    Если известна $Q^*(s,a)$, то оптимальное действие в состоянии $s$: $\pi^*(s) = \arg\max_a Q^*(s,a)$.

**4.1. Уравнения Беллмана**
Рекуррентные соотношения, связывающие ценности состояний (или пар состояние-действие) с ценностями последующих состояний.
*   **$Q_\pi$ через $V_\pi$:**
    $$ Q_\pi(s,a) = \mathbb{E}_\pi[r_{t+1} + \gamma V_\pi(s_{t+1}) | s_t=s, a_t=a] = \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V_\pi(s')) $$
    (Если награда $r(s,a)$ дается за действие $a$ в $s$, то $r(s,a)$).
*   **$V_\pi$ через $Q_\pi$:**
    $$ V_\pi(s) = \mathbb{E}_{a \sim \pi(a|s)}[Q_\pi(s,a)] = \sum_a \pi(a|s) Q_\pi(s,a) $$
*   **$V_\pi$ через $V_\pi$ (Уравнение Беллмана для $V_\pi$):**
    $$ V_\pi(s) = \sum_a \pi(a|s) \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V_\pi(s')) $$
*   **$Q_\pi$ через $Q_\pi$ (Уравнение Беллмана для $Q_\pi$):**
    $$ Q_\pi(s,a) = \sum_{s'} P(s'|s,a) \left(r(s,a,s') + \gamma \sum_{a'} \pi(a'|s') Q_\pi(s',a')\right) $$

*   **Оптимальные уравнения Беллмана (Bellman Optimality Equations):**
    $$ V^*(s) = \max_a \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma V^*(s')) $$
    $$ Q^*(s,a) = \sum_{s'} P(s'|s,a) (r(s,a,s') + \gamma \max_{a'} Q^*(s',a')) $$

**4.2. Q-learning (Off-policy TD Control)**
Алгоритм обучения без модели (model-free), который напрямую аппроксимирует оптимальную функцию $Q^*(s,a)$.
*   Изначально таблица $Q(s,a)$ (для всех пар $(s,a)$) заполнена случайными числами (или нулями).
*   **Итерации (пока $Q(s,a)$ не сошлась):**
    Находясь в состоянии $s$:
    1.  **Выбрать действие $a$:** Обычно с использованием $\epsilon$-жадной стратегии на основе текущих $Q(s,a')$ (например, $a = \arg\max_{a'} Q(s,a')$ с вероятностью $1-\epsilon$, иначе случайное действие).
    2.  **Совершить действие $a$, наблюдать награду $r$ и новое состояние $s'$**. (Переход $T(s,a) \rightarrow s'$, награда $r(s,a)$).
    3.  **Обновить $Q(s,a)$:**
        $$ Q(s,a) \leftarrow (1-\alpha) Q(s,a) + \alpha \left( r + \gamma \max_{a'} Q(s',a') \right) $$
        где $\alpha$ – скорость обучения.
        Это обновление основано на TD-ошибке (Temporal Difference error): $r + \gamma \max_{a'} Q(s',a') - Q(s,a)$.
    4.  **Перейти в новое состояние:** $s \leftarrow s'$.
    5.  Если $s$ – не терминальное состояние, вернуться к шагу 1.
*   Q-learning является **off-policy** алгоритмом: он учит оптимальную $Q^*$-функцию (соответствующую жадной стратегии) независимо от того, какая стратегия используется для выбора действий на этапе исследования (behavior policy).

**4.3. Более сложные алгоритмы RL**
*   **Проблема:** Множество состояний $S$ и действий $A$ может быть очень большим или непрерывным. Хранить таблицу $Q(s,a)$ невозможно.
*   **Решение:** Использовать **аппроксиматоры функций** (например, нейронные сети) для представления $V_\pi(s)$, $Q_\pi(s,a)$ или $\pi(a|s)$. Это область Deep Reinforcement Learning (DRL).
    *   Аналог контекстных многоруких бандитов: $Q(s,a) \approx f(s,a;\theta)$.
*   **Типы RL методов:**
    *   **Value-based методы:** Учат функцию ценности (V или Q), а стратегия выводится из нее (например, жадная). (Q-learning, DQN, SARSA).
    *   **Policy-based методы (Прямые RL методы):** Явно моделируют и оптимизируют стратегию $\pi(a|s;\theta)$ напрямую. (REINFORCE, A2C, A3C).
    *   **Actor-Critic методы:** Моделируют и стратегию (**Actor**, $\pi(a|s;\theta_\pi)$) и функцию ценности (**Critic**, $V(s;\theta_V)$ или $Q(s,a;\theta_Q)$). Актер выбирает действия, критик оценивает эти действия и помогает актеру улучшаться.
*   **Offline RL (Batch RL):** Обучение на основе заранее собранного ("чужого") набора траекторий (истории взаимодействий $(s,a,r,s')$), без возможности дальнейшего взаимодействия со средой.
*   **Off-policy алгоритмы:** Стратегия, используемая для генерации данных (behavior policy), может отличаться от стратегии, которая оценивается и улучшается (target policy). Q-learning – пример.
*   **On-policy алгоритмы:** Стратегия, используемая для генерации данных, та же самая, что и улучшаемая стратегия. (SARSA, A2C).

---

## 5. Задачи обучения с подкреплением (Примеры и Среды)

**Gymnasium (бывший OpenAI Gym)**
Популярная библиотека для разработки и сравнения алгоритмов RL. Предоставляет стандартизированный интерфейс к различным средам.
*   **GitHub:** [https://github.com/Farama-Foundation/Gymnasium](https://github.com/Farama-Foundation/Gymnasium)
*   **Классические задачи управления:** CartPole, MountainCar, Pendulum, Acrobot.
    [https://gymnasium.farama.org/](https://gymnasium.farama.org/)
*   **Игры Atari:** Большой набор классических игр Atari 2600, предоставляющих сложные среды с визуальным входом.
    [https://ale.farama.org/](https://ale.farama.org/)
*   Другие среды: робототехника (MuJoCo), настольные игры (Go, шахматы) и т.д.

---
