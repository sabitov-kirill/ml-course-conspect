# Лекция 2: Глубокое обучение

- [Лекция 2: Глубокое обучение](#лекция-2-глубокое-обучение)
  - [1. Дропаут (Dropout)](#1-дропаут-dropout)
  - [2. Батчевая нормализация (Batch Normalization, BN)](#2-батчевая-нормализация-batch-normalization-bn)
  - [3. Инициализация параметров](#3-инициализация-параметров)
  - [4. Практика глубокого обучения](#4-практика-глубокого-обучения)
  - [5. Нейронные сети (Обзор и история)](#5-нейронные-сети-обзор-и-история)

---

## 1. Дропаут (Dropout)

**1.1. Основная идея**
Дропаут – это техника регуляризации, используемая для предотвращения переобучения в нейронных сетях.
*   **Механизм:** Во время обучения на каждой итерации (для каждого мини-батча) выходы нейронов (или сами нейроны) в определённом слое "выключаются" (обнуляются) с некоторой вероятностью $p$ (часто $p=0.5$).
*   Обнулённые нейроны не участвуют в прямом и обратном распространении сигнала на данной итерации.
*   Таким образом, на каждой итерации обучения используется "прореженная" версия сети. Все эти подсети разделяют общие параметры (веса).

**1.2. Эффекты и преимущества**
*   **Уменьшение соадаптации нейронов (co-adaptation):** Нейроны становятся менее зависимыми друг от друга, так как они не могут полагаться на наличие конкретных других нейронов. Каждый нейрон вынужден учиться извлекать более полезные и робастные признаки самостоятельно.
*   **Обучение более робастных представлений:** Сеть учится представлениям, которые устойчивы к отсутствию части информации.
*   **Ансамбль моделей:** Дропаут можно рассматривать как обучение большого ансамбля различных "прореженных" сетей с общими весами. На этапе тестирования (inference) обычно используется вся сеть, но выходы нейронов, к которым применялся дропаут, масштабируются (умножаются на $1-p$ или, что эквивалентно и чаще используется на практике, во время обучения активные нейроны масштабируются на $1/(1-p)$ - "inverted dropout"). Это делается для того, чтобы ожидаемая сумма входов в следующий слой была такой же, как и во время обучения.
*   **Снижение переобучения:** Как видно из графиков, сети без дропаута значительно сильнее переобучаются (ошибка на тестовой выборке выше и менее стабильна).
*   **Увеличение времени обучения:** Дропаут может увеличить количество итераций, необходимых для сходимости (примерно вдвое, если $p=0.5$), так как на каждой итерации обновляется только часть параметров.

---

## 2. Батчевая нормализация (Batch Normalization, BN)

**2.1. Проблема внутреннего ковариационного сдвига (Internal Covariate Shift)**
В процессе обучения нейронной сети параметры каждого слоя постоянно меняются. Это приводит к тому, что распределение активаций (выходов) каждого слоя также меняется на протяжении обучения. Изменения в распределении входов последующих слоёв называются внутренним ковариационным сдвигом. Это замедляет обучение, так как слоям приходится постоянно адаптироваться к изменяющимся входным данным.

**2.2. Основная идея BN**
Поддерживать стабильное распределение активаций для входов каждого слоя путем их нормализации. Для каждой активации (каждого нейрона или канала в сверточных слоях) в пределах текущего мини-батча:
1.  Вычисляется среднее значение и дисперсия активаций по мини-батчу.
2.  Активации нормализуются (приводятся к нулевому среднему и единичной дисперсии).
    $$ \hat{x}_d = \frac{x_d - \mathbb{E}[x_d]}{\sqrt{\mathbb{D}[x_d] + \epsilon}} $$
    где $x_d$ – активация $d$-го нейрона, $\mathbb{E}[x_d]$ и $\mathbb{D}[x_d]$ – среднее и дисперсия по мини-батчу, $\epsilon$ – малая константа для численной стабильности.

**2.3. Параметрический слой для масштабирования и сдвига**
Простая нормализация может ограничить выразительную способность слоя (например, если сигмоида получит на вход значения, близкие к нулю, она будет работать в своей линейной области). Чтобы этого избежать, после нормализации вводится параметрическое преобразование:
$$ \hat{y}_d = \gamma_d \hat{x}_d + \beta_d $$
где $\gamma_d$ и $\beta_d$ – обучаемые параметры масштаба и сдвига для $d$-й активации. Если сеть "захочет", она может выучить $\gamma_d = \sqrt{\mathbb{D}[x_d] + \epsilon}$ и $\beta_d = \mathbb{E}[x_d]$, фактически отменив нормализацию.

**2.4. Алгоритм батчевой нормализации (на этапе обучения)**
**Вход:** Значения активаций $x$ по мини-батчу $\mathcal{B} = \{x_1, \dots, x_m\}$; обучаемые параметры $\gamma, \beta$.
**Выход:** $\{y_i = \text{BN}_{\gamma,\beta}(x_i)\}$.

1.  **Среднее по мини-батчу:** $$ \mu_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^m x_i $$
2.  **Дисперсия по мини-батчу:** $$ \sigma^2_{\mathcal{B}} \leftarrow \frac{1}{m} \sum_{i=1}^m (x_i - \mu_{\mathcal{B}})^2 $$
3.  **Нормализация:** $$ \hat{x}_i \leftarrow \frac{x_i - \mu_{\mathcal{B}}}{\sqrt{\sigma^2_{\mathcal{B}} + \epsilon}} $$
4.  **Масштабирование и сдвиг:** $$ y_i \leftarrow \gamma \hat{x}_i + \beta \equiv \text{BN}_{\gamma,\beta}(x_i) $$

**На этапе инференса (тестирования):**
Среднее $\mu$ и дисперсия $\sigma^2$ не вычисляются по текущему батчу, а используются накопленные (экспоненциально сглаженные) статистики со всего обучающего набора:
$$ \hat{x} = \frac{x - \mu_{population}}{\sqrt{\sigma^2_{population} + \epsilon}} $$
$$ y = \gamma \hat{x} + \beta $$

**2.5. Градиентный спуск для батчевой нормализации**
Слой BN является дифференцируемым, и градиенты по его входам $x_i$ и параметрам $\gamma, \beta$ могут быть вычислены с помощью обратного распространения ошибки. Формулы для производных (представлены на слайде 10) учитывают зависимость всех $\hat{x}_i$ от $\mu_{\mathcal{B}}$ и $\sigma^2_{\mathcal{B}}$, которые, в свою очередь, зависят от всех $x_i$ в батче.

**2.6. Анализ и эффекты BN**
*   **Ускорение обучения:** Позволяет использовать более высокие скорости обучения и ускоряет сходимость.
*   **Стабилизация обучения:** Уменьшает проблему внутреннего ковариационного сдвига.
*   **Регуляризация:** Оказывает небольшой регуляризующий эффект (из-за шума, вносимого статистиками мини-батча), что может снизить потребность в других методах регуляризации, таких как Dropout.
*   **Меньшая чувствительность к инициализации весов.**
*   **Сравнение (слайд 11):** Графики показывают, что сети с BN (например, BN-x5, BN-x30) сходятся значительно быстрее и достигают лучшей точности, чем базовые модели (Inception без BN, BN-Baseline).

---

## 3. Инициализация параметров

Правильная инициализация весов критически важна для успешного обучения глубоких нейронных сетей.

**3.1. Стандартные подходы**
*   **Векторы сдвига (biases):** Обычно инициализируются нулями.
*   **Матрицы весов (weights):**
    *   **Нельзя инициализировать нулями:** Если все веса нулевые, все нейроны в слое будут вычислять одно и то же, и градиенты будут одинаковыми, что помешает обучению (проблема симметрии).
    *   **Нельзя инициализировать одинаковыми значениями:** По той же причине симметрии.
    *   **Инициализируются случайными значениями:** Обычно из некоторого распределения (например, равномерного или нормального).

**3.2. Проблема затухания/взрыва активаций и градиентов**
Если веса слишком малы, активации и градиенты могут затухать по мере прохождения через слои. Если веса слишком велики, они могут взрываться. Цель хорошей инициализации – поддерживать дисперсию активаций и градиентов примерно одинаковой на всех слоях.

**3.3. Метод Xavier/Glorot (для tanh-подобных активаций)**
*   **Мотивация:** Предполагается, что функция активации $f(x)$ линейна вблизи нуля (например, $f(x) \approx x$ для $\tanh(x)$ при малых $x$). Идея – поддерживать дисперсию активаций и градиентов постоянной.
*   Пусть $u_d$ – активация на слое $d$, $w_d$ – веса слоя $d$, $n_d$ – количество нейронов на входе слоя $d$ (fan-in), $n_{d+1}$ – количество нейронов на выходе слоя $d$ (fan-out).
*   Для сохранения дисперсии активаций при прямом проходе: $$ \text{Var}(u_{d+1}) = n_d \text{Var}(u_d) \text{Var}(w_d) $$
    Чтобы $\text{Var}(u_{d+1}) = \text{Var}(u_d)$, нужно $n_d \text{Var}(w_d) = 1$.
*   Для сохранения дисперсии градиентов при обратном проходе: $$ \text{Var}\left(\frac{\partial L}{\partial u_d}\right) = n_{d+1} \text{Var}\left(\frac{\partial L}{\partial u_{d+1}}\right) \text{Var}(w_d) $$
    Чтобы дисперсии градиентов были равны, нужно $n_{d+1} \text{Var}(w_d) = 1$.
*   **Компромисс Xavier/Glorot:** $$ \text{Var}(w_d) = \frac{2}{n_d + n_{d+1}} $$
    Веса $w_d$ инициализируются из равномерного распределения:
    $$ w_d \sim U\left[-\sqrt{\frac{6}{n_d + n_{d+1}}}, \sqrt{\frac{6}{n_d + n_{d+1}}}\right] $$
    Или из нормального распределения с $\mu=0$ и $\sigma^2 = \frac{2}{n_d + n_{d+1}}$.

**3.4. Метод He (для ReLU-подобных активаций)**
*   ReLU "обнуляет" половину своих входов, что уменьшает дисперсию примерно вдвое.
*   Для сохранения дисперсии активаций при прямом проходе с ReLU:
    $$ \text{Var}(u_{d+1}) = n_d \text{Var}(u_d) \frac{1}{2}\text{Var}(w_d) $$
    Чтобы $\text{Var}(u_{d+1}) = \text{Var}(u_d)$, нужно $\frac{1}{2} n_d \text{Var}(w_d) = 1$, т.е. $n_d \text{Var}(w_d) = 2$.
*   **Инициализация He:** $$ \text{Var}(w_d) = \frac{2}{n_d} $$
    Веса $w_d$ инициализируются из нормального распределения:
    $$ w_d \sim N\left(0, \sqrt{\frac{2}{n_d}}\right) $$
    Или из равномерного распределения $U\left[-\sqrt{\frac{6}{n_d}}, \sqrt{\frac{6}{n_d}}\right]$.

*   **Сравнение Xavier vs He (слайды 20-21):** Графики показывают, что для сетей с ReLU инициализация He приводит к более быстрой сходимости и лучшей производительности, особенно для глубоких сетей (например, 30 слоёв), где Xavier может привести к затуханию сигнала.

---

## 4. Практика глубокого обучения

**4.1. Вычисления на GPU (GPGPU - General-Purpose computing on Graphics Processing Units)**
*   Видеокарты (GPU) оптимизированы для массовой параллельной обработки однотипных данных, что идеально подходит для матричных операций в нейронных сетях.
*   Используют SIMD (Single Instruction, Multiple Data) параллелизм: одна инструкция выполняется одновременно над многими элементами данных.
*   **CUDA (Compute Unified Device Architecture):** Проприетарная технология NVIDIA, доминирующая в области вычислений на GPU для глубокого обучения. OpenCL является открытой альтернативой, но имеет меньшую поддержку и экосистему.

**4.2. Архитектура AlexNet (2012)**
*   Знаковая архитектура, популяризовавшая глубокое обучение, выиграв соревнование ImageNet.
*   Особенности: использование ReLU, Dropout, обучение на нескольких GPU (архитектура не помещалась в память одной видеокарты того времени).
*   Имела 5 сверточных слоёв и 3 полносвязных.

**4.3. Распараллеливание обучения глубоких моделей**
*   **Data Parallelism (Параллелизм по данным):**
    *   Модель полностью копируется на каждое из $N$ устройств (GPU).
    *   Каждое устройство обрабатывает свою часть (1/N) батча данных.
    *   После вычисления градиентов на каждом устройстве, они агрегируются (например, усредняются) и параметры модели обновляются синхронно на всех устройствах.
    *   Наиболее распространенный тип.
*   **Model Parallelism (Параллелизм по модели) / Tensor Parallelism:**
    *   Части самой модели (слои или даже отдельные тензоры весов) распределяются по разным устройствам.
    *   Данные проходят последовательно через части модели на разных устройствах.
    *   Используется, когда модель слишком велика, чтобы поместиться на одном GPU.
*   **Pipeline Parallelism (Конвейерный параллелизм):**
    *   Модель делится на последовательные стадии (chunks), каждая из которых выполняется на отдельном устройстве.
    *   Мини-батч делится на микро-батчи, которые обрабатываются конвейерно.
    *   Проблема: "пузыри" (bubbles) или простои устройств, когда одна стадия ждет другую. Решается с помощью планирования и перекрытия вычислений (например, GPipe, PipeDream).

**4.4. Квантизация (Quantization)**
Снижение точности представления весов и/или активаций модели (например, с 32-битных чисел с плавающей точкой (FP32) до 16-битных (FP16, BFloat16) или 8-битных целых чисел (INT8)).
*   **Цели:**
    *   Уменьшение размера модели.
    *   Ускорение вычислений (операции с меньшей точностью быстрее и менее энергозатратны).
    *   Снижение энергопотребления.
*   **Типы:**
    *   **Симметричная квантизация:** Диапазон вещественных чисел отображается симметрично относительно нуля в целочисленный диапазон.
        $$ r = S \cdot (Q - Z) $$
        где $Z$ обычно 0 для симметричной.
    *   **Несимметричная квантизация:** Отображение может быть несимметричным, используется точка нуля $Z$ (zero-point).
        $$ r \approx S \cdot (Q - Z) $$
        где $r$ - вещественное значение, $Q$ - квантованное целочисленное, $S$ - масштаб (scale), $Z$ - точка нуля.
*   **Производительность и энергопотребление (слайд 29):** Операции с INT8/INT4 значительно быстрее и требуют меньше энергии, чем FP32.
*   **Различные числовые типы (слайд 30):**
    *   Int16: Целочисленный.
    *   Float32: Стандартная одинарная точность.
    *   Float16: Половинная точность (меньший диапазон, но быстрее).
    *   Bfloat16 (Brain Floating Point): Половинная точность, но с диапазоном как у Float32 (за счет меньшей точности мантиссы).
    *   TensorFloat32 (TF32): Используется в GPU NVIDIA Ampere, внутренне для умножения матриц; точность выше FP16, скорость выше FP32.
    *   E4M3/E5M2: Форматы с плавающей точкой с еще меньшим количеством бит (например, 8-битные FP8).
*   **Обучение с учётом квантизации (Quantization-Aware Training, QAT):**
    *   Во время прямого прохода веса и/или активации квантуются.
    *   Во время обратного прохода градиенты вычисляются для полных (неквантованных) весов, но пропускаются через "симулятор" квантизации. Часто используется **Straight-Through Estimator (STE):** градиент функции квантования аппроксимируется градиентом тождественной функции (или функции отсечения по диапазону).
    *   Это позволяет модели адаптироваться к потере точности из-за квантизации во время обучения.

**4.5. Дистилляция знаний (Knowledge Distillation)**
Метод переноса знаний от большой, сложной "учительской" модели (teacher model) к меньшей, более быстрой "студенческой" модели (student model).
1.  Обучается большая учительская модель.
2.  Студенческая модель обучается предсказывать:
    *   **"Мягкие" метки (soft labels):** Выходы учительской модели (логиты или вероятности после softmax с "температурой" $T > 1$). Температура "сглаживает" распределение вероятностей, давая больше информации о том, как учитель "думает".
        $$ q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)} $$
        где $z_i$ - логиты.
    *   **"Твердые" метки (hard labels):** Истинные метки из обучающего набора (как при обычном обучении).
3.  Функция потерь для студента – это комбинация потерь на мягких метках (distillation loss) и потерь на твердых метках (student loss):
    $$ L = \alpha L_{soft} + (1-\alpha) L_{hard} $$
    $L_{soft}$ часто является KL-дивергенцией между распределениями учителя и студента, или кросс-энтропией с мягкими метками учителя.

---

## 5. Нейронные сети (Обзор и история)

**5.1. Мозг vs. Искусственная нейронная сеть (ИНС) vs. Обучаемая функция**
*   **Мозг:** Биологическая система, обладающая интеллектом, сознанием и т.д. Нейроны – его структурные единицы.
    *   *Вопросы:* Почему именно нейроны? Как точно их работа аппроксимируется? Как из их работы возникает интеллект?
*   **ИНС:** Математическая модель, вдохновленная структурой мозга. Состоит из связанных "искусственных нейронов".
    *   *Вопросы:* Как обучать ИНС? Как применять их для задач МО?
*   **Обучаемая функция:** Более общее понятие. Это любая параметрическая функция, параметры которой можно настроить (обучить) для решения задачи, минимизируя функцию потерь.
    *   Не обязательно должна иметь "нейронную" архитектуру.
    *   Может не иметь циклов (для прямого распространения).
    *   Обучается градиентным спуском.
    *   Пример: дерево решений - тоже обучаемая функция, но не ИНС в классическом понимании.

**5.2. Искусственный нейрон (Перцептрон)**
*   **Реальный нейрон:** Сигналы от дендритов накапливаются. Разные дендриты имеют разную важность. Активация происходит после достижения некоторого порога.
*   **Искусственный нейрон (модель Мак-Каллока-Питтса, перцептрон Розенблатта):**
    1.  Входные значения $x_i$ суммируются с весами $w_{ji}$: $$ \text{net}_j = \sum_i w_{ji} x_i $$
    2.  Применяется функция активации $f$: $$ o_j = f(\text{net}_j - \theta_j) $$ или $$ o_j = f(\text{net}_j + b_j) $$
        где $\theta_j$ – порог, $b_j$ – сдвиг.

**5.3. Правила обучения "до изобретения" обратного распространения**
*   **Правило Хебба (1949) (для {-1, 1} классификации):** "Neurons that fire together, wire together". Если предсказание $(\langle w^{[k]}, x_{(k)} \rangle)$ и истинная метка $y_{(k)}$ имеют разные знаки, то вес обновляется в сторону $x_{(k)}y_{(k)}$.
    $$ \text{Если } (\langle w^{[k]}, x_{(k)} \rangle) y_{(k)} < 0, \text{ то } w^{[k+1]} := w^{[k]} + \eta x_{(k)} y_{(k)} $$
*   **Правило Розенблатта (1957) (для {0, 1} классификации):**
    $$ w^{[k+1]} := w^{[k]} - \eta (a_{w}(x_{(k)}) - y_{(k)}) x_{(k)} $$
    где $a_w(x_{(k)})$ – выход перцептрона (0 или 1).
*   **Дельта-правило (Уидроу-Хофф, 1960) (для Adaline):** Минимизирует среднеквадратичную ошибку $L(a_w, x) = (\langle w, x \rangle - y)^2$ (где выход $a_w = \langle w, x \rangle$ еще до применения пороговой функции).
    $$ w^{[k+1]} := w^{[k]} - \eta (\langle w^{[k]}, x_{(k)} \rangle - y_{(k)}) x_{(k)} $$
    Это фактически градиентный спуск для линейного нейрона.

**5.4. Краткая история**
*   **1943:** Искусственный нейрон (Мак-Каллок и Питтс).
*   **1949:** Правило обучения нейронов (Хебб).
*   **1957:** Перцептрон (Розенблатт).
*   **1960:** Правило обучения перцептрона Adaline (Уидроу и Хофф), дельта-правило.
*   **1969:** Книга "Персептроны" (Минский и Паперт) – показала ограничения однослойных перцептронов (не могут решить XOR), что привело к "зиме ИИ".
*   **1974 (и ранее):** Алгоритм обратного распространения ошибки (backpropagation) (предложен независимо несколькими исследователями: Уэрбoc, Линнайнмаа, Галушкин). Популяризован Румельхартом, Хинтоном и Уильямсом в 1986.
*   **1980:** Сверточные сети (Neocognitron, Фукусима).
*   **1982:** Рекуррентные сети Хопфилда.
*   **1991:** Проблема затухания градиента (Хохрайтер).
*   **1997:** LSTM-модуль (Хохрайтер и Шмидхубер).
*   **1998:** Градиентный спуск для сверточных сетей (LeNet, ЛеКун и др.).
*   **1998:** Обобщенная аппроксимационная теорема (Универсальная теорема аппроксимации для сетей с различными активациями, не только сигмоидами).
*   **2006:** Глубокие сети доверия (Deep Belief Networks, Хинтон и др.) – начало возрождения глубокого обучения (послойная предобучение).

**5.5. Начало современности**
*   **2012:** Хинтон, Крижевский и Суцкевер предложили Dropout и создали AlexNet, которая с большим отрывом победила на соревновании ImageNet. Это событие считается началом современной эры глубокого обучения.
*   График ILSVRC top-5 error on ImageNet показывает резкое снижение ошибки после 2012 года благодаря DL.

**5.6. Почему "сейчас"? (Факторы успеха DL)**
1.  **Огромные наборы данных (Huge Datasets):** Наличие больших аннотированных датасетов (ImageNet, Wikipedia, Common Crawl и т.д.).
2.  **Мощное оборудование (Powerful Hardware):** Развитие GPU, обеспечивающих необходимой вычислительной мощностью.
3.  **Новые алгоритмы и архитектуры (New Algorithms):** Разработка эффективных архитектур (ResNet, Transformers), техник регуляризации (Dropout, BN), оптимизаторов (Adam) и методов инициализации.

---
